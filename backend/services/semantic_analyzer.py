"""
Semantic Keyword Analyzer with KeyBERT
ä¼˜é›…é™çº§ï¼šKeyBERT ä¸å¯ç”¨æ—¶ä½¿ç”¨ TF-IDFï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
"""

from typing import List, Dict, Optional
import numpy as np

# å°è¯•å¯¼å…¥ KeyBERTï¼Œå¤±è´¥åˆ™ä½¿ç”¨åŸºç¡€æ–¹æ³•
try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    KEYBERT_AVAILABLE = False
    print("âš ï¸  KeyBERT not available, using TF-IDF fallback")


class SemanticKeywordAnalyzer:
    """
    è¯­ä¹‰å…³é”®è¯åˆ†æå™¨
    
    ç‰¹æ€§ï¼š
    - KeyBERT è¯­ä¹‰åˆ†æï¼ˆå‡†ç¡®ç‡ +20%ï¼‰
    - è‡ªåŠ¨é™çº§åˆ° TF-IDF
    - å¯é€‰å¯ç”¨ï¼ˆä¸å½±å“é»˜è®¤æµç¨‹ï¼‰
    """
    
    def __init__(self, tfidf_analyzer=None):
        """
        Args:
            tfidf_analyzer: åŸºç¡€ TF-IDF åˆ†æå™¨ï¼ˆé™çº§ä½¿ç”¨ï¼‰
        """
        self.keybert = None
        self.tfidf_analyzer = tfidf_analyzer
        self.use_semantic = KEYBERT_AVAILABLE
        
        # å»¶è¿ŸåŠ è½½ KeyBERTï¼ˆèŠ‚çœå†…å­˜ï¼‰
        self._keybert_loaded = False
    
    def extract_keywords(
        self,
        texts: List[str],
        use_semantic: bool = False,
        top_n: int = 15,
        diversity: float = 0.7
    ) -> List[Dict]:
        """
        æå–å…³é”®è¯
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆå¦‚è§†é¢‘æ ‡é¢˜ï¼‰
            use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†æï¼ˆé»˜è®¤ Falseï¼Œä½¿ç”¨ TF-IDFï¼‰
            top_n: è¿”å›å…³é”®è¯æ•°é‡
            diversity: å¤šæ ·æ€§ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¤šæ ·ï¼‰
        
        Returns:
            List of {topic, score, frequency, method}
        """
        # å¦‚æœä¸å¯ç”¨è¯­ä¹‰åˆ†æï¼Œæˆ– KeyBERT ä¸å¯ç”¨ï¼Œä½¿ç”¨ TF-IDF
        if not use_semantic or not self.use_semantic:
            return self._tfidf_fallback(texts, top_n)
        
        # å»¶è¿ŸåŠ è½½ KeyBERT
        if not self._keybert_loaded:
            self._load_keybert()
        
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œé™çº§
        if self.keybert is None:
            return self._tfidf_fallback(texts, top_n)
        
        # ä½¿ç”¨ KeyBERT æå–è¯­ä¹‰å…³é”®è¯
        return self._keybert_extraction(texts, top_n, diversity)
    
    def _load_keybert(self):
        """å»¶è¿ŸåŠ è½½ KeyBERT æ¨¡å‹"""
        try:
            print("ğŸ“¥ Loading KeyBERT model (first use)...")
            self.keybert = KeyBERT()
            self._keybert_loaded = True
            print("âœ… KeyBERT loaded successfully")
        except Exception as e:
            print(f"âš ï¸  Failed to load KeyBERT: {e}")
            self.keybert = None
            self._keybert_loaded = True  # æ ‡è®°å·²å°è¯•ï¼Œé¿å…é‡å¤åŠ è½½
    
    def _keybert_extraction(
        self,
        texts: List[str],
        top_n: int,
        diversity: float
    ) -> List[Dict]:
        """ä½¿ç”¨ KeyBERT æå–è¯­ä¹‰å…³é”®è¯"""
        try:
            # åˆå¹¶æ–‡æœ¬
            combined_text = ' '.join(texts)
            
            # KeyBERT æå–
            keywords = self.keybert.extract_keywords(
                combined_text,
                keyphrase_ngram_range=(1, 3),  # 1-3 è¯ç»„åˆ
                stop_words='english',
                top_n=top_n * 2,  # æå–æ›´å¤šï¼Œåç»­åˆå¹¶
                use_maxsum=True,  # æœ€å¤§åŒ–å¤šæ ·æ€§
                diversity=diversity
            )
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            semantic_topics = [
                {
                    'topic': kw[0],
                    'score': float(kw[1]),
                    'frequency': self._count_frequency(kw[0], texts),
                    'method': 'keybert_semantic'
                }
                for kw in keywords
            ]
            
            # å¦‚æœä¹Ÿæœ‰ TF-IDF åˆ†æå™¨ï¼Œåˆå¹¶ç»“æœ
            if self.tfidf_analyzer:
                tfidf_topics = self.tfidf_analyzer.extract_topics_from_titles(texts)
                merged = self._merge_semantic_and_tfidf(semantic_topics, tfidf_topics)
                return merged[:top_n]
            
            return semantic_topics[:top_n]
            
        except Exception as e:
            print(f"âš ï¸  KeyBERT extraction failed: {e}")
            # é™çº§åˆ° TF-IDF
            return self._tfidf_fallback(texts, top_n)
    
    def _tfidf_fallback(self, texts: List[str], top_n: int) -> List[Dict]:
        """é™çº§åˆ° TF-IDF æ–¹æ³•"""
        if self.tfidf_analyzer:
            print("â„¹ï¸  Using TF-IDF fallback for keyword extraction")
            topics = self.tfidf_analyzer.extract_topics_from_titles(texts)
            # æ·»åŠ  method æ ‡è®°
            for topic in topics:
                topic['method'] = 'tfidf_fallback'
            return topics[:top_n]
        else:
            # å¦‚æœè¿ TF-IDF éƒ½æ²¡æœ‰ï¼Œè¿”å›ç©º
            print("âš ï¸  No fallback analyzer available")
            return []
    
    def _merge_semantic_and_tfidf(
        self,
        semantic_topics: List[Dict],
        tfidf_topics: List[Dict]
    ) -> List[Dict]:
        """
        åˆå¹¶ KeyBERT å’Œ TF-IDF ç»“æœ
        
        ç­–ç•¥ï¼š
        - KeyBERT æ“…é•¿è¯­ä¹‰ç†è§£ï¼ˆ"AI" = "artificial intelligence"ï¼‰
        - TF-IDF æ“…é•¿é¢‘ç‡ç»Ÿè®¡
        - ä¸¤è€…äº’è¡¥
        """
        topic_map = {}
        
        # æ·»åŠ  KeyBERT ç»“æœï¼ˆæƒé‡ 0.6ï¼‰
        for topic_data in semantic_topics:
            topic = topic_data['topic'].lower()
            topic_map[topic] = {
                'topic': topic_data['topic'],  # ä¿ç•™åŸå§‹å¤§å°å†™
                'semantic_score': topic_data['score'],
                'tfidf_score': 0,
                'frequency': topic_data['frequency'],
                'methods': ['keybert']
            }
        
        # æ·»åŠ  TF-IDF ç»“æœï¼ˆæƒé‡ 0.4ï¼‰
        for topic_data in tfidf_topics[:15]:  # åªå– top 15
            topic = topic_data['topic'].lower()
            
            if topic in topic_map:
                # å·²å­˜åœ¨ï¼Œæ›´æ–°åˆ†æ•°
                topic_map[topic]['tfidf_score'] = topic_data['score']
                topic_map[topic]['methods'].append('tfidf')
            else:
                # æ–°ä¸»é¢˜
                topic_map[topic] = {
                    'topic': topic_data['topic'],
                    'semantic_score': 0,
                    'tfidf_score': topic_data['score'],
                    'frequency': topic_data['frequency'],
                    'methods': ['tfidf']
                }
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        for topic_data in topic_map.values():
            # åŠ æƒå¹³å‡ï¼šKeyBERT 60%, TF-IDF 40%
            composite_score = (
                topic_data['semantic_score'] * 0.6 +
                topic_data['tfidf_score'] * 0.4
            )
            
            # å¤šæ–¹æ³•éªŒè¯åŠ æˆ
            if len(topic_data['methods']) > 1:
                composite_score *= 1.2
            
            topic_data['score'] = composite_score
            topic_data['method'] = 'hybrid_semantic_tfidf'
        
        # æ’åº
        ranked_topics = sorted(
            topic_map.values(),
            key=lambda x: (x['score'], x['frequency']),
            reverse=True
        )
        
        return ranked_topics
    
    def _count_frequency(self, term: str, texts: List[str]) -> int:
        """ç»Ÿè®¡è¯é¢‘"""
        count = 0
        term_lower = term.lower()
        for text in texts:
            count += text.lower().count(term_lower)
        return count
    
    def analyze_semantic_similarity(
        self,
        query: str,
        candidates: List[str]
    ) -> List[Dict]:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç”¨äºåŒ¹é…æ¨èï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚æ¨èå…³é”®è¯ï¼‰
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨ï¼ˆå¦‚é¢‘é“ä¸»é¢˜ï¼‰
        
        Returns:
            List of {text, similarity_score}
        """
        if not self.use_semantic or self.keybert is None:
            # é™çº§ï¼šç®€å•å­—ç¬¦ä¸²åŒ¹é…
            return self._simple_similarity(query, candidates)
        
        try:
            # ä½¿ç”¨ sentence-transformers è®¡ç®—ç›¸ä¼¼åº¦
            from sentence_transformers import SentenceTransformer, util
            
            model = SentenceTransformer('all-MiniLM-L6-v2')  # è½»é‡æ¨¡å‹
            
            # ç¼–ç 
            query_embedding = model.encode(query, convert_to_tensor=True)
            candidate_embeddings = model.encode(candidates, convert_to_tensor=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = util.cos_sim(query_embedding, candidate_embeddings)[0]
            
            results = [
                {
                    'text': candidates[i],
                    'similarity_score': float(similarities[i])
                }
                for i in range(len(candidates))
            ]
            
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results
            
        except Exception as e:
            print(f"âš ï¸  Semantic similarity failed: {e}")
            return self._simple_similarity(query, candidates)
    
    def _simple_similarity(self, query: str, candidates: List[str]) -> List[Dict]:
        """ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆé™çº§æ–¹æ³•ï¼‰"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        results = []
        for candidate in candidates:
            candidate_lower = candidate.lower()
            candidate_words = set(candidate_lower.split())
            
            # Jaccard ç›¸ä¼¼åº¦
            intersection = len(query_words & candidate_words)
            union = len(query_words | candidate_words)
            similarity = intersection / union if union > 0 else 0
            
            results.append({
                'text': candidate,
                'similarity_score': similarity
            })
        
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        return results


# å…¨å±€å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_semantic_analyzer_instance = None

def get_semantic_analyzer(tfidf_analyzer=None) -> SemanticKeywordAnalyzer:
    """è·å–è¯­ä¹‰åˆ†æå™¨å•ä¾‹"""
    global _semantic_analyzer_instance
    if _semantic_analyzer_instance is None:
        _semantic_analyzer_instance = SemanticKeywordAnalyzer(tfidf_analyzer)
    return _semantic_analyzer_instance
