"""
ML-based Performance Predictor with XGBoost
ä¼˜é›…é™çº§ï¼šæ— è®­ç»ƒæ•°æ®æ—¶ä½¿ç”¨è§„åˆ™æ–¹æ³•ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
"""

import numpy as np
import re
from typing import Dict, Optional
import json
import os
from datetime import datetime

# å°è¯•å¯¼å…¥ ML åº“ï¼Œå¤±è´¥åˆ™ä½¿ç”¨è§„åˆ™æ–¹æ³•
try:
    import xgboost as xgb
    from sklearn.preprocessing import StandardScaler
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoost not available, using rule-based prediction")


class MLPerformancePredictor:
    """
    åŸºäº XGBoost çš„æ’­æ”¾é‡é¢„æµ‹å™¨
    
    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨é™çº§åˆ°è§„åˆ™æ–¹æ³•ï¼ˆå¦‚æœ XGBoost ä¸å¯ç”¨ï¼‰
    - ç‰¹å¾å·¥ç¨‹ï¼ˆ13 ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
    - ç½®ä¿¡åº¦è¯„ä¼°
    """
    
    def __init__(self, model_path: Optional[str] = None):
        self.model = None
        self.scaler = StandardScaler() if XGBOOST_AVAILABLE else None
        self.is_trained = False
        self.use_ml = XGBOOST_AVAILABLE
        
        # ç‰¹å¾åç§°ï¼ˆç”¨äºå¯è§£é‡Šæ€§ï¼‰
        self.feature_names = [
            'title_length', 'title_has_numbers', 'title_has_emoji',
            'title_emotional_words', 'desc_length', 'tag_count',
            'log_subscribers', 'log_channel_avg_views', 'channel_engagement',
            'trend_score', 'trend_growth', 'multi_platform', 'relevance_score'
        ]
        
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self._load_model(model_path)
    
    def predict_performance(
        self,
        keyword: str,
        channel_analysis: Dict,
        trend: Dict,
        relevance_score: float = 0.0
    ) -> Dict:
        """
        é¢„æµ‹è§†é¢‘è¡¨ç°
        
        Args:
            keyword: æ¨èçš„å…³é”®è¯/ä¸»é¢˜
            channel_analysis: é¢‘é“åˆ†ææ•°æ®
            trend: ç¤¾äº¤è¶‹åŠ¿æ•°æ®
            relevance_score: ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        
        Returns:
            {
                'predicted_views': int,
                'confidence': float,
                'tier': str,
                'method': str,
                'feature_importance': dict (å¦‚æœä½¿ç”¨ ML)
            }
        """
        # å¦‚æœ ML å¯ç”¨ä¸”å·²è®­ç»ƒï¼Œä½¿ç”¨ ML é¢„æµ‹
        if self.use_ml and self.is_trained:
            return self._ml_prediction(keyword, channel_analysis, trend, relevance_score)
        
        # å¦åˆ™ä½¿ç”¨è§„åˆ™æ–¹æ³•ï¼ˆä¸åŸä»£ç é€»è¾‘ä¸€è‡´ï¼‰
        return self._rule_based_prediction(keyword, channel_analysis, trend, relevance_score)
    
    def _extract_features(
        self,
        keyword: str,
        channel_analysis: Dict,
        trend: Dict,
        relevance_score: float
    ) -> np.ndarray:
        """æå– 13 ä¸ªé¢„æµ‹ç‰¹å¾"""
        
        high_performers = channel_analysis.get('high_performers', {})
        target_audience = channel_analysis.get('target_audience', {})
        channel_data = channel_analysis.get('channel_data', {})
        
        # 1. æ ‡é¢˜ç‰¹å¾
        title_length = len(keyword)
        title_has_numbers = int(bool(re.search(r'\d', keyword)))
        title_has_emoji = int(bool(re.search(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF]', keyword)))
        
        # æƒ…æ„Ÿè¯ï¼ˆæ ¹æ®å­¦æœ¯ç ”ç©¶ï¼Œæƒ…æ„Ÿè¯æå‡ç‚¹å‡»ç‡ï¼‰
        emotional_words = ['amazing', 'shocking', 'incredible', 'best', 'worst', 
                          'ultimate', 'secret', 'proven', 'must', 'never']
        title_emotional = sum(1 for word in emotional_words if word.lower() in keyword.lower())
        
        # 2. æè¿°ç‰¹å¾ï¼ˆä½¿ç”¨å…³é”®è¯é•¿åº¦ä½œä¸ºä»£ç†ï¼‰
        desc_length = len(keyword) * 10  # ä¼°ç®—
        
        # 3. æ ‡ç­¾ç‰¹å¾
        tag_count = len(keyword.split())  # ä¼°ç®—æ ‡ç­¾æ•°é‡
        
        # 4. é¢‘é“ç‰¹å¾
        subscriber_count = channel_data.get('subscriberCount', 1000) if isinstance(channel_data, dict) else 1000
        log_subscribers = np.log1p(subscriber_count)
        
        channel_avg_views = high_performers.get('median_views', 10000)
        log_channel_avg_views = np.log1p(channel_avg_views)
        
        # è®¡ç®—é¢‘é“äº’åŠ¨ç‡
        engagement_rate_str = target_audience.get('engagement_rate', '0.5%') if isinstance(target_audience, dict) else '0.5%'
        channel_engagement = float(engagement_rate_str.replace('%', '')) / 100 if '%' in str(engagement_rate_str) else 0.005
        
        # 5. è¶‹åŠ¿ç‰¹å¾
        trend_score = trend.get('composite_score', 0)
        trend_growth = trend.get('growth_rate', 0)
        multi_platform = len(trend.get('sources', []))
        
        # 6. ç›¸å…³æ€§ç‰¹å¾
        relevance = relevance_score / 100  # å½’ä¸€åŒ–åˆ° 0-1
        
        # ç»„è£…ç‰¹å¾å‘é‡
        features = np.array([
            title_length,
            title_has_numbers,
            title_has_emoji,
            title_emotional,
            desc_length,
            tag_count,
            log_subscribers,
            log_channel_avg_views,
            channel_engagement,
            trend_score,
            trend_growth,
            multi_platform,
            relevance
        ]).reshape(1, -1)
        
        return features
    
    def _ml_prediction(
        self,
        keyword: str,
        channel_analysis: Dict,
        trend: Dict,
        relevance_score: float
    ) -> Dict:
        """ä½¿ç”¨ XGBoost è¿›è¡Œé¢„æµ‹"""
        
        # æå–ç‰¹å¾
        features = self._extract_features(keyword, channel_analysis, trend, relevance_score)
        
        # æ ‡å‡†åŒ–
        features_scaled = self.scaler.transform(features)
        
        # é¢„æµ‹ï¼ˆlog-transformed viewsï¼‰
        log_predicted_views = self.model.predict(features_scaled)[0]
        predicted_views = int(np.expm1(log_predicted_views))  # å log è½¬æ¢
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç‰¹å¾èŒƒå›´ï¼‰
        confidence = self._calculate_confidence(features_scaled)
        
        # åˆ†ç±»ç­‰çº§
        tier = self._classify_tier(predicted_views, channel_analysis)
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆç”¨äºå¯è§£é‡Šæ€§ï¼‰
        feature_importance = self._get_feature_importance(features)
        
        return {
            'predicted_views': max(100, predicted_views),  # æœ€ä½ 100 æ’­æ”¾
            'confidence': confidence,
            'tier': tier,
            'description': self._get_tier_description(tier),
            'method': 'xgboost_ml',
            'feature_importance': feature_importance
        }
    
    def _rule_based_prediction(
        self,
        keyword: str,
        channel_analysis: Dict,
        trend: Dict,
        relevance_score: float
    ) -> Dict:
        """
        åŸºäºè§„åˆ™çš„é¢„æµ‹ï¼ˆä¸åŸ intelligent_recommender.py é€»è¾‘ä¸€è‡´ï¼‰
        ç¡®ä¿é™çº§æ—¶ç»“æœä¸å˜
        """
        
        high_performers = channel_analysis.get('high_performers', {})
        
        # 1. è·å–é¢‘é“åŸºå‡†æ’­æ”¾é‡ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼Œæ›´ç¨³å®šï¼‰
        median_views = high_performers.get('median_views', 10000)
        avg_views = high_performers.get('avg_views', 10000)
        
        # åŠ æƒå¹³å‡ï¼ˆä¸­ä½æ•°æƒé‡æ›´é«˜ï¼‰
        base_views = int(median_views * 0.7 + avg_views * 0.3) if median_views and avg_views else 10000
        base_views = max(500, base_views)  # æœ€ä½åŸºå‡†
        
        # 2. è®¡ç®—çƒ­åº¦å€æ•°ï¼ˆç—…æ¯’æ½œåŠ›ï¼‰
        viral_potential = self._calculate_viral_potential(trend)
        
        if viral_potential >= 90:
            viral_multiplier = 2.2 + (viral_potential - 90) * 0.03
        elif viral_potential >= 70:
            viral_multiplier = 1.6 + (viral_potential - 70) * 0.03
        elif viral_potential >= 50:
            viral_multiplier = 1.2 + (viral_potential - 50) * 0.02
        else:
            viral_multiplier = 0.9 + (viral_potential / 50) * 0.3
        
        viral_multiplier = max(0.7, min(3.0, viral_multiplier))
        
        # 3. ç›¸å…³æ€§è°ƒæ•´
        if relevance_score >= 80:
            relevance_multiplier = 1.0 + (relevance_score - 80) * 0.01
        elif relevance_score >= 60:
            relevance_multiplier = 0.85 + (relevance_score - 60) * 0.0075
        else:
            relevance_multiplier = 0.7 + (relevance_score / 60) * 0.15
        
        # 4. é¢‘é“è§„æ¨¡è°ƒæ•´
        total_videos = high_performers.get('total_videos', 50)
        if total_videos > 100:
            channel_stability = 0.95  # æˆç†Ÿé¢‘é“ï¼Œæ³¢åŠ¨å°
        elif total_videos > 50:
            channel_stability = 1.0
        else:
            channel_stability = 1.1  # æ–°é¢‘é“ï¼Œæ½œåŠ›å¤§
        
        # 5. æ ‡é¢˜ä¼˜åŒ–åŠ æˆ
        avg_title_length = high_performers.get('avg_title_length', 50)
        title_length = len(keyword)
        if 30 <= title_length <= 70 and 30 <= avg_title_length <= 60:
            title_optimization = 1.05
        else:
            title_optimization = 0.98
        
        # 6. ç»¼åˆè®¡ç®—
        predicted_views = int(
            base_views * 
            viral_multiplier * 
            relevance_multiplier * 
            channel_stability *
            title_optimization
        )
        
        predicted_views = max(500, predicted_views)
        
        # 7. åˆ†ç±»ç­‰çº§
        tier = self._classify_tier(predicted_views, channel_analysis)
        
        # 8. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ•°æ®å®Œæ•´æ€§ï¼‰
        confidence = self._calculate_rule_confidence(channel_analysis, trend)
        
        return {
            'predicted_views': predicted_views,
            'confidence': confidence,
            'tier': tier,
            'description': self._get_tier_description(tier),
            'method': 'rule_based'
        }
    
    def _calculate_viral_potential(self, trend: Dict) -> float:
        """è®¡ç®—ç—…æ¯’æ½œåŠ›åˆ†æ•°ï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰"""
        composite_score = trend.get('composite_score', 0)
        growth_rate = trend.get('growth_rate', 0)
        source_count = len(trend.get('sources', []))
        
        base_score = composite_score
        growth_bonus = min(30, growth_rate * 0.3)
        platform_bonus = min(20, (source_count - 1) * 10)
        
        viral_score = base_score + growth_bonus + platform_bonus
        return min(100, round(viral_score, 2))
    
    def _classify_tier(self, predicted_views: int, channel_analysis: Dict) -> str:
        """åˆ†ç±»è¡¨ç°ç­‰çº§"""
        high_performers = channel_analysis.get('high_performers', {})
        median_views = high_performers.get('median_views', 10000)
        
        # ç›¸å¯¹äºé¢‘é“ä¸­ä½æ•°çš„è¡¨ç°
        if predicted_views >= median_views * 2:
            return 'excellent'
        elif predicted_views >= median_views * 1.3:
            return 'good'
        elif predicted_views >= median_views * 0.8:
            return 'moderate'
        else:
            return 'low'
    
    def _get_tier_description(self, tier: str) -> str:
        """è·å–ç­‰çº§æè¿°"""
        descriptions = {
            'excellent': 'é¢„è®¡è¡¨ç°ä¼˜å¼‚ï¼Œå¯èƒ½æˆä¸ºçˆ†æ¬¾',
            'good': 'é¢„è®¡è¡¨ç°è‰¯å¥½ï¼Œé«˜äºå¹³å‡æ°´å¹³',
            'moderate': 'é¢„è®¡è¡¨ç°ä¸­ç­‰ï¼Œç¨³å®šæµé‡',
            'low': 'é¢„è®¡è¡¨ç°ä¸€èˆ¬ï¼Œå¯ä½œä¸ºå°è¯•'
        }
        return descriptions.get(tier, 'é¢„è®¡è¡¨ç°ä¸­ç­‰')
    
    def _calculate_confidence(self, features_scaled: np.ndarray) -> float:
        """è®¡ç®— ML é¢„æµ‹çš„ç½®ä¿¡åº¦"""
        # ç®€åŒ–ç‰ˆï¼šåŸºäºç‰¹å¾æ˜¯å¦åœ¨è®­ç»ƒèŒƒå›´å†…
        # å®é™…åº”è¯¥ä½¿ç”¨æ¨¡å‹çš„ prediction interval
        
        # å‡è®¾è®­ç»ƒæ•°æ®çš„ç‰¹å¾èŒƒå›´åœ¨ [-3, 3]ï¼ˆæ ‡å‡†åŒ–åï¼‰
        out_of_range = np.sum(np.abs(features_scaled) > 3)
        
        if out_of_range == 0:
            confidence = 0.9
        elif out_of_range <= 2:
            confidence = 0.75
        else:
            confidence = 0.6
        
        return round(confidence, 2)
    
    def _calculate_rule_confidence(self, channel_analysis: Dict, trend: Dict) -> float:
        """è®¡ç®—è§„åˆ™æ–¹æ³•çš„ç½®ä¿¡åº¦"""
        confidence = 0.6  # åŸºç¡€ç½®ä¿¡åº¦
        
        # æ•°æ®å®Œæ•´æ€§åŠ æˆ
        high_performers = channel_analysis.get('high_performers', {})
        
        if high_performers.get('total_videos', 0) > 50:
            confidence += 0.1  # é¢‘é“æ•°æ®å……è¶³
        
        if trend.get('sources', []):
            confidence += 0.05 * len(trend['sources'])  # å¤šå¹³å°éªŒè¯
        
        if high_performers.get('median_views', 0) > 0:
            confidence += 0.1  # æœ‰å¯é åŸºå‡†
        
        return min(0.9, round(confidence, 2))
    
    def _get_feature_importance(self, features: np.ndarray) -> Dict:
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä»…åœ¨ ML æ¨¡å¼ä¸‹ï¼‰"""
        if not self.is_trained or not hasattr(self.model, 'feature_importances_'):
            return {}
        
        importance = self.model.feature_importances_
        
        # è¿”å› top 5 é‡è¦ç‰¹å¾
        feature_scores = list(zip(self.feature_names, importance))
        feature_scores.sort(key=lambda x: x[1], reverse=True)
        
        return {
            name: round(float(score), 3) 
            for name, score in feature_scores[:5]
        }
    
    def _load_model(self, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            self.model = xgb.XGBRegressor()
            self.model.load_model(model_path)
            
            # åŠ è½½ scaler
            scaler_path = model_path.replace('.json', '_scaler.pkl')
            if os.path.exists(scaler_path):
                import pickle
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
            
            self.is_trained = True
            print(f"âœ… ML model loaded from {model_path}")
        except Exception as e:
            print(f"âš ï¸  Failed to load ML model: {e}")
            self.is_trained = False
    
    def save_training_data(self, data_point: Dict, output_dir: str = 'data/ml_training'):
        """
        ä¿å­˜è®­ç»ƒæ•°æ®ç‚¹ï¼ˆç”¨äºæœªæ¥è®­ç»ƒæ¨¡å‹ï¼‰
        
        Args:
            data_point: {
                'keyword': str,
                'channel_analysis': dict,
                'trend': dict,
                'relevance_score': float,
                'actual_views': int (å¯é€‰ï¼Œç¨åæ›´æ–°),
                'timestamp': str
            }
        """
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            filename = f"{output_dir}/training_data_{timestamp}.json"
            
            # æ·»åŠ ç‰¹å¾å‘é‡
            features = self._extract_features(
                data_point['keyword'],
                data_point['channel_analysis'],
                data_point['trend'],
                data_point.get('relevance_score', 0)
            )
            
            data_point['features'] = features.tolist()[0]
            data_point['feature_names'] = self.feature_names
            
            # ä¿å­˜
            with open(filename, 'w') as f:
                json.dump(data_point, f, indent=2)
            
            print(f"ğŸ“Š Training data saved: {filename}")
        except Exception as e:
            print(f"âš ï¸  Failed to save training data: {e}")


# å…¨å±€å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_predictor_instance = None

def get_ml_performance_predictor(model_path: Optional[str] = None) -> MLPerformancePredictor:
    """è·å–é¢„æµ‹å™¨å•ä¾‹"""
    global _predictor_instance
    if _predictor_instance is None:
        _predictor_instance = MLPerformancePredictor(model_path)
    return _predictor_instance
