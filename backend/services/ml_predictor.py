"""
Machine Learning Enhanced Predictor
ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æå‡é¢„æµ‹å‡†ç¡®æ€§
"""

from typing import List, Dict, Optional, Tuple
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning Libraries
try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold
    from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
    from sklearn.feature_selection import SelectKBest, f_regression
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
    from sklearn.linear_model import Ridge, Lasso, ElasticNet
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸  scikit-learn not available")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except (ImportError, Exception) as e:
    XGBOOST_AVAILABLE = False
    print(f"âš ï¸  XGBoost not available: {e}")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except (ImportError, Exception) as e:
    LIGHTGBM_AVAILABLE = False
    print(f"âš ï¸  LightGBM not available: {e}")


class MLPredictor:
    """
    æœºå™¨å­¦ä¹ é¢„æµ‹å™¨ - ä½¿ç”¨å¤šç§æ¨¡å‹æå‡é¢„æµ‹å‡†ç¡®æ€§
    """
    
    def __init__(self):
        self.models = {}
        # ä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
        self.scaler = RobustScaler() if SKLEARN_AVAILABLE else None
        self.is_trained = False
        self.feature_importance = {}
        self.feature_selector = None
        self.selected_features = None
        # ç”¨äºå¯¹æ•°å˜æ¢çš„æ ‡è®°
        self.use_log_transform = False
        self.y_scaler = None  # ç”¨äºç›®æ ‡å˜é‡çš„æ ‡å‡†åŒ–
        
    def extract_features(
        self,
        video: Dict,
        channel_analysis: Dict,
        trend_data: Dict,
        period_avg: float = 0
    ) -> np.ndarray:
        """
        æå–ç‰¹å¾å‘é‡
        
        ç‰¹å¾åŒ…æ‹¬ï¼š
        1. é¢‘é“ç‰¹å¾ï¼šå¹³å‡æ’­æ”¾é‡ã€ä¸­ä½æ•°æ’­æ”¾é‡ã€è§†é¢‘æ€»æ•°
        2. è¶‹åŠ¿ç‰¹å¾ï¼šçƒ­åº¦åˆ†æ•°ã€ç›¸å…³æ€§åˆ†æ•°ã€è¡¨ç°æ½œåŠ›åˆ†æ•°
        3. å†…å®¹ç‰¹å¾ï¼šæ ‡é¢˜é•¿åº¦ã€å…³é”®è¯æ•°é‡ã€å†…å®¹ä¸»é¢˜
        4. æ—¶é—´ç‰¹å¾ï¼šå‘å¸ƒæ—¶é—´ï¼ˆå°æ—¶ã€æ˜ŸæœŸï¼‰ã€å‘å¸ƒæ—¶é—´æ®µ
        5. äº’åŠ¨ç‰¹å¾ï¼šå†å²å¹³å‡äº’åŠ¨ç‡
        """
        features = []
        
        # 1. é¢‘é“ç‰¹å¾
        high_performers = channel_analysis.get('high_performers', {})
        features.append(float(high_performers.get('avg_views', 0) or 0))
        features.append(float(high_performers.get('median_views', 0) or 0))
        features.append(float(high_performers.get('total_videos', 0) or 0))
        features.append(float(period_avg or 0))
        
        # 2. è¶‹åŠ¿ç‰¹å¾
        features.append(float(trend_data.get('viral_potential', 50)))
        features.append(float(trend_data.get('relevance_score', 50)))
        features.append(float(trend_data.get('performance_score', 50)))
        features.append(float(trend_data.get('match_score', 50)))
        features.append(float(trend_data.get('growth_rate', 0)))
        
        # 3. å†…å®¹ç‰¹å¾
        title = video.get('title', '')
        description = video.get('description', '')
        features.append(float(len(title)))
        features.append(float(len(description)))
        
        # æ ‡é¢˜å…³é”®è¯æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰
        title_words = len(title.split())
        features.append(float(title_words))
        
        # å†…å®¹ä¸»é¢˜åŒ¹é…åº¦
        content_style = channel_analysis.get('content_style', {})
        primary_style = content_style.get('primary_style', 'general')
        style_score = 1.0 if primary_style != 'general' else 0.5
        features.append(float(style_score))
        
        # 4. æ—¶é—´ç‰¹å¾
        published_at = video.get('publishedAt', '')
        try:
            if isinstance(published_at, str):
                publish_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            else:
                publish_date = published_at
            features.append(float(publish_date.hour))  # å‘å¸ƒå°æ—¶
            features.append(float(publish_date.weekday()))  # æ˜ŸæœŸå‡ 
            # æ˜¯å¦å‘¨æœ«
            features.append(float(1.0 if publish_date.weekday() >= 5 else 0.0))
        except:
            features.extend([12.0, 3.0, 0.0])  # é»˜è®¤å€¼
        
        # 5. äº’åŠ¨ç‰¹å¾ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
        features.append(float(high_performers.get('avg_engagement_rate', 0) or 0))
        
        # 6. é¢‘é“è§„æ¨¡ç‰¹å¾
        subscriber_count = channel_analysis.get('target_audience', {}).get('subscriber_count', 0)
        features.append(float(subscriber_count))
        
        # 7. æ ‡é¢˜ä¼˜åŒ–ç‰¹å¾
        title_length = len(title)
        if 30 <= title_length <= 60:
            title_opt = 1.0
        elif 20 <= title_length < 30 or 60 < title_length <= 70:
            title_opt = 0.8
        else:
            title_opt = 0.5
        features.append(float(title_opt))
        
        # 8. é«˜çº§ç‰¹å¾ï¼šè§†é¢‘æ—¶é•¿ï¼ˆå¦‚æœæœ‰ï¼‰
        duration_seconds = video.get('duration', 0)
        if isinstance(duration_seconds, str):
            # è§£æISO 8601æ ¼å¼ï¼ˆå¦‚PT5M30Sï¼‰
            import re
            match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_seconds)
            if match:
                hours = int(match.group(1) or 0)
                minutes = int(match.group(2) or 0)
                seconds = int(match.group(3) or 0)
                duration_seconds = hours * 3600 + minutes * 60 + seconds
            else:
                duration_seconds = 0
        features.append(float(duration_seconds))
        
        # 9. é«˜çº§ç‰¹å¾ï¼šè§†é¢‘æ—¶é•¿ç±»åˆ«
        if duration_seconds > 0:
            if duration_seconds < 60:
                duration_category = 1  # çŸ­è§†é¢‘
            elif duration_seconds < 300:
                duration_category = 2  # ä¸­ç­‰è§†é¢‘
            elif duration_seconds < 600:
                duration_category = 3  # é•¿è§†é¢‘
            else:
                duration_category = 4  # è¶…é•¿è§†é¢‘
        else:
            duration_category = 2  # é»˜è®¤ä¸­ç­‰
        features.append(float(duration_category))
        
        # 10. é«˜çº§ç‰¹å¾ï¼šæ ‡é¢˜æƒ…æ„Ÿå€¾å‘ï¼ˆç®€å•ä¼°ç®—ï¼‰
        title_lower = title.lower()
        positive_words = ['best', 'great', 'amazing', 'awesome', 'top', 'win', 'success']
        negative_words = ['worst', 'bad', 'fail', 'lose', 'terrible', 'awful']
        positive_count = sum(1 for word in positive_words if word in title_lower)
        negative_count = sum(1 for word in negative_words if word in title_lower)
        sentiment_score = (positive_count - negative_count) / max(1, len(title.split()))
        features.append(float(sentiment_score))
        
        # 11. é«˜çº§ç‰¹å¾ï¼šæ ‡é¢˜åŒ…å«æ•°å­—
        has_numbers = 1.0 if any(char.isdigit() for char in title) else 0.0
        features.append(float(has_numbers))
        
        # 12. é«˜çº§ç‰¹å¾ï¼šæ ‡é¢˜åŒ…å«é—®å·
        has_question = 1.0 if '?' in title else 0.0
        features.append(float(has_question))
        
        # 13. é«˜çº§ç‰¹å¾ï¼šæ ‡é¢˜åŒ…å«æ„Ÿå¹å·
        has_exclamation = 1.0 if '!' in title else 0.0
        features.append(float(has_exclamation))
        
        # 14. é«˜çº§ç‰¹å¾ï¼šé¢‘é“å¢é•¿è¶‹åŠ¿ï¼ˆåŸºäºè§†é¢‘æ€»æ•°ï¼‰
        total_videos = high_performers.get('total_videos', 0)
        if total_videos > 0:
            # ä¼°ç®—é¢‘é“å¹´é¾„ï¼ˆå‡è®¾æ¯å‘¨å‘å¸ƒ1ä¸ªè§†é¢‘ï¼‰
            estimated_age_weeks = total_videos
            growth_trend = min(1.0, estimated_age_weeks / 100)  # å½’ä¸€åŒ–åˆ°0-1
        else:
            growth_trend = 0.0
        features.append(float(growth_trend))
        
        # 15. é«˜çº§ç‰¹å¾ï¼šæ’­æ”¾é‡ç¨³å®šæ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
        avg_views = high_performers.get('avg_views', 0)
        median_views = high_performers.get('median_views', 0)
        if avg_views > 0 and median_views > 0:
            # ä½¿ç”¨ä¸­ä½æ•°å’Œå¹³å‡å€¼çš„å·®å¼‚ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡
            stability = 1.0 - abs(avg_views - median_views) / max(avg_views, median_views)
        else:
            stability = 0.5
        features.append(float(stability))
        
        # 16. é«˜çº§ç‰¹å¾ï¼šè¶‹åŠ¿å¢é•¿ç‡ï¼ˆå½’ä¸€åŒ–ï¼‰
        growth_rate = trend_data.get('growth_rate', 0)
        normalized_growth = min(1.0, max(0.0, (growth_rate + 100) / 200))  # å½’ä¸€åŒ–åˆ°0-1
        features.append(float(normalized_growth))
        
        # 17. é«˜çº§ç‰¹å¾ï¼šç»¼åˆåŒ¹é…åˆ†æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
        match_score = trend_data.get('match_score', 50)
        normalized_match = match_score / 100.0
        features.append(float(normalized_match))
        
        # 18. é«˜çº§ç‰¹å¾ï¼šçƒ­åº¦ä¸ç›¸å…³æ€§çš„äº¤äº’
        viral_potential = trend_data.get('viral_potential', 50)
        relevance_score = trend_data.get('relevance_score', 50)
        interaction = (viral_potential / 100.0) * (relevance_score / 100.0)
        features.append(float(interaction))
        
        return np.array(features, dtype=np.float32)
    
    def train_models(
        self,
        X: np.ndarray,
        y: np.ndarray,
        test_size: float = 0.2,
        use_cross_validation: bool = True,
        cv_folds: int = 5
    ) -> Dict:
        """
        è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹ - ä¼˜åŒ–ç‰ˆï¼ˆç¡®ä¿è·¨é¢‘é“ä¸€è‡´æ€§ï¼‰
        
        æ”¹è¿›ï¼š
        1. å¼‚å¸¸å€¼å¤„ç†
        2. ç‰¹å¾é€‰æ‹©
        3. è¶…å‚æ•°è°ƒä¼˜
        4. é›†æˆå­¦ä¹ 
        5. äº¤å‰éªŒè¯ï¼ˆç¡®ä¿è¯„ä¼°ä¸€è‡´æ€§ï¼‰
        6. å¯¹æ•°å˜æ¢ï¼ˆå‡å°‘æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼‰
        7. è‡ªé€‚åº”è¶…å‚æ•°ï¼ˆæ ¹æ®æ•°æ®ç‰¹å¾è°ƒæ•´ï¼‰
        """
        if not SKLEARN_AVAILABLE:
            return {'error': 'scikit-learn not available'}
        
        # åˆ†ææ•°æ®ç‰¹å¾ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨å¯¹æ•°å˜æ¢
        y_mean = np.mean(y)
        y_std = np.std(y)
        y_cv = y_std / y_mean if y_mean > 0 else 0  # å˜å¼‚ç³»æ•°
        
        # å¦‚æœå˜å¼‚ç³»æ•° > 0.5ï¼Œä½¿ç”¨å¯¹æ•°å˜æ¢ï¼ˆå‡å°‘ä¸åŒé¢‘é“æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼‰
        self.use_log_transform = y_cv > 0.5 and y_mean > 0
        if self.use_log_transform:
            print(f"ğŸ“Š æ•°æ®å˜å¼‚ç³»æ•°: {y_cv:.2f}ï¼Œä½¿ç”¨å¯¹æ•°å˜æ¢ä»¥å‡å°‘åˆ†å¸ƒå·®å¼‚")
            y_clean = np.log1p(y)  # log1p = log(1+x)ï¼Œé¿å…log(0)
            X_clean = X.copy()
        else:
            X_clean = X
            y_clean = y.copy()
        
        # å¼‚å¸¸å€¼å¤„ç†ï¼šä½¿ç”¨æ›´å®½æ¾çš„æ ‡å‡†ï¼ˆä¿ç•™æ›´å¤šæ•°æ®ï¼‰
        # åªç§»é™¤æç«¯å¼‚å¸¸å€¼ï¼ˆè¶…è¿‡3ä¸ªæ ‡å‡†å·®ï¼‰
        if not self.use_log_transform:
            y_mean_clean = np.mean(y_clean)
            y_std_clean = np.std(y_clean)
            if y_std_clean > 0:
                lower_bound = max(0, y_mean_clean - 3 * y_std_clean)
                upper_bound = y_mean_clean + 3 * y_std_clean
                mask = (y_clean >= lower_bound) & (y_clean <= upper_bound)
                X_clean = X_clean[mask]
                y_clean = y_clean[mask]
        
        if len(X_clean) < 10:
            # å¦‚æœæ¸…ç†åæ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
            X_clean = X
            y_clean = np.log1p(y) if self.use_log_transform else y
        
        # è‡ªé€‚åº”è¶…å‚æ•°ï¼šæ ¹æ®æ•°æ®é‡è°ƒæ•´
        n_samples = len(X_clean)
        if n_samples < 30:
            # å°æ•°æ®é›†ï¼šä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
            max_depth_rf = 8
            max_depth_gb = 4
            n_estimators = 100
        elif n_samples < 50:
            # ä¸­ç­‰æ•°æ®é›†
            max_depth_rf = 10
            max_depth_gb = 5
            n_estimators = 120
        else:
            # å¤§æ•°æ®é›†
            max_depth_rf = 12
            max_depth_gb = 6
            n_estimators = 150
        
        print(f"ğŸ“Š è‡ªé€‚åº”å‚æ•°: n_samples={n_samples}, max_depth_rf={max_depth_rf}, max_depth_gb={max_depth_gb}")
        
        # åˆå§‹åŒ–K-Foldï¼ˆå¦‚æœéœ€è¦ï¼‰
        kf = None
        if use_cross_validation and n_samples >= 20:
            # ä½¿ç”¨K-Foldäº¤å‰éªŒè¯è·å¾—æ›´ç¨³å¥çš„è¯„ä¼°
            kf = KFold(n_splits=min(cv_folds, n_samples // 5), shuffle=True, random_state=42)
            print(f"ğŸ“Š ä½¿ç”¨ {kf.n_splits}-Fold äº¤å‰éªŒè¯")
        
        # æ•°æ®åˆ†å‰²ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        split_idx = int(len(X_clean) * (1 - test_size))
        X_train = X_clean[:split_idx]
        X_test = X_clean[split_idx:]
        y_train = y_clean[:split_idx]
        y_test = y_clean[split_idx:]
        
        # ç¡®ä¿æµ‹è¯•é›†è‡³å°‘æœ‰10ä¸ªæ ·æœ¬ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
        min_test_samples = 10
        if len(X_test) < min_test_samples and len(X_clean) >= min_test_samples * 2:
            # å¦‚æœæµ‹è¯•é›†å¤ªå°ä¸”æ•°æ®è¶³å¤Ÿï¼Œä»è®­ç»ƒé›†ä¸­å†åˆ†ä¸€äº›
            additional_test = min_test_samples - len(X_test)
            if additional_test <= len(X_train):
                X_test = np.concatenate([X_test, X_train[-additional_test:]])
                y_test = np.concatenate([y_test, y_train[-additional_test:]])
                X_train = X_train[:-additional_test]
                y_train = y_train[:-additional_test]
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {len(X_train)} ä¸ªæ ·æœ¬, æµ‹è¯•é›† {len(X_test)} ä¸ªæ ·æœ¬")
        if not self.use_log_transform:
            print(f"   æµ‹è¯•é›†æ’­æ”¾é‡èŒƒå›´: {y_test.min():.0f} - {y_test.max():.0f}, å‡å€¼: {y_test.mean():.0f}")
        else:
            print(f"   æµ‹è¯•é›†ï¼ˆå¯¹æ•°å˜æ¢åï¼‰èŒƒå›´: {y_test.min():.3f} - {y_test.max():.3f}, å‡å€¼: {y_test.mean():.3f}")
        
        # ç‰¹å¾é€‰æ‹©ï¼šé€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆæ›´ä¿å®ˆï¼Œä¿ç•™æ›´å¤šç‰¹å¾ï¼‰
        if len(X_train) > 15 and X_train.shape[1] > 10:
            try:
                # é€‰æ‹©å‰kä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆk = min(ç‰¹å¾æ•°, 20)ï¼Œä¿ç•™æ›´å¤šç‰¹å¾ï¼‰
                k = min(X_train.shape[1], 20)
                self.feature_selector = SelectKBest(score_func=f_regression, k=k)
                X_train_selected = self.feature_selector.fit_transform(X_train, y_train)
                X_test_selected = self.feature_selector.transform(X_test)
                self.selected_features = self.feature_selector.get_support()
                print(f"âœ… ç‰¹å¾é€‰æ‹©ï¼šä» {X_train.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {k} ä¸ªæœ€é‡è¦çš„")
            except Exception as e:
                print(f"âš ï¸  ç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾: {e}")
                # å¦‚æœç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾
                X_train_selected = X_train
                X_test_selected = X_test
                self.selected_features = None
        else:
            X_train_selected = X_train
            X_test_selected = X_test
            self.selected_features = None
        
        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥ï¼‰
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        results = {}
        
        # 1. éšæœºæ£®æ—ï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…å‚æ•°ï¼‰
        try:
            rf_model = RandomForestRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth_rf,
                min_samples_split=5,
                min_samples_leaf=3,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if kf is not None:
                cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=kf, scoring='r2', n_jobs=-1)
                cv_mae_scores = -cross_val_score(rf_model, X_train_scaled, y_train, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1)
                print(f"   RF CV RÂ²: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
            
            rf_model.fit(X_train_scaled, y_train)
            rf_pred = rf_model.predict(X_test_scaled)
            
            # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
            if self.use_log_transform:
                rf_pred = np.expm1(rf_pred)
                y_test_orig = np.expm1(y_test)
            else:
                y_test_orig = y_test
            
            rf_mae = mean_absolute_error(y_test_orig, rf_pred)
            rf_r2 = r2_score(y_test_orig, rf_pred)
            rf_mape = mean_absolute_percentage_error(y_test_orig, rf_pred) * 100
            rf_rmse = np.sqrt(mean_squared_error(y_test_orig, rf_pred))
            
            self.models['random_forest'] = rf_model
            results['random_forest'] = {
                'mae': float(rf_mae),
                'mape': float(rf_mape),
                'rmse': float(rf_rmse),
                'r2': float(rf_r2),
                'feature_importance': {
                    f'feature_{i}': float(imp)
                    for i, imp in enumerate(rf_model.feature_importances_)
                }
            }
        except Exception as e:
            print(f"Random Forest training failed: {e}")
        
        # 2. æ¢¯åº¦æå‡ï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…å‚æ•°ï¼‰
        try:
            gb_model = GradientBoostingRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth_gb,
                learning_rate=0.08,
                min_samples_split=5,
                min_samples_leaf=3,
                subsample=0.85,
                random_state=42
            )
            # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if kf is not None:
                cv_scores = cross_val_score(gb_model, X_train_scaled, y_train, cv=kf, scoring='r2', n_jobs=-1)
                print(f"   GB CV RÂ²: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
            
            gb_model.fit(X_train_scaled, y_train)
            gb_pred = gb_model.predict(X_test_scaled)
            
            # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
            if self.use_log_transform:
                gb_pred = np.expm1(gb_pred)
                y_test_orig = np.expm1(y_test)
            else:
                y_test_orig = y_test
            
            gb_mae = mean_absolute_error(y_test_orig, gb_pred)
            gb_r2 = r2_score(y_test_orig, gb_pred)
            gb_mape = mean_absolute_percentage_error(y_test_orig, gb_pred) * 100
            gb_rmse = np.sqrt(mean_squared_error(y_test_orig, gb_pred))
            
            self.models['gradient_boosting'] = gb_model
            results['gradient_boosting'] = {
                'mae': float(gb_mae),
                'mape': float(gb_mape),
                'rmse': float(gb_rmse),
                'r2': float(gb_r2),
                'feature_importance': {
                    f'feature_{i}': float(imp)
                    for i, imp in enumerate(gb_model.feature_importances_)
                }
            }
        except Exception as e:
            print(f"Gradient Boosting training failed: {e}")
        
        # 3. XGBoostï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…å‚æ•°ï¼‰
        if XGBOOST_AVAILABLE:
            try:
                xgb_model = xgb.XGBRegressor(
                    n_estimators=n_estimators,
                    max_depth=max_depth_gb,
                    learning_rate=0.08,
                    min_child_weight=5,
                    subsample=0.85,
                    colsample_bytree=0.85,
                    gamma=0.2,
                    reg_alpha=0.2,
                    reg_lambda=1.5,
                    objective='reg:squarederror',
                    random_state=42,
                    n_jobs=-1
                )
                xgb_model.fit(X_train_scaled, y_train)
                xgb_pred = xgb_model.predict(X_test_scaled)
                
                # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
                if self.use_log_transform:
                    xgb_pred = np.expm1(xgb_pred)
                    y_test_orig = np.expm1(y_test)
                else:
                    y_test_orig = y_test
                
                xgb_mae = mean_absolute_error(y_test_orig, xgb_pred)
                xgb_r2 = r2_score(y_test_orig, xgb_pred)
                xgb_mape = mean_absolute_percentage_error(y_test_orig, xgb_pred) * 100
                xgb_rmse = np.sqrt(mean_squared_error(y_test_orig, xgb_pred))
                
                self.models['xgboost'] = xgb_model
                results['xgboost'] = {
                    'mae': float(xgb_mae),
                    'mape': float(xgb_mape),
                    'rmse': float(xgb_rmse),
                    'r2': float(xgb_r2),
                    'feature_importance': {
                        f'feature_{i}': float(imp)
                        for i, imp in enumerate(xgb_model.feature_importances_)
                    }
                }
            except Exception as e:
                print(f"XGBoost training failed: {e}")
        
        # 4. LightGBMï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…å‚æ•°ï¼‰
        if LIGHTGBM_AVAILABLE:
            try:
                lgb_model = lgb.LGBMRegressor(
                    n_estimators=n_estimators,
                    max_depth=max_depth_gb + 1,  # LightGBMé€šå¸¸éœ€è¦ç¨æ·±çš„æ ‘
                    learning_rate=0.08,
                    num_leaves=25,
                    min_child_samples=25,
                    subsample=0.85,
                    colsample_bytree=0.85,
                    reg_alpha=0.2,
                    reg_lambda=1.5,
                    random_state=42,
                    n_jobs=-1,
                    verbose=-1
                )
                lgb_model.fit(X_train_scaled, y_train)
                lgb_pred = lgb_model.predict(X_test_scaled)
                
                # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
                if self.use_log_transform:
                    lgb_pred = np.expm1(lgb_pred)
                    y_test_orig = np.expm1(y_test)
                else:
                    y_test_orig = y_test
                
                lgb_mae = mean_absolute_error(y_test_orig, lgb_pred)
                lgb_r2 = r2_score(y_test_orig, lgb_pred)
                lgb_mape = mean_absolute_percentage_error(y_test_orig, lgb_pred) * 100
                lgb_rmse = np.sqrt(mean_squared_error(y_test_orig, lgb_pred))
                
                self.models['lightgbm'] = lgb_model
                results['lightgbm'] = {
                    'mae': float(lgb_mae),
                    'mape': float(lgb_mape),
                    'rmse': float(lgb_rmse),
                    'r2': float(lgb_r2),
                    'feature_importance': {
                        f'feature_{i}': float(imp)
                        for i, imp in enumerate(lgb_model.feature_importances_)
                    }
                }
            except Exception as e:
                print(f"LightGBM training failed: {e}")
        
        # 5. é›†æˆæ¨¡å‹ï¼šStackingï¼ˆå¦‚æœè‡³å°‘æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼‰
        if len(self.models) >= 2:
            try:
                # ä½¿ç”¨å‰ä¸¤ä¸ªæœ€ä½³æ¨¡å‹ä½œä¸ºåŸºæ¨¡å‹
                base_models = list(self.models.items())[:2]
                base_estimators = [(name, model) for name, model in base_models]
                
                # ä½¿ç”¨Ridgeä½œä¸ºå…ƒæ¨¡å‹
                meta_model = Ridge(alpha=1.0)
                stacking_model = StackingRegressor(
                    estimators=base_estimators,
                    final_estimator=meta_model,
                    cv=3,
                    n_jobs=-1
                )
                stacking_model.fit(X_train_scaled, y_train)
                stacking_pred = stacking_model.predict(X_test_scaled)
                
                # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
                if self.use_log_transform:
                    stacking_pred = np.expm1(stacking_pred)
                    y_test_orig = np.expm1(y_test)
                else:
                    y_test_orig = y_test
                
                stacking_mae = mean_absolute_error(y_test_orig, stacking_pred)
                stacking_r2 = r2_score(y_test_orig, stacking_pred)
                stacking_mape = mean_absolute_percentage_error(y_test_orig, stacking_pred) * 100
                stacking_rmse = np.sqrt(mean_squared_error(y_test_orig, stacking_pred))
                
                self.models['stacking'] = stacking_model
                results['stacking'] = {
                    'mae': float(stacking_mae),
                    'mape': float(stacking_mape),
                    'rmse': float(stacking_rmse),
                    'r2': float(stacking_r2)
                }
            except Exception as e:
                print(f"Stacking model training failed: {e}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆç»¼åˆè€ƒè™‘RÂ²ã€MAPEå’Œç¨³å®šæ€§ï¼Œç¡®ä¿è·¨é¢‘é“ä¸€è‡´æ€§ï¼‰
        # ç›®æ ‡ï¼šRÂ² > 0.5, MAPE < 30%ï¼ˆæ›´å®½æ¾ä½†ä¸€è‡´çš„æ ‡å‡†ï¼‰
        best_model_name = None
        best_score = -float('inf')
        
        for model_name, metrics in results.items():
            r2 = metrics.get('r2', 0)
            mape = metrics.get('mape', 100)
            mae = metrics.get('mae', float('inf'))
            rmse = metrics.get('rmse', float('inf'))
            
            # è®¡ç®—ç›¸å¯¹MAEï¼ˆç›¸å¯¹äºå‡å€¼ï¼‰ï¼Œç¡®ä¿è·¨é¢‘é“ä¸€è‡´æ€§
            y_mean_orig = np.expm1(y_test.mean()) if self.use_log_transform else y_test.mean()
            relative_mae = (mae / y_mean_orig) * 100 if y_mean_orig > 0 else 100
            
            # ç»¼åˆè¯„åˆ†ï¼šRÂ²æƒé‡50%ï¼ŒMAPEæƒé‡30%ï¼Œç›¸å¯¹MAEæƒé‡20%ï¼ˆæ›´å¹³è¡¡ï¼‰
            # ä½¿ç”¨æ›´ä¸€è‡´çš„æ ‡å‡†ï¼Œä¸åå‘ç‰¹å®šæ•°æ®åˆ†å¸ƒ
            bonus = 0
            if r2 >= 0.5:  # é™ä½é˜ˆå€¼ï¼Œæ›´ä¸€è‡´
                bonus += 0.1
            if mape <= 30:  # æ›´å®½æ¾çš„MAPEæ ‡å‡†
                bonus += 0.1
            if relative_mae <= 20:  # ç›¸å¯¹MAE <= 20%
                bonus += 0.05
            
            # MAPEå½’ä¸€åŒ–åˆ°0-1ï¼ˆå‡è®¾æœ€å¤§MAPEä¸º100%ï¼‰
            mape_score = max(0, 1 - min(mape, 100) / 100)
            # ç›¸å¯¹MAEå½’ä¸€åŒ–
            relative_mae_score = max(0, 1 - min(relative_mae, 100) / 100)
            
            # æ›´å¹³è¡¡çš„è¯„åˆ†ï¼šRÂ² 50%, MAPE 30%, ç›¸å¯¹MAE 20%
            score = r2 * 0.5 + mape_score * 0.3 + relative_mae_score * 0.2 + bonus
            
            if score > best_score:
                best_score = score
                best_model_name = model_name
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ¨¡å‹ï¼Œä½¿ç”¨RÂ²æœ€é«˜çš„
        if best_model_name is None:
            best_r2 = -float('inf')
            for model_name, metrics in results.items():
                if metrics.get('r2', 0) > best_r2:
                    best_r2 = metrics.get('r2', 0)
                    best_model_name = model_name
        
        print(f"âœ… æœ€ä½³æ¨¡å‹é€‰æ‹©: {best_model_name}, RÂ²={results.get(best_model_name, {}).get('r2', 0):.3f}, MAPE={results.get(best_model_name, {}).get('mape', 0):.1f}%")
        
        self.best_model_name = best_model_name
        self.is_trained = True
        
        best_metrics = results.get(best_model_name, {})
        results['best_model'] = best_model_name
        results['best_r2'] = best_metrics.get('r2', 0)
        results['best_mape'] = best_metrics.get('mape', 0)
        results['best_mae'] = best_metrics.get('mae', 0)
        results['best_rmse'] = best_metrics.get('rmse', 0)
        
        return results
    
    def predict(
        self,
        video: Dict,
        channel_analysis: Dict,
        trend_data: Dict,
        period_avg: float = 0
    ) -> Dict:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        """
        if not self.is_trained or not self.best_model_name:
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            return self._fallback_predict(video, channel_analysis, trend_data, period_avg)
        
        # æå–ç‰¹å¾
        features = self.extract_features(video, channel_analysis, trend_data, period_avg)
        features = features.reshape(1, -1)
        
        # ç‰¹å¾é€‰æ‹©ï¼ˆå¦‚æœå·²è®­ç»ƒï¼‰
        if self.feature_selector is not None:
            features = self.feature_selector.transform(features)
        
        # æ ‡å‡†åŒ–
        features_scaled = self.scaler.transform(features)
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
        best_model = self.models.get(self.best_model_name)
        if best_model:
            prediction = best_model.predict(features_scaled)[0]
            
            # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
            if self.use_log_transform:
                prediction = np.expm1(prediction)
            
            prediction = max(500, int(prediction))  # ç¡®ä¿æœ€å°å€¼
            
            # ä½¿ç”¨é›†æˆé¢„æµ‹ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼‰
            ensemble_predictions = []
            model_weights = []
            
            for model_name, model in self.models.items():
                try:
                    pred = model.predict(features_scaled)[0]
                    # å¦‚æœä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å°ºåº¦
                    if self.use_log_transform:
                        pred = np.expm1(pred)
                    ensemble_predictions.append(pred)
                    
                    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ†é…æƒé‡
                    if model_name == self.best_model_name:
                        model_weights.append(0.4)  # æœ€ä½³æ¨¡å‹æƒé‡æœ€é«˜
                    elif model_name == 'stacking':
                        model_weights.append(0.3)  # Stackingæ¨¡å‹æƒé‡è¾ƒé«˜
                    else:
                        model_weights.append(0.1)  # å…¶ä»–æ¨¡å‹æƒé‡è¾ƒä½
                except:
                    pass
            
            if ensemble_predictions and len(ensemble_predictions) > 1:
                # å½’ä¸€åŒ–æƒé‡
                total_weight = sum(model_weights)
                if total_weight > 0:
                    model_weights = [w / total_weight for w in model_weights]
                    # ä½¿ç”¨åŠ æƒå¹³å‡
                    ensemble_pred = sum(pred * weight for pred, weight in zip(ensemble_predictions, model_weights))
                    prediction = max(500, int(ensemble_pred))
            
            return {
                'predicted_views': prediction,
                'model_used': self.best_model_name,
                'confidence': 0.8 if self.is_trained else 0.5
            }
        else:
            return self._fallback_predict(video, channel_analysis, trend_data, period_avg)
    
    def _fallback_predict(
        self,
        video: Dict,
        channel_analysis: Dict,
        trend_data: Dict,
        period_avg: float
    ) -> Dict:
        """
        å›é€€åˆ°ä¼ ç»Ÿé¢„æµ‹æ–¹æ³•
        """
        high_performers = channel_analysis.get('high_performers', {})
        median_views = high_performers.get('median_views')
        avg_views = high_performers.get('avg_views')
        
        if median_views and avg_views:
            base_views = int(median_views * 0.7 + avg_views * 0.3)
        elif median_views:
            base_views = int(median_views)
        elif avg_views:
            base_views = int(avg_views)
        else:
            base_views = int(period_avg) if period_avg > 0 else 10000
        
        viral_potential = trend_data.get('viral_potential', 50)
        if viral_potential >= 90:
            multiplier = 2.2 + (viral_potential - 90) * 0.03
        elif viral_potential >= 70:
            multiplier = 1.6 + (viral_potential - 70) * 0.03
        elif viral_potential >= 50:
            multiplier = 1.2 + (viral_potential - 50) * 0.02
        else:
            multiplier = 0.9 + (viral_potential / 50) * 0.3
        
        predicted_views = int(base_views * multiplier)
        return {
            'predicted_views': max(500, predicted_views),
            'model_used': 'fallback',
            'confidence': 0.5
        }


# å…¨å±€å®ä¾‹
ml_predictor = MLPredictor()
