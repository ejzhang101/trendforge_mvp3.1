"""
Enhanced YouTube Channel Analyzer with Deep Content Analysis
MVP 2.0 - Intelligent Topic Extraction & Video Content Analysis
"""

import re
from typing import List, Dict, Optional, Tuple
from collections import Counter
import numpy as np

# NLP Libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Try to import spaCy (optional for lightweight deployment)
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    spacy = None
    nlp = None
    print("‚ö†Ô∏è  spaCy not available. Using NLTK-only mode. Install: pip install spacy")

# Machine Learning (optional for lightweight deployment)
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    TfidfVectorizer = None
    KMeans = None
    print("‚ö†Ô∏è  scikit-learn not available. Using NLTK-only mode. Install: pip install scikit-learn")

# YouTube Transcript (optional)
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    YOUTUBE_TRANSCRIPT_AVAILABLE = True
except ImportError:
    YOUTUBE_TRANSCRIPT_AVAILABLE = False
    YouTubeTranscriptApi = None
    print("‚ö†Ô∏è  youtube-transcript-api not available. Transcript analysis disabled.")

# Semantic Keywords (optional)
try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    KEYBERT_AVAILABLE = False
    KeyBERT = None
    print("‚ö†Ô∏è  KeyBERT not available. Install: pip install keybert")

# Download required NLTK data
# NLTK 3.8.1+ uses punkt_tab instead of punkt
import os

# Set NLTK data path
nltk_data_path = os.getenv('NLTK_DATA', '/usr/local/share/nltk_data')
if nltk_data_path not in nltk.data.path:
    nltk.data.path.append(nltk_data_path)

# Check and download NLTK data
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        # Download both for compatibility
        print("üì¶ Downloading NLTK punkt resources...")
        try:
            nltk.download('punkt', quiet=True)
        except Exception as e:
            print(f"‚ö†Ô∏è punkt download failed: {e}")
        try:
            nltk.download('punkt_tab', quiet=True)
        except Exception as e:
            print(f"‚ö†Ô∏è punkt_tab download failed: {e}")
        try:
            nltk.download('stopwords', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('wordnet', quiet=True)
        except Exception as e:
            print(f"‚ö†Ô∏è Other NLTK data download failed: {e}")

# Load spaCy model (if available)
if SPACY_AVAILABLE:
    try:
        nlp = spacy.load('en_core_web_sm')
    except OSError:
        try:
            print("‚ö†Ô∏è  Downloading spaCy model...")
            import subprocess
            subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'], check=False)
            nlp = spacy.load('en_core_web_sm')
        except:
            nlp = None
            print("‚ö†Ô∏è  Failed to load spaCy model. Using NLTK-only mode.")
else:
    nlp = None


class EnhancedContentAnalyzer:
    """
    Advanced content analyzer with NLP capabilities
    """
    
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # Add custom stop words
        self.stop_words.update([
            'video', 'youtube', 'like', 'subscribe', 'comment', 
            'channel', 'watch', 'episode', 'part', 'new', 'best',
            'top', 'first', 'last', 'full', 'official'
        ])
        
        if KEYBERT_AVAILABLE:
            self.keybert = KeyBERT()
        else:
            self.keybert = None
    
    def extract_topics_from_titles(self, titles: List[str]) -> List[Dict]:
        """
        Extract meaningful topics from video titles using NLP
        
        Returns:
            List of topics with scores and related terms
        """
        # Combine all titles
        combined_text = ' '.join(titles)
        
        # Method 1: TF-IDF + Noun Extraction (or NLTK-only fallback)
        tfidf_topics = self._extract_tfidf_topics(titles)
        
        # Method 2: Named Entity Recognition (spaCy or NLTK fallback)
        ner_topics = self._extract_named_entities(combined_text)
        
        # Method 3: KeyBERT semantic keywords (if available)
        if self.keybert:
            semantic_topics = self._extract_semantic_keywords(combined_text)
        else:
            semantic_topics = []
        
        # Merge and rank topics
        all_topics = self._merge_topics(tfidf_topics, ner_topics, semantic_topics)
        
        return all_topics[:15]  # Top 15 topics
    
    def _extract_nltk_topics(self, titles: List[str]) -> List[Dict]:
        """
        Extract topics using NLTK only (lightweight mode)
        """
        # Combine all titles
        combined_text = ' '.join(titles).lower()
        
        # Tokenize and filter
        tokens = word_tokenize(combined_text)
        tokens = [t for t in tokens if t.isalnum() and t not in self.stop_words and len(t) > 2]
        
        # POS tagging
        pos_tags = pos_tag(tokens)
        nouns = [word for word, pos in pos_tags if pos.startswith('NN')]
        
        # Count frequency
        topic_counter = Counter(nouns)
        
        topics = []
        for topic, count in topic_counter.most_common(15):
            topics.append({
                'topic': topic,
                'score': count / len(nouns) if nouns else 0,
                'type': 'nltk',
                'frequency': count
            })
        
        return topics
    
    def _extract_tfidf_topics(self, titles: List[str]) -> List[Dict]:
        """
        Use TF-IDF to extract important terms, filtered by POS tags
        Falls back to NLTK-only mode if spaCy or sklearn unavailable
        """
        # Check if required libraries are available
        if not SKLEARN_AVAILABLE:
            return self._extract_nltk_topics(titles)
        
        # Preprocess: extract nouns and proper nouns only
        processed_titles = []
        for title in titles:
            if SPACY_AVAILABLE and nlp:
                # Use spaCy if available
                try:
                    doc = nlp(title.lower())
                    tokens = [
                        token.lemma_ for token in doc 
                        if token.pos_ in ['NOUN', 'PROPN'] 
                        and token.text not in self.stop_words
                        and len(token.text) > 2
                        and not token.is_punct
                    ]
                    processed_titles.append(' '.join(tokens))
                except:
                    # Fallback to NLTK if spaCy fails
                    tokens = word_tokenize(title.lower())
                    pos_tags = pos_tag(tokens)
                    nouns = [word for word, pos in pos_tags 
                            if pos.startswith('NN') 
                            and word not in self.stop_words 
                            and len(word) > 2]
                    processed_titles.append(' '.join(nouns))
            else:
                # Fallback to NLTK
                tokens = word_tokenize(title.lower())
                pos_tags = pos_tag(tokens)
                nouns = [word for word, pos in pos_tags 
                        if pos.startswith('NN') 
                        and word not in self.stop_words 
                        and len(word) > 2]
                processed_titles.append(' '.join(nouns))
        
        if not any(processed_titles):
            return self._extract_nltk_topics(titles)
        
        # TF-IDF
        try:
            vectorizer = TfidfVectorizer(
                max_features=30,
                ngram_range=(1, 3),  # Include bigrams and trigrams
                min_df=1,
                max_df=0.8
            )
            
            tfidf_matrix = vectorizer.fit_transform(processed_titles)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calculate average TF-IDF scores
            avg_scores = tfidf_matrix.mean(axis=0).A1
            topics = [
                {
                    'topic': feature_names[i],
                    'score': float(avg_scores[i]),
                    'type': 'tfidf',
                    'frequency': self._count_occurrences(feature_names[i], titles)
                }
                for i in avg_scores.argsort()[-15:][::-1]
            ]
            
            return topics
        except Exception as e:
            print(f"‚ö†Ô∏è  TF-IDF extraction error: {e}, falling back to NLTK")
            return self._extract_nltk_topics(titles)
    
    def _extract_proper_nouns_nltk(self, text: str) -> List[Dict]:
        """
        Extract proper nouns using NLTK (fallback when spaCy unavailable)
        """
        try:
            # Tokenize and POS tag
            tokens = word_tokenize(text)
            pos_tags = pos_tag(tokens)
            
            # Extract proper nouns (NNP, NNPS)
            proper_nouns = [
                word.lower() 
                for word, pos in pos_tags 
                if pos in ['NNP', 'NNPS'] 
                and word.isalnum() 
                and len(word) > 2
                and word.lower() not in self.stop_words
            ]
            
            # Count frequency
            entity_counter = Counter(proper_nouns)
            
            entities = []
            for entity, count in entity_counter.most_common(10):
                entities.append({
                    'topic': entity,
                    'score': count / len(proper_nouns) if proper_nouns else 0,
                    'type': 'entity',
                    'frequency': count
                })
            
            return entities
        except Exception as e:
            print(f"‚ö†Ô∏è  NLTK proper noun extraction error: {e}")
            return []
    
    def _extract_named_entities(self, text: str) -> List[Dict]:
        """
        Extract named entities (brands, products, technologies, etc.)
        Uses spaCy if available, falls back to NLTK proper noun extraction
        """
        if not SPACY_AVAILABLE or not nlp:
            return self._extract_proper_nouns_nltk(text)
        
        try:
            doc = nlp(text)
            entities = []
            
            entity_counter = Counter()
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PRODUCT', 'GPE', 'PERSON', 'WORK_OF_ART', 'EVENT']:
                    entity_counter[ent.text.lower()] += 1
            
            for entity, count in entity_counter.most_common(10):
                entities.append({
                    'topic': entity,
                    'score': count / len(doc.ents) if doc.ents else 0,
                    'type': 'entity',
                    'frequency': count
                })
            
            return entities
        except Exception as e:
            print(f"‚ö†Ô∏è  spaCy NER error: {e}, falling back to NLTK")
            return self._extract_proper_nouns_nltk(text)
    
    def _extract_semantic_keywords(self, text: str) -> List[Dict]:
        """
        Use KeyBERT for semantic keyword extraction
        """
        if not self.keybert or not text.strip():
            return []
        
        try:
            keywords = self.keybert.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 3),
                stop_words='english',
                top_n=10,
                use_maxsum=True,
                diversity=0.7
            )
            
            return [
                {
                    'topic': kw[0],
                    'score': float(kw[1]),
                    'type': 'semantic',
                    'frequency': self._count_occurrences(kw[0], text)
                }
                for kw in keywords
            ]
        except:
            return []
    
    def _merge_topics(self, *topic_lists) -> List[Dict]:
        """
        Merge topics from different methods and rank by composite score
        """
        topic_map = {}
        
        for topics in topic_lists:
            for topic_data in topics:
                topic = topic_data['topic']
                score = topic_data['score']
                
                if topic in topic_map:
                    # Average scores from different methods
                    topic_map[topic]['score'] = (topic_map[topic]['score'] + score) / 2
                    topic_map[topic]['methods'].append(topic_data['type'])
                else:
                    topic_map[topic] = {
                        'topic': topic,
                        'score': score,
                        'frequency': topic_data.get('frequency', 1),
                        'methods': [topic_data['type']]
                    }
        
        # Boost score for topics found by multiple methods
        for topic_data in topic_map.values():
            method_bonus = len(topic_data['methods']) * 0.2
            topic_data['score'] = topic_data['score'] * (1 + method_bonus)
        
        # Sort by composite score
        ranked_topics = sorted(
            topic_map.values(),
            key=lambda x: (x['score'], x['frequency']),
            reverse=True
        )
        
        return ranked_topics
    
    def _count_occurrences(self, term: str, texts) -> int:
        """Count how many times a term appears across texts"""
        if isinstance(texts, str):
            texts = [texts]
        
        count = 0
        term_lower = term.lower()
        for text in texts:
            count += text.lower().count(term_lower)
        return count
    
    def analyze_video_content(self, video_id: str) -> Optional[Dict]:
        """
        Analyze video content from transcript
        
        Returns:
            {
                'main_topics': [...],
                'summary': '...',
                'sentiment': 'positive/neutral/negative',
                'key_points': [...]
            }
        """
        if not YOUTUBE_TRANSCRIPT_AVAILABLE:
            return None
        
        try:
            # Get transcript
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
            transcript_text = ' '.join([entry['text'] for entry in transcript_list])
            
            # Extract topics
            topics = self.extract_topics_from_titles([transcript_text])[:5]
            
            # Generate summary (first 500 chars)
            summary = transcript_text[:500] + '...' if len(transcript_text) > 500 else transcript_text
            
            # Basic sentiment (can be enhanced with proper sentiment analysis)
            sentiment = self._analyze_sentiment(transcript_text)
            
            return {
                'main_topics': [t['topic'] for t in topics],
                'summary': summary,
                'sentiment': sentiment,
                'transcript_length': len(transcript_text),
                'has_transcript': True
            }
        
        except Exception as e:
            print(f"Could not get transcript for {video_id}: {e}")
            return {
                'has_transcript': False,
                'error': str(e)
            }
    
    def _analyze_sentiment(self, text: str) -> str:
        """
        Basic sentiment analysis
        (Can be enhanced with proper sentiment models)
        """
        positive_words = {'great', 'amazing', 'awesome', 'excellent', 'best', 'love', 'perfect'}
        negative_words = {'bad', 'worst', 'terrible', 'awful', 'hate', 'horrible', 'disappointing'}
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count * 1.5:
            return 'positive'
        elif neg_count > pos_count * 1.5:
            return 'negative'
        else:
            return 'neutral'
    
    def analyze_high_performing_videos(self, videos: List[Dict]) -> Dict:
        """
        Analyze what makes high-performing videos successful
        
        Args:
            videos: List of video dicts with viewCount, title, etc.
        
        Returns:
            {
                'common_topics': [...],
                'avg_title_length': int,
                'best_posting_time': str,
                'engagement_patterns': {...}
            }
        """
        if not videos:
            return {}
        
        # Sort by view count
        sorted_videos = sorted(videos, key=lambda x: x.get('viewCount', 0), reverse=True)
        top_20_percent = sorted_videos[:max(1, len(sorted_videos) // 5)]
        
        # Extract topics from top performers
        top_titles = [v['title'] for v in top_20_percent]
        common_topics = self.extract_topics_from_titles(top_titles)
        
        # Analyze patterns
        avg_title_length = np.mean([len(v['title']) for v in top_20_percent])
        
        # Posting time analysis (hour of day)
        posting_hours = [v.get('publishedAt', '').split('T')[1].split(':')[0] 
                        for v in top_20_percent if 'publishedAt' in v]
        best_hour = Counter(posting_hours).most_common(1)[0][0] if posting_hours else 'N/A'
        
        # Engagement rate
        avg_engagement = np.mean([
            (v.get('likeCount', 0) + v.get('commentCount', 0)) / max(v.get('viewCount', 1), 1)
            for v in top_20_percent
        ])
        
        # Calculate average views from all videos (not just top performers)
        # This is the benchmark for prediction
        all_views = [v.get('viewCount', 0) for v in videos if v.get('viewCount', 0) > 0]
        avg_views = int(np.mean(all_views)) if all_views else 10000
        
        # Also calculate median for more robust estimate
        median_views = int(np.median(all_views)) if all_views else 10000
        
        return {
            'common_topics': common_topics[:5],
            'avg_title_length': int(avg_title_length),
            'best_posting_hour': best_hour,
            'avg_engagement_rate': float(avg_engagement),
            'top_performer_count': len(top_20_percent),
            'avg_views': avg_views,  # È¢ëÈÅìÂπ≥ÂùáÊí≠ÊîæÈáèÔºàÁî®‰∫éÈ¢ÑÊµãÂü∫ÂáÜÔºâ
            'median_views': median_views,  # ‰∏≠‰ΩçÊï∞Êí≠ÊîæÈáèÔºàÊõ¥Á®≥ÂÅ•Ôºâ
            'total_videos': len(videos)
        }
    
    def identify_content_style(self, videos: List[Dict]) -> Dict:
        """
        Identify the channel's content style and format
        """
        titles = [v['title'] for v in videos]
        combined = ' '.join(titles).lower()
        
        # Identify style indicators
        styles = {
            'tutorial': ['how to', 'tutorial', 'guide', 'tips', 'learn'],
            'review': ['review', 'unboxing', 'first look', 'hands on', 'vs'],
            'entertainment': ['funny', 'prank', 'challenge', 'compilation', 'fails'],
            'news': ['news', 'update', 'breaking', 'latest', 'today'],
            'vlog': ['vlog', 'daily', 'day in', 'life', 'routine'],
            'educational': ['explained', 'science', 'history', 'documentary', 'facts'],
            'gaming': ['gameplay', 'walkthrough', 'let\'s play', 'speedrun'],
            'tech': ['tech', 'gadget', 'phone', 'laptop', 'specs']
        }
        
        style_scores = {}
        for style, keywords in styles.items():
            score = sum(combined.count(kw) for kw in keywords)
            if score > 0:
                style_scores[style] = score
        
        # Get dominant styles
        sorted_styles = sorted(style_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'primary_style': sorted_styles[0][0] if sorted_styles else 'general',
            'style_distribution': dict(sorted_styles[:3]),
            'is_multi_format': len(sorted_styles) > 3
        }


class EnhancedAudienceAnalyzer:
    """
    ‰ºòÂåñÁöÑÂèó‰ºóÂàÜÊûêÂô® - Êõ¥Á≤æÁªÜÁöÑÂπ¥ÈæÑÂíåÂÖ¥Ë∂£Ê†áÁ≠æ
    """
    
    def analyze_target_audience(self, videos: List[Dict], channel_data: Dict) -> Dict:
        """
        ÁªÜÁ≤íÂ∫¶Âèó‰ºóÂàÜÊûê
        """
        titles_text = ' '.join([v['title'] for v in videos]).lower()
        descriptions = ' '.join([v.get('description', '')[:200] for v in videos]).lower()
        combined_text = titles_text + ' ' + descriptions
        
        # Êõ¥ÁªÜÂåñÁöÑÂπ¥ÈæÑÊÆµÊåáÊ†á
        age_indicators = {
            '6-12Â≤Å (ÂÑøÁ´•)': {
                'keywords': ['kids', 'children', 'cartoon', 'toy', 'fun', 'animation', 'disney'],
                'score': 0
            },
            '13-17Â≤Å (ÈùíÂ∞ëÂπ¥)': {
                'keywords': ['teen', 'school', 'student', 'fortnite', 'tiktok', 'roblox'],
                'score': 0
            },
            '18-24Â≤Å (Â§ßÂ≠¶Áîü/Âπ¥ËΩª‰∫∫)': {
                'keywords': ['college', 'university', 'study', 'dorm', 'dating', 'meme', 'viral'],
                'score': 0
            },
            '25-34Â≤Å (ËÅåÂú∫Êñ∞‰∫∫)': {
                'keywords': ['career', 'startup', 'side hustle', 'productivity', 'freelance', 'crypto'],
                'score': 0
            },
            '35-44Â≤Å (ÊàêÁÜüËÅåÂú∫)': {
                'keywords': ['professional', 'business', 'management', 'investment', 'finance', 'real estate'],
                'score': 0
            },
            '45+ (ËµÑÊ∑±/ÈÄÄ‰ºë)': {
                'keywords': ['retirement', 'classic', 'history', 'documentary', 'gardening', 'travel'],
                'score': 0
            },
            'ÂÖ®Âπ¥ÈæÑ': {
                'keywords': ['family', 'everyone', 'beginner', 'basic', 'guide', 'tutorial'],
                'score': 0
            }
        }
        
        # ËÆ°ÁÆóÊØè‰∏™Âπ¥ÈæÑÊÆµÁöÑÂåπÈÖçÂàÜÊï∞
        for age_group, data in age_indicators.items():
            for keyword in data['keywords']:
                if keyword in combined_text:
                    data['score'] += 1
        
        # ÊéíÂ∫èÂπ∂Ëé∑ÂèñÂâç2‰∏™ÊúÄÂèØËÉΩÁöÑÂπ¥ÈæÑÊÆµ
        sorted_ages = sorted(
            age_indicators.items(), 
            key=lambda x: x[1]['score'], 
            reverse=True
        )
        
        primary_age = sorted_ages[0][0] if sorted_ages[0][1]['score'] > 0 else '18-24Â≤Å (Â§ßÂ≠¶Áîü/Âπ¥ËΩª‰∫∫)'
        secondary_age = sorted_ages[1][0] if len(sorted_ages) > 1 and sorted_ages[1][1]['score'] > 0 else None
        
        # ÂÖ¥Ë∂£Ê†áÁ≠æËØÜÂà´
        interest_indicators = {
            'ÁßëÊäÄÁà±Â•ΩËÄÖ': ['tech', 'gadget', 'phone', 'computer', 'software', 'app', 'ai'],
            'Ê∏∏ÊàèÁé©ÂÆ∂': ['game', 'gaming', 'gameplay', 'esports', 'console', 'pc'],
            'Âàõ‰∏öËÄÖ': ['startup', 'business', 'entrepreneur', 'side hustle', 'passive income'],
            'Â≠¶Áîü/Â≠¶‰π†ËÄÖ': ['learn', 'study', 'education', 'tutorial', 'course', 'exam'],
            'Êó∂Â∞ö/ÁæéÂ¶Ü': ['fashion', 'makeup', 'beauty', 'style', 'outfit'],
            'ÂÅ•Ë∫´/ÂÅ•Â∫∑': ['fitness', 'workout', 'health', 'diet', 'gym', 'nutrition'],
            'ÊäïËµÑ/ÁêÜË¥¢': ['invest', 'stock', 'crypto', 'finance', 'money', 'wealth'],
            'Â®±‰πêÁà±Â•ΩËÄÖ': ['entertainment', 'movie', 'music', 'celebrity', 'comedy'],
            '‰∏ì‰∏ö‰∫∫Â£´': ['professional', 'expert', 'advanced', 'industry', 'career']
        }
        
        interest_scores = {}
        for interest, keywords in interest_indicators.items():
            score = sum(1 for kw in keywords if kw in combined_text)
            if score > 0:
                interest_scores[interest] = score
        
        top_interests = sorted(
            interest_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:3]
        
        # ‰∫íÂä®Ê∞¥Âπ≥ÂàÜÊûê
        avg_comment_rate = np.mean([
            v.get('commentCount', 0) / max(v.get('viewCount', 1), 1) * 100
            for v in videos
        ])
        
        engagement_level = (
            'ÊûÅÈ´ò (Ê¥ªË∑ÉÁ§æÂå∫)' if avg_comment_rate > 1.0 else
            'È´ò (ÁßØÊûÅ‰∫íÂä®)' if avg_comment_rate > 0.5 else
            '‰∏≠Á≠â (Ê≠£Â∏∏Ê∞¥Âπ≥)' if avg_comment_rate > 0.2 else
            '‰Ωé (ËßÇÁúã‰∏∫‰∏ª)'
        )
        
        # Ê∂àË¥πËÉΩÂäõÊé®ÊµãÔºàÂü∫‰∫éÂÜÖÂÆπÁ±ªÂûãÔºâ
        purchasing_power_indicators = {
            'È´òÊ∂àË¥π': ['luxury', 'premium', 'expensive', 'high-end', 'professional'],
            '‰∏≠Á≠âÊ∂àË¥π': ['affordable', 'budget', 'value', 'best', 'review'],
            '‰ª∑Ê†ºÊïèÊÑü': ['cheap', 'free', 'budget', 'affordable', 'deal']
        }
        
        purchasing_scores = {}
        for level, keywords in purchasing_power_indicators.items():
            score = sum(1 for kw in keywords if kw in combined_text)
            purchasing_scores[level] = score
        
        purchasing_power = max(purchasing_scores.items(), key=lambda x: x[1])[0] if any(purchasing_scores.values()) else '‰∏≠Á≠âÊ∂àË¥π'
        
        return {
            'primary_age_group': primary_age,
            'secondary_age_group': secondary_age,
            'age_confidence': f"{sorted_ages[0][1]['score']}‰∏™ÂåπÈÖçÊåáÊ†á",
            'top_interests': [interest for interest, _ in top_interests],
            'engagement_level': engagement_level,
            'engagement_rate': f"{avg_comment_rate:.3f}%",
            'purchasing_power': purchasing_power,
            'subscriber_count': channel_data.get('subscriberCount', 0),
            'audience_size_tier': (
                'Â§ßÂûãÈ¢ëÈÅì (10‰∏á+)' if channel_data.get('subscriberCount', 0) > 100000 else
                '‰∏≠ÂûãÈ¢ëÈÅì (1‰∏á-10‰∏á)' if channel_data.get('subscriberCount', 0) > 10000 else
                'Â∞èÂûãÈ¢ëÈÅì (1ÂçÉ-1‰∏á)' if channel_data.get('subscriberCount', 0) > 1000 else
                'Êñ∞È¢ëÈÅì (<1ÂçÉ)'
            ),
            'audience_insights': self._generate_insights(
                primary_age, 
                [i for i, _ in top_interests],
                engagement_level
            )
        }
    
    def _generate_insights(self, age_group: str, interests: List[str], engagement: str) -> str:
        """ÁîüÊàêÂèó‰ºóÊ¥ûÂØüÊñáÊú¨"""
        insights = []
        
        # Âπ¥ÈæÑÊÆµÊ¥ûÂØü
        if '18-24' in age_group:
            insights.append("Âπ¥ËΩªËßÇ‰ºóÁæ§‰ΩìÔºåÂñúÊ¨¢Êñ∞ÊΩÆ„ÄÅÁóÖÊØíÂºèÂÜÖÂÆπ")
        elif '25-34' in age_group:
            insights.append("ËÅåÂú∫Êñ∞‰∫∫‰∏∫‰∏ªÔºåÂÖ≥Ê≥®ÂÆûÁî®ÊäÄËÉΩÂíåËÅå‰∏öÂèëÂ±ï")
        elif '35-44' in age_group:
            insights.append("ÊàêÁÜüËßÇ‰ºóÔºåÂÅèÂ•ΩÊ∑±Â∫¶ÂÜÖÂÆπÂíå‰∏ì‰∏öÁü•ËØÜ")
        
        # ÂÖ¥Ë∂£Ê¥ûÂØü
        if interests:
            insights.append(f"Ê†∏ÂøÉÂÖ¥Ë∂£Ôºö{', '.join(interests[:2])}")
        
        # ‰∫íÂä®Ê¥ûÂØü
        if 'ÊûÅÈ´ò' in engagement or 'È´ò' in engagement:
            insights.append("Á§æÂå∫Ê¥ªË∑ÉÂ∫¶È´òÔºåÈÄÇÂêà‰∫íÂä®ÊÄßÂÜÖÂÆπ")
        
        return "Ôºõ".join(insights) if insights else "ËßÇ‰ºóÁæ§‰ΩìÂ§öÂÖÉÂåñ"


# Initialize analyzers
content_analyzer = EnhancedContentAnalyzer()
audience_analyzer = EnhancedAudienceAnalyzer()


def analyze_channel_deeply(videos: List[Dict], channel_data: Dict) -> Dict:
    """
    Comprehensive channel analysis with all new features
    
    Returns:
        {
            'topics': [...],           # Extracted topics
            'high_performers': {...},  # What works well
            'content_style': {...},    # Channel style
            'target_audience': {...},  # Audience insights
            'video_analyses': [...]    # Individual video insights
        }
    """
    # Extract topics
    titles = [v['title'] for v in videos]
    topics = content_analyzer.extract_topics_from_titles(titles)
    
    # Analyze high performers
    high_performers = content_analyzer.analyze_high_performing_videos(videos)
    
    # Identify content style
    content_style = content_analyzer.identify_content_style(videos)
    
    # Analyze target audience
    target_audience = audience_analyzer.analyze_target_audience(videos, channel_data)
    
    # Analyze individual videos (DISABLED for performance - transcript analysis is slow)
    # This can be enabled if needed, but significantly slows down analysis
    video_analyses = []
    # Skip transcript analysis for faster processing
    # top_videos = sorted(videos, key=lambda x: x.get('viewCount', 0), reverse=True)[:5]
    # for video in top_videos:
    #     analysis = content_analyzer.analyze_video_content(video['videoId'])
    #     if analysis and analysis.get('has_transcript'):
    #         video_analyses.append({
    #             'video_id': video['videoId'],
    #             'title': video['title'],
    #             'views': video.get('viewCount', 0),
    #             'analysis': analysis
    #         })
    
    return {
        'topics': topics,
        'high_performers': high_performers,
        'content_style': content_style,
        'target_audience': target_audience,
        'video_analyses': video_analyses,
        'total_videos_analyzed': len(videos)
    }
