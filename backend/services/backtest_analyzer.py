"""
Backtest Analyzer for Prediction Algorithm
å›æµ‹åˆ†æå™¨ - è¯„ä¼°é¢„æµ‹ç®—æ³•å‡†ç¡®æ€§å¹¶åˆ†æä¼˜ç§€è¡¨ç°è§†é¢‘
"""

from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np
from collections import defaultdict

# Import content analyzer for video content analysis
from services.enhanced_youtube_analyzer import content_analyzer

# Import ML predictor for enhanced predictions
try:
    from services.ml_predictor import ml_predictor
    ML_PREDICTOR_AVAILABLE = True
except ImportError:
    ML_PREDICTOR_AVAILABLE = False
    print("âš ï¸  ML Predictor not available")


class BacktestAnalyzer:
    """
    å›æµ‹åˆ†æå™¨ - ä½¿ç”¨å†å²æ•°æ®è¯„ä¼°é¢„æµ‹ç®—æ³•
    """
    
    def __init__(self, recommendation_engine, social_aggregator):
        """
        åˆå§‹åŒ–å›æµ‹åˆ†æå™¨
        
        Args:
            recommendation_engine: æ¨èå¼•æ“å®ä¾‹
            social_aggregator: ç¤¾äº¤åª’ä½“èšåˆå™¨å®ä¾‹
        """
        self.recommendation_engine = recommendation_engine
        self.social_aggregator = social_aggregator
    
    def backtest_predictions(
        self,
        videos: List[Dict],
        channel_analysis: Dict,
        historical_trends: Optional[Dict] = None,
        use_ml_model: bool = True
    ) -> Dict:
        """
        å›æµ‹é¢„æµ‹ç®—æ³•
        
        Args:
            videos: å†å²è§†é¢‘åˆ—è¡¨ï¼ŒåŒ…å« viewCount, publishedAt, title ç­‰
            channel_analysis: é¢‘é“åˆ†ææ•°æ®
            historical_trends: å†å²è¶‹åŠ¿æ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ¨¡æ‹Ÿï¼‰
        
        Returns:
            {
                'backtest_results': [...],  # æ¯ä¸ªè§†é¢‘çš„å›æµ‹ç»“æœ
                'accuracy_metrics': {...},  # å‡†ç¡®åº¦æŒ‡æ ‡
                'top_outliers': [...]       # ä¼˜ç§€è¡¨ç°è§†é¢‘åˆ†æ
            }
        """
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        sorted_videos = sorted(
            videos,
            key=lambda v: v.get('publishedAt', ''),
            reverse=False  # ä»æ—©åˆ°æ™š
        )
        
        # ç¡®ä¿è‡³å°‘å¤„ç†50ä¸ªè§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # å¦‚æœè§†é¢‘æ•°é‡å°‘äº50ï¼Œä½¿ç”¨æ‰€æœ‰è§†é¢‘ï¼›å¦‚æœå¤šäº50ï¼Œä½¿ç”¨æœ€è¿‘çš„50ä¸ª
        min_videos_required = 50
        if len(sorted_videos) > min_videos_required:
            # ä½¿ç”¨æœ€è¿‘çš„50ä¸ªè§†é¢‘ï¼ˆæœ€æ–°çš„æ•°æ®æ›´ç›¸å…³ï¼‰
            sorted_videos = sorted_videos[-min_videos_required:]
            print(f"ğŸ“Š ä½¿ç”¨æœ€è¿‘çš„ {min_videos_required} ä¸ªè§†é¢‘è¿›è¡Œå›æµ‹ï¼ˆå…± {len(videos)} ä¸ªè§†é¢‘ï¼‰")
        else:
            print(f"ğŸ“Š ä½¿ç”¨æ‰€æœ‰ {len(sorted_videos)} ä¸ªè§†é¢‘è¿›è¡Œå›æµ‹")
        
        backtest_results = []
        all_predictions = []
        all_actuals = []
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„å¹³å‡æ’­æ”¾é‡ï¼ˆç”¨äºè¯†åˆ«outlierï¼‰
        time_periods = self._group_videos_by_period(sorted_videos)
        
        # å¦‚æœä½¿ç”¨MLæ¨¡å‹ï¼Œå…ˆè®­ç»ƒæ¨¡å‹
        if use_ml_model and ML_PREDICTOR_AVAILABLE and len(sorted_videos) >= 20:
            print("ğŸ¤– Training ML models for enhanced prediction...")
            try:
                # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œä¸åˆ†å‰²ï¼Œå› ä¸ºè¿™æ˜¯å›æµ‹ï¼‰
                X_train = []
                y_train = []
                
                for video in sorted_videos:  # ä½¿ç”¨æ‰€æœ‰æ•°æ®è®­ç»ƒï¼ˆå›æµ‹åœºæ™¯ï¼‰
                    # æ¨¡æ‹Ÿè¶‹åŠ¿æ•°æ®
                    keywords = self._extract_keywords_from_title(video.get('title', ''))
                    try:
                        publish_date = datetime.fromisoformat(video.get('publishedAt', '').replace('Z', '+00:00'))
                    except:
                        publish_date = datetime.now()
                    period_key = self._get_period_key(publish_date)
                    period_avg = time_periods.get(period_key, {}).get('avg_views', video.get('viewCount', 0))
                    
                    trend_data = self._simulate_historical_trend(
                        keywords,
                        video.get('viewCount', 0),
                        period_avg
                    )
                    
                    # æå–ç‰¹å¾
                    features = ml_predictor.extract_features(
                        video,
                        channel_analysis,
                        trend_data,
                        period_avg
                    )
                    X_train.append(features)
                    y_train.append(video.get('viewCount', 0))
                
                if X_train and y_train and len(X_train) >= 10:
                    X_train = np.array(X_train)
                    y_train = np.array(y_train)
                    
                    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(X_train)} ä¸ªæ ·æœ¬, {X_train.shape[1]} ä¸ªç‰¹å¾")
                    print(f"   æ’­æ”¾é‡èŒƒå›´: {y_train.min():.0f} - {y_train.max():.0f}, å‡å€¼: {y_train.mean():.0f}")
                    
                    # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨äº¤å‰éªŒè¯å’Œä¸€è‡´çš„è¯„ä¼°æ ‡å‡†ï¼‰
                    # å¯¹äºå›æµ‹ï¼Œä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†ï¼ˆ30-40%ï¼‰ä»¥è·å¾—æ›´å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°
                    # ç¡®ä¿æµ‹è¯•é›†è‡³å°‘æœ‰10ä¸ªæ ·æœ¬ï¼Œä½†ä¸è¶…è¿‡40%
                    min_test_samples = min(10, len(X_train) // 3)  # è‡³å°‘10ä¸ªæˆ–æ€»æ•°çš„1/3
                    test_size = max(0.3, min(0.4, min_test_samples / len(X_train)))  # 30-40%çš„æµ‹è¯•é›†
                    print(f"ğŸ“Š æµ‹è¯•é›†æ¯”ä¾‹: {test_size:.1%} ({int(len(X_train) * test_size)} ä¸ªæ ·æœ¬)")
                    # ä½¿ç”¨äº¤å‰éªŒè¯ç¡®ä¿è·¨é¢‘é“ä¸€è‡´æ€§
                    use_cv = len(X_train) >= 20
                    training_results = ml_predictor.train_models(
                        X_train, 
                        y_train, 
                        test_size=test_size,
                        use_cross_validation=use_cv,
                        cv_folds=5
                    )
                    
                    best_model = training_results.get('best_model', 'N/A')
                    best_r2 = training_results.get('best_r2', 0)
                    best_mape = training_results.get('best_mape', 0)
                    best_mae = training_results.get('best_mae', 0)
                    best_rmse = training_results.get('best_rmse', 0)
                    
                    print(f"âœ… ML models trained. Best model: {best_model}")
                    print(f"   Best RÂ²: {best_r2:.3f}")
                    print(f"   Best MAPE: {best_mape:.1f}%")
                    print(f"   Best MAE: {best_mae:.0f}")
                    print(f"   Best RMSE: {best_rmse:.0f}")
                    
                    # å¦‚æœæœ€ä½³æ¨¡å‹çš„RÂ²ä»ç„¶å¾ˆä½ï¼Œè€ƒè™‘ä¸ä½¿ç”¨MLæ¨¡å‹
                    if best_r2 < 0.3:
                        print(f"âš ï¸  æœ€ä½³æ¨¡å‹RÂ²è¿‡ä½ï¼ˆ{best_r2:.3f}ï¼‰ï¼Œå¯èƒ½å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                else:
                    print(f"âš ï¸  è®­ç»ƒæ•°æ®ä¸è¶³ï¼ˆ{len(X_train) if X_train else 0} ä¸ªæ ·æœ¬ï¼‰ï¼Œè·³è¿‡MLè®­ç»ƒ")
                    use_ml_model = False
            except Exception as e:
                import traceback
                print(f"âš ï¸  ML model training failed: {e}")
                traceback.print_exc()
                use_ml_model = False
        
        for video in sorted_videos:
            result = self._backtest_single_video(
                video,
                channel_analysis,
                time_periods,
                historical_trends,
                use_ml_model=use_ml_model
            )
            backtest_results.append(result)
            
            if result.get('predicted_views') and result.get('actual_views'):
                all_predictions.append(result['predicted_views'])
                all_actuals.append(result['actual_views'])
        
        # è®¡ç®—å‡†ç¡®åº¦æŒ‡æ ‡
        accuracy_metrics = self._calculate_accuracy_metrics(
            all_predictions,
            all_actuals
        )
        
        # è¯†åˆ«ä¼˜ç§€è¡¨ç°è§†é¢‘ï¼ˆoutlierï¼‰
        top_outliers = self._identify_top_outliers(
            backtest_results,
            time_periods,
            videos=sorted_videos  # ä¼ é€’åŸå§‹è§†é¢‘æ•°æ®ä»¥ä¾¿è·å–å®Œæ•´ä¿¡æ¯
        )
        
        return {
            'backtest_results': backtest_results,
            'accuracy_metrics': {
                'mae': float(accuracy_metrics.get('mae', 0)),
                'mape': float(accuracy_metrics.get('mape', 0)),
                'rmse': float(accuracy_metrics.get('rmse', 0)),
                'r2_score': float(accuracy_metrics.get('r2_score', 0)),
                'correlation': float(accuracy_metrics.get('correlation', 0))
            },
            'top_outliers': top_outliers,
            'total_videos_tested': int(len(sorted_videos))
        }
    
    def _backtest_single_video(
        self,
        video: Dict,
        channel_analysis: Dict,
        time_periods: Dict,
        historical_trends: Optional[Dict],
        use_ml_model: bool = False
    ) -> Dict:
        """
        å›æµ‹å•ä¸ªè§†é¢‘çš„é¢„æµ‹
        
        Args:
            video: è§†é¢‘æ•°æ®
            channel_analysis: é¢‘é“åˆ†æ
            time_periods: æ—¶é—´æ®µåˆ†ç»„æ•°æ®
            historical_trends: å†å²è¶‹åŠ¿æ•°æ®
        """
        video_id = video.get('videoId', '')
        title = video.get('title', '')
        actual_views = video.get('viewCount', 0)
        published_at = video.get('publishedAt', '')
        
        # è§£æå‘å¸ƒæ—¶é—´
        try:
            if isinstance(published_at, str):
                publish_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            else:
                publish_date = published_at
        except:
            publish_date = datetime.now()
        
        # è·å–è¯¥æ—¶é—´ç‚¹çš„åŒæœŸå¹³å‡æ’­æ”¾é‡
        period_key = self._get_period_key(publish_date)
        period_avg = time_periods.get(period_key, {}).get('avg_views', actual_views)
        
        # æ¨¡æ‹Ÿè¯¥è§†é¢‘å‘å¸ƒæ—¶çš„è¶‹åŠ¿æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼‰
        if not historical_trends:
            # ä»è§†é¢‘æ ‡é¢˜æå–å…³é”®è¯
            keywords = self._extract_keywords_from_title(title)
            # æ¨¡æ‹Ÿå†å²è¶‹åŠ¿ï¼ˆåŸºäºè§†é¢‘å®é™…è¡¨ç°åæ¨ï¼‰
            simulated_trend = self._simulate_historical_trend(
                keywords,
                actual_views,
                period_avg
            )
        else:
            # ä½¿ç”¨çœŸå®å†å²è¶‹åŠ¿æ•°æ®
            simulated_trend = historical_trends.get(video_id, {})
        
        # è®¡ç®—é¢„æµ‹è§‚çœ‹æ•°ï¼ˆä½¿ç”¨MLæ¨¡å‹æˆ–ä¼ ç»Ÿç®—æ³•ï¼‰
        if use_ml_model and ML_PREDICTOR_AVAILABLE and ml_predictor.is_trained:
            try:
                ml_result = ml_predictor.predict(
                    video,
                    channel_analysis,
                    simulated_trend,
                    period_avg
                )
                predicted_views = ml_result['predicted_views']
            except Exception as e:
                print(f"âš ï¸  ML prediction failed, using fallback: {e}")
                predicted_views = self._predict_for_historical_video(
                    video,
                    channel_analysis,
                    simulated_trend,
                    period_avg
                )
        else:
            predicted_views = self._predict_for_historical_video(
                video,
                channel_analysis,
                simulated_trend,
                period_avg
            )
        
        # è®¡ç®—è¯¯å·®
        error = abs(predicted_views - actual_views)
        error_percentage = (error / actual_views * 100) if actual_views > 0 else 0
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºoutlierï¼ˆé«˜äºåŒæœŸå¹³å‡1.2å€ä»¥ä¸Šï¼Œé™ä½é˜ˆå€¼ä»¥è¯†åˆ«æ›´å¤šä¼˜ç§€è§†é¢‘ï¼‰
        is_outlier = bool(actual_views > period_avg * 1.2)
        
        return {
            'video_id': str(video_id),
            'title': str(title),
            'published_at': str(published_at) if published_at else None,
            'actual_views': int(actual_views),
            'predicted_views': int(predicted_views),
            'period_avg_views': float(period_avg),
            'error': float(error),
            'error_percentage': float(error_percentage),
            'is_outlier': bool(is_outlier),
            'outlier_ratio': float(actual_views / period_avg if period_avg > 0 else 1.0),
            'simulated_trend': {
                k: (float(v) if isinstance(v, (np.integer, np.floating)) else 
                    bool(v) if isinstance(v, np.bool_) else
                    str(v) if isinstance(v, np.str_) else v)
                for k, v in simulated_trend.items()
            } if simulated_trend else {}
        }
    
    def _predict_for_historical_video(
        self,
        video: Dict,
        channel_analysis: Dict,
        trend_data: Dict,
        period_avg: float
    ) -> int:
        """
        ä¸ºå†å²è§†é¢‘è®¡ç®—é¢„æµ‹è§‚çœ‹æ•°ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„ç®—æ³•ï¼‰
        
        æ”¹è¿›ï¼š
        1. ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºåŸºå‡†
        2. è€ƒè™‘è§†é¢‘å®é™…ç‰¹å¾ï¼ˆæ ‡é¢˜é•¿åº¦ã€å‘å¸ƒæ—¶é—´ç­‰ï¼‰
        3. ä½¿ç”¨ä¼˜åŒ–åçš„ç³»æ•°è®¡ç®—
        """
        high_performers = channel_analysis.get('high_performers', {})
        
        # ä½¿ç”¨ä¸­ä½æ•°å’Œå¹³å‡å€¼çš„åŠ æƒå¹³å‡
        median_views = high_performers.get('median_views')
        avg_views = high_performers.get('avg_views')
        
        if median_views and avg_views:
            base_views = int(median_views * 0.7 + avg_views * 0.3)
        elif median_views:
            base_views = int(median_views)
        elif avg_views:
            base_views = int(avg_views)
        else:
            base_views = int(period_avg) if period_avg > 0 else 10000
        
        if base_views <= 0:
            base_views = 10000
        
        # ä»è¶‹åŠ¿æ•°æ®ä¸­æå–åˆ†æ•°
        viral_potential = trend_data.get('viral_potential', 50)
        relevance_score = trend_data.get('relevance_score', 50)
        performance_score = trend_data.get('performance_score', 50)
        match_score = trend_data.get('match_score', 50)
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„ç®—æ³•è®¡ç®—
        # 1. çƒ­åº¦å¢é•¿ç³»æ•°ï¼ˆè¿ç»­å‡½æ•°ï¼‰
        if viral_potential >= 90:
            viral_multiplier = 2.2 + (viral_potential - 90) * 0.03
        elif viral_potential >= 70:
            viral_multiplier = 1.6 + (viral_potential - 70) * 0.03
        elif viral_potential >= 50:
            viral_multiplier = 1.2 + (viral_potential - 50) * 0.02
        else:
            viral_multiplier = 0.9 + (viral_potential / 50) * 0.3
        viral_multiplier = max(0.7, min(3.0, viral_multiplier))
        
        # 2. ç›¸å…³æ€§è°ƒæ•´ï¼ˆæ›´ä¿å®ˆï¼‰
        if relevance_score >= 80:
            relevance_multiplier = 1.0 + (relevance_score - 80) * 0.01
        elif relevance_score >= 60:
            relevance_multiplier = 0.85 + (relevance_score - 60) * 0.0075
        elif relevance_score >= 40:
            relevance_multiplier = 0.75 + (relevance_score - 40) * 0.005
        else:
            relevance_multiplier = 0.65 + (relevance_score / 40) * 0.1
        
        # 3. è¡¨ç°æ½œåŠ›ç³»æ•°
        if performance_score >= 80:
            performance_multiplier = 1.2 + (performance_score - 80) * 0.015
        elif performance_score >= 60:
            performance_multiplier = 1.0 + (performance_score - 60) * 0.01
        elif performance_score >= 40:
            performance_multiplier = 0.85 + (performance_score - 40) * 0.0075
        else:
            performance_multiplier = 0.7 + (performance_score / 40) * 0.15
        
        # 4. æ—¶æ•ˆæ€§åŠ æˆ
        timeliness_multiplier = 0.9 + (match_score / 100) * 0.25
        
        # 5. æ ‡é¢˜ä¼˜åŒ–ï¼ˆåŸºäºå®é™…æ ‡é¢˜é•¿åº¦ï¼‰
        title = video.get('title', '')
        title_length = len(title) if title else 50
        if 30 <= title_length <= 60:
            title_optimization = 1.05
        else:
            title_optimization = 0.98
        
        # 6. é¢‘é“è§„æ¨¡è°ƒæ•´
        total_videos = high_performers.get('total_videos', 0)
        if total_videos > 100:
            channel_stability = 0.95
        elif total_videos > 50:
            channel_stability = 1.0
        else:
            channel_stability = 1.1
        
        # 7. ç¡®å®šæ€§å› å­ï¼ˆåŸºäºmatch_scoreï¼‰
        confidence_factor = 0.9 + (match_score / 100) * 0.2
        
        # ç»¼åˆè®¡ç®—
        predicted_views = int(
            base_views *
            viral_multiplier *
            relevance_multiplier *
            performance_multiplier *
            timeliness_multiplier *
            title_optimization *
            channel_stability *
            confidence_factor
        )
        
        return max(500, predicted_views)
    
    def _simulate_historical_trend(
        self,
        keywords: List[str],
        actual_views: float,
        period_avg: float
    ) -> Dict:
        """
        æ¨¡æ‹Ÿå†å²è¶‹åŠ¿æ•°æ®ï¼ˆåŸºäºå®é™…è¡¨ç°åæ¨ï¼Œä¼˜åŒ–ç‰ˆï¼‰
        
        æ”¹è¿›ï¼š
        1. æ›´å‡†ç¡®çš„åæ¨ç®—æ³•
        2. è€ƒè™‘éçº¿æ€§å…³ç³»
        3. æ·»åŠ éšæœºæ€§ä»¥æ¨¡æ‹ŸçœŸå®æƒ…å†µ
        """
        # æ ¹æ®å®é™…è¡¨ç°åæ¨çƒ­åº¦
        performance_ratio = actual_views / period_avg if period_avg > 0 else 1.0
        
        # ä½¿ç”¨æ›´å¹³æ»‘çš„åæ¨å‡½æ•°
        # è¡¨ç°è¶Šå¥½ï¼Œè¯´æ˜çƒ­åº¦ã€ç›¸å…³æ€§ç­‰è¶Šé«˜ï¼Œä½†å­˜åœ¨ä¸Šé™
        if performance_ratio > 3.0:
            # æç«¯è¡¨ç°ï¼ˆå¯èƒ½æ˜¯ç—…æ¯’å¼ä¼ æ’­ï¼‰
            viral_potential = min(98, 60 + (performance_ratio - 3.0) * 5)
            relevance_score = min(95, 55 + (performance_ratio - 3.0) * 4)
            performance_score = min(95, 60 + (performance_ratio - 3.0) * 5)
        elif performance_ratio > 2.0:
            # è¡¨ç°éå¸¸å¥½
            viral_potential = min(90, 50 + (performance_ratio - 2.0) * 20)
            relevance_score = min(90, 50 + (performance_ratio - 2.0) * 15)
            performance_score = min(90, 50 + (performance_ratio - 2.0) * 20)
        elif performance_ratio > 1.5:
            # è¡¨ç°è‰¯å¥½
            viral_potential = min(80, 50 + (performance_ratio - 1.5) * 20)
            relevance_score = min(80, 50 + (performance_ratio - 1.5) * 15)
            performance_score = min(80, 50 + (performance_ratio - 1.5) * 20)
        elif performance_ratio > 1.2:
            # è¡¨ç°ç•¥å¥½
            viral_potential = min(70, 50 + (performance_ratio - 1.2) * 33)
            relevance_score = min(70, 50 + (performance_ratio - 1.2) * 25)
            performance_score = min(70, 50 + (performance_ratio - 1.2) * 33)
        elif performance_ratio > 0.8:
            # è¡¨ç°æ­£å¸¸
            viral_potential = 50 + (performance_ratio - 0.8) * 25
            relevance_score = 50 + (performance_ratio - 0.8) * 20
            performance_score = 50 + (performance_ratio - 0.8) * 25
        else:
            # è¡¨ç°è¾ƒå·®
            viral_potential = max(30, 50 - (0.8 - performance_ratio) * 50)
            relevance_score = max(30, 50 - (0.8 - performance_ratio) * 40)
            performance_score = max(30, 50 - (0.8 - performance_ratio) * 50)
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆç»¼åˆå„é¡¹ï¼‰
        match_score = (viral_potential * 0.4 + relevance_score * 0.35 + performance_score * 0.25)
        
        return {
            'keywords': keywords,
            'viral_potential': float(viral_potential),
            'relevance_score': float(relevance_score),
            'performance_score': float(performance_score),
            'match_score': float(match_score)
        }
    
    def _group_videos_by_period(self, videos: List[Dict]) -> Dict:
        """
        æŒ‰æ—¶é—´æ®µåˆ†ç»„è§†é¢‘ï¼Œè®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„å¹³å‡æ’­æ”¾é‡
        """
        periods = defaultdict(lambda: {'views': [], 'count': 0})
        
        for video in videos:
            published_at = video.get('publishedAt', '')
            try:
                if isinstance(published_at, str):
                    publish_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                else:
                    publish_date = published_at
            except:
                continue
            
            period_key = self._get_period_key(publish_date)
            view_count = video.get('viewCount', 0)
            
            if view_count > 0:
                periods[period_key]['views'].append(view_count)
                periods[period_key]['count'] += 1
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„å¹³å‡æ’­æ”¾é‡
        period_stats = {}
        for period_key, data in periods.items():
            if data['views']:
                avg_views = np.mean(data['views'])
                median_views = np.median(data['views'])
                period_stats[period_key] = {
                    'avg_views': float(avg_views) if not np.isnan(avg_views) else 0.0,
                    'median_views': float(median_views) if not np.isnan(median_views) else 0.0,
                    'count': int(data['count'])
                }
        
        return period_stats
    
    def _get_period_key(self, date: datetime) -> str:
        """
        è·å–æ—¶é—´æ®µé”®ï¼ˆæŒ‰æœˆåˆ†ç»„ï¼‰
        """
        return f"{date.year}-{date.month:02d}"
    
    def _extract_keywords_from_title(self, title: str) -> List[str]:
        """
        ä»æ ‡é¢˜æå–å…³é”®è¯
        """
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
        words = title.lower().split()
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        return keywords[:5]  # è¿”å›å‰5ä¸ªå…³é”®è¯
    
    def _calculate_accuracy_metrics(
        self,
        predictions: List[float],
        actuals: List[float]
    ) -> Dict:
        """
        è®¡ç®—å‡†ç¡®åº¦æŒ‡æ ‡
        """
        if not predictions or not actuals or len(predictions) != len(actuals):
            return {
                'mae': 0,
                'mape': 0,
                'rmse': 0,
                'r2_score': 0,
                'correlation': 0
            }
        
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        # å¹³å‡ç»å¯¹è¯¯å·®
        mae = np.mean(np.abs(predictions - actuals))
        
        # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
        mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100
        
        # å‡æ–¹æ ¹è¯¯å·®
        rmse = np.sqrt(np.mean((predictions - actuals) ** 2))
        
        # RÂ² åˆ†æ•°
        ss_res = np.sum((actuals - predictions) ** 2)
        ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        # ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(predictions, actuals)[0, 1] if len(predictions) > 1 else 0
        
        return {
            'mae': float(mae) if not np.isnan(mae) else 0.0,
            'mape': float(mape) if not np.isnan(mape) else 0.0,
            'rmse': float(rmse) if not np.isnan(rmse) else 0.0,
            'r2_score': float(r2_score) if not np.isnan(r2_score) else 0.0,
            'correlation': float(correlation) if not np.isnan(correlation) else 0.0
        }
    
    def _identify_top_outliers(
        self,
        backtest_results: List[Dict],
        time_periods: Dict,
        videos: Optional[List[Dict]] = None
    ) -> List[Dict]:
        """
        è¯†åˆ«ä¼˜ç§€è¡¨ç°è§†é¢‘ï¼ˆoutlierï¼‰
        
        æ ‡å‡†ï¼šé«˜äºåŒæœŸå¹³å‡1.2å€ä»¥ä¸Šï¼Œä¸”æŒ‰outlier_ratioæ’åº
        å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„outlierï¼Œåˆ™æ˜¾ç¤ºè¡¨ç°æœ€å¥½çš„Top 5è§†é¢‘
        """
        outliers = [
            r for r in backtest_results
            if r.get('is_outlier', False) and r.get('actual_views', 0) > 0
        ]
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„outlierï¼ˆå°‘äº5ä¸ªï¼‰ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰è§†é¢‘æŒ‰outlier_ratioæ’åº
        if len(outliers) < 5:
            # ä½¿ç”¨æ‰€æœ‰æœ‰å®é™…æ’­æ”¾é‡çš„è§†é¢‘ï¼ŒæŒ‰outlier_ratioæ’åº
            all_videos = [
                r for r in backtest_results
                if r.get('actual_views', 0) > 0 and r.get('outlier_ratio', 0) > 0
            ]
            outliers_sorted = sorted(
                all_videos,
                key=lambda x: x.get('outlier_ratio', 0),
                reverse=True
            )
            # å–å‰5ä¸ªï¼Œå³ä½¿ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„outlier
            top_5 = outliers_sorted[:5]
        else:
            # æŒ‰outlier_ratioæ’åºï¼ˆè¡¨ç°è¶…å‡ºåŒæœŸå¹³å‡çš„å€æ•°ï¼‰
            outliers_sorted = sorted(
                outliers,
                key=lambda x: x.get('outlier_ratio', 0),
                reverse=True
            )
            # å–å‰5ä¸ª
            top_5 = outliers_sorted[:5]
        
        # ä¸ºæ¯ä¸ªoutlieræ·»åŠ åˆ†æ
        analyzed_outliers = []
        for outlier in top_5:
            # ä»åŸå§‹è§†é¢‘æ•°æ®ä¸­è·å–å®Œæ•´ä¿¡æ¯
            video_id = outlier.get('video_id', '')
            video_data = {}
            
            # å¦‚æœæœ‰videosåˆ—è¡¨ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„è§†é¢‘æ•°æ®
            if videos:
                for video in videos:
                    if video.get('videoId') == video_id:
                        video_data = {
                            'title': video.get('title', outlier.get('title', '')),
                            'description': video.get('description', ''),
                            'likeCount': video.get('likeCount', 0),
                            'commentCount': video.get('commentCount', 0),
                            'viewCount': video.get('viewCount', 0)
                        }
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨outlierä¸­çš„åŸºæœ¬ä¿¡æ¯
            if not video_data:
                video_data = {
                    'title': outlier.get('title', ''),
                    'description': '',
                    'likeCount': 0,
                    'commentCount': 0,
                    'viewCount': outlier.get('actual_views', 0)
                }
            
            analysis = self._analyze_outlier_video(outlier, backtest_results, video_data)
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            cleaned_outlier = {
                'video_id': str(outlier.get('video_id', '')),
                'title': str(outlier.get('title', '')),
                'published_at': str(outlier.get('published_at', '')) if outlier.get('published_at') else None,
                'actual_views': int(outlier.get('actual_views', 0)),
                'predicted_views': int(outlier.get('predicted_views', 0)),
                'period_avg_views': float(outlier.get('period_avg_views', 0)),
                'error': float(outlier.get('error', 0)),
                'error_percentage': float(outlier.get('error_percentage', 0)),
                'is_outlier': bool(outlier.get('is_outlier', False)),
                'outlier_ratio': float(outlier.get('outlier_ratio', 1.0)),
                'simulated_trend': outlier.get('simulated_trend', {}),
                'analysis': analysis
            }
            analyzed_outliers.append(cleaned_outlier)
        
        return analyzed_outliers
    
    def _analyze_outlier_video(
        self,
        outlier: Dict,
        all_results: List[Dict],
        video_data: Optional[Dict] = None
    ) -> Dict:
        """
        æ·±åº¦åˆ†æoutlierè§†é¢‘ä¸ºä½•çˆ†é‡ - å¢å¼ºç‰ˆ
        
        æ–°å¢åˆ†æç»´åº¦ï¼š
        1. è§†é¢‘å†…å®¹åˆ†æï¼ˆå…³é”®è¯ã€ä¸»è¦å†…å®¹æå–ï¼‰
        2. æ—¶ä¸‹çƒ­ç‚¹æå–
        3. äº’åŠ¨ç‡æ•°æ®åˆ†æ
        4. AIæ·±åº¦åˆ†æçˆ†ç«åŸå› 
        5. å¯è½åœ°ã€å¤ç”¨çš„ç†ç”±
        """
        title = outlier.get('title', '')
        video_id = outlier.get('video_id', '')
        actual_views = outlier.get('actual_views', 0)
        predicted_views = outlier.get('predicted_views', 0)
        period_avg = outlier.get('period_avg_views', 0)
        outlier_ratio = outlier.get('outlier_ratio', 1.0)
        trend_data = outlier.get('simulated_trend', {})
        published_at = outlier.get('published_at', '')
        
        # è·å–è§†é¢‘å®Œæ•´æ•°æ®ï¼ˆå¦‚æœæä¾›ï¼‰
        if video_data is None:
            video_data = {}
        
        # ========== 1. è§†é¢‘å†…å®¹åˆ†æ ==========
        content_analysis = self._analyze_video_content(title, video_data)
        
        # ========== 2. æ—¶ä¸‹çƒ­ç‚¹æå– ==========
        trending_topics = self._extract_trending_topics(published_at, trend_data, content_analysis)
        
        # ========== 3. äº’åŠ¨ç‡æ•°æ®åˆ†æ ==========
        engagement_metrics = self._analyze_engagement_metrics(video_data, actual_views, all_results)
        
        # ========== 4. ç»¼åˆåˆ†æåŸå›  ==========
        reasons = []
        
        # 4.1 äº’è”ç½‘çƒ­åº¦åˆ†æ
        viral_potential = trend_data.get('viral_potential', 50)
        if viral_potential >= 90:
            reasons.append({
                'factor': 'äº’è”ç½‘çƒ­åº¦',
                'score': viral_potential,
                'impact': 'æé«˜',
                'description': f'è¯¥è¯é¢˜åœ¨å½“æ—¶äº’è”ç½‘çƒ­åº¦æé«˜ï¼ˆ{viral_potential:.0f}åˆ†ï¼‰ï¼Œå¸å¼•äº†å¤§é‡å…³æ³¨',
                'actionable_insight': 'å»ºè®®ï¼šå…³æ³¨ç¤¾äº¤åª’ä½“è¶‹åŠ¿ï¼Œåœ¨è¯é¢˜çƒ­åº¦è¾¾åˆ°å³°å€¼å‰48å°æ—¶å†…å‘å¸ƒç›¸å…³å†…å®¹',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹çƒ­ç‚¹ç›‘æ§ç³»ç»Ÿï¼Œè®¾ç½®å…³é”®è¯æé†’ï¼Œå¿«é€Ÿå“åº”çƒ­é—¨è¯é¢˜'
            })
        elif viral_potential >= 70:
            reasons.append({
                'factor': 'äº’è”ç½‘çƒ­åº¦',
                'score': viral_potential,
                'impact': 'é«˜',
                'description': f'è¯¥è¯é¢˜åœ¨å½“æ—¶æ˜¯çƒ­é—¨è¯é¢˜ï¼ˆ{viral_potential:.0f}åˆ†ï¼‰ï¼Œæœ‰è¾ƒå¥½çš„ä¼ æ’­æ½œåŠ›',
                'actionable_insight': 'å»ºè®®ï¼šæŒç»­å…³æ³¨è¯é¢˜çƒ­åº¦å˜åŒ–ï¼Œåœ¨ä¸Šå‡æœŸå‘å¸ƒå†…å®¹',
                'reusable_strategy': 'ç­–ç•¥ï¼šæ¯å‘¨åˆ†æçƒ­é—¨è¯é¢˜è¶‹åŠ¿ï¼Œæå‰å‡†å¤‡ç›¸å…³å†…å®¹'
            })
        
        # 4.2 å†…å®¹ç›¸å…³æ€§åˆ†æ
        relevance_score = trend_data.get('relevance_score', 50)
        if relevance_score >= 80:
            reasons.append({
                'factor': 'å†…å®¹ç›¸å…³æ€§',
                'score': relevance_score,
                'impact': 'æé«˜',
                'description': f'å†…å®¹ä¸é¢‘é“æ ¸å¿ƒä¸»é¢˜é«˜åº¦ç›¸å…³ï¼ˆ{relevance_score:.0f}åˆ†ï¼‰ï¼Œç²¾å‡†åŒ¹é…ç›®æ ‡å—ä¼—',
                'actionable_insight': 'å»ºè®®ï¼šä¿æŒå†…å®¹ä¸é¢‘é“å®šä½çš„ä¸€è‡´æ€§ï¼Œæ·±åº¦æŒ–æ˜æ ¸å¿ƒä¸»é¢˜çš„ç»†åˆ†é¢†åŸŸ',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹å†…å®¹ä¸»é¢˜çŸ©é˜µï¼Œç¡®ä¿æ–°å†…å®¹ä¸æ ¸å¿ƒä¸»é¢˜æœ‰å¼ºå…³è”'
            })
        
        # 4.3 è§†é¢‘å†…å®¹å…³é”®è¯åˆ†æ
        if content_analysis.get('top_keywords'):
            top_keywords = content_analysis['top_keywords'][:5]
            reasons.append({
                'factor': 'å†…å®¹å…³é”®è¯',
                'score': 85,
                'impact': 'é«˜',
                'description': f'è§†é¢‘åŒ…å«çƒ­é—¨å…³é”®è¯ï¼š{", ".join([kw["keyword"] for kw in top_keywords])}ï¼Œè¿™äº›å…³é”®è¯åœ¨å½“æ—¶æœç´¢é‡è¾ƒé«˜',
                'actionable_insight': f'å»ºè®®ï¼šåœ¨æ ‡é¢˜å’Œæè¿°ä¸­è‡ªç„¶èå…¥è¿™äº›å…³é”®è¯ï¼š{", ".join([kw["keyword"] for kw in top_keywords])}',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹å…³é”®è¯åº“ï¼Œå®šæœŸæ›´æ–°çƒ­é—¨å…³é”®è¯ï¼Œåœ¨å†…å®¹ä¸­è‡ªç„¶èå…¥'
            })
        
        # 4.4 æ—¶ä¸‹çƒ­ç‚¹åŒ¹é…
        if trending_topics.get('matched_trends'):
            matched = trending_topics['matched_trends'][:3]
            reasons.append({
                'factor': 'æ—¶ä¸‹çƒ­ç‚¹åŒ¹é…',
                'score': 90,
                'impact': 'æé«˜',
                'description': f'è§†é¢‘å†…å®¹ä¸æ—¶ä¸‹çƒ­ç‚¹é«˜åº¦åŒ¹é…ï¼š{", ".join([t["topic"] for t in matched])}',
                'actionable_insight': f'å»ºè®®ï¼šå…³æ³¨è¿™äº›çƒ­ç‚¹è¯é¢˜çš„åç»­å‘å±•ï¼Œåˆ¶ä½œç³»åˆ—å†…å®¹ï¼š{", ".join([t["topic"] for t in matched])}',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹çƒ­ç‚¹è¿½è¸ªç³»ç»Ÿï¼Œåˆ†æçƒ­ç‚¹è¯é¢˜çš„ç”Ÿå‘½å‘¨æœŸï¼Œåœ¨æœ€ä½³æ—¶æœºå‘å¸ƒ'
            })
        
        # 4.5 äº’åŠ¨ç‡åˆ†æ
        if engagement_metrics.get('engagement_rate') > 0:
            engagement_rate = engagement_metrics['engagement_rate']
            if engagement_rate > 0.01:  # 1%ä»¥ä¸Š
                reasons.append({
                    'factor': 'é«˜äº’åŠ¨ç‡',
                    'score': min(100, engagement_rate * 1000),
                    'impact': 'é«˜',
                    'description': f'è§†é¢‘äº’åŠ¨ç‡é«˜è¾¾{engagement_rate*100:.2f}%ï¼Œè¿œè¶…å¹³å‡æ°´å¹³ï¼Œè¯´æ˜å†…å®¹å¼•å‘å¼ºçƒˆå…±é¸£',
                    'actionable_insight': f'å»ºè®®ï¼šåˆ†æè¯¥è§†é¢‘çš„äº’åŠ¨ç‚¹ï¼ˆè¯„è®ºã€ç‚¹èµã€åˆ†äº«ï¼‰ï¼Œåœ¨åç»­å†…å®¹ä¸­å¤ç°è¿™äº›å…ƒç´ ',
                    'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹äº’åŠ¨ç‡åˆ†ææ¨¡æ¿ï¼Œè¯†åˆ«é«˜äº’åŠ¨å†…å®¹ç‰¹å¾ï¼Œåœ¨å†…å®¹ç­–åˆ’æ—¶ä¼˜å…ˆè€ƒè™‘'
                })
        
        # 4.6 è¡¨ç°è¶…å‡ºé¢„æœŸ
        if actual_views > predicted_views * 1.2:
            overperformance = ((actual_views / predicted_views - 1) * 100)
            reasons.append({
                'factor': 'è¡¨ç°è¶…å‡ºé¢„æœŸ',
                'score': min(100, overperformance),
                'impact': 'é«˜',
                'description': f'å®é™…æ’­æ”¾é‡è¶…å‡ºé¢„æµ‹{overperformance:.0f}%ï¼Œè¯´æ˜æœ‰å…¶ä»–æˆåŠŸå› ç´ ',
                'actionable_insight': 'å»ºè®®ï¼šæ·±å…¥åˆ†æè¯¥è§†é¢‘çš„ç‹¬ç‰¹ä¹‹å¤„ï¼ˆæ ‡é¢˜ã€ç¼©ç•¥å›¾ã€å†…å®¹ç»“æ„ã€å‘å¸ƒæ—¶é—´ç­‰ï¼‰ï¼Œæ‰¾å‡ºå¯å¤åˆ¶çš„æˆåŠŸæ¨¡å¼',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹"è¶…é¢„æœŸè¡¨ç°"åˆ†ææ¡†æ¶ï¼Œå®šæœŸå¤ç›˜ä¼˜ç§€è§†é¢‘ï¼Œæç‚¼å¯å¤ç”¨çš„æˆåŠŸè¦ç´ '
            })
        
        # 4.7 åŒæœŸå¯¹æ¯”
        if outlier_ratio > 2.0:
            reasons.append({
                'factor': 'åŒæœŸè¡¨ç°',
                'score': min(100, outlier_ratio * 20),
                'impact': 'æé«˜',
                'description': f'æ’­æ”¾é‡æ˜¯åŒæœŸå¹³å‡çš„{outlier_ratio:.1f}å€ï¼Œè¡¨ç°å¼‚å¸¸çªå‡º',
                'actionable_insight': f'å»ºè®®ï¼šåˆ†æè¯¥è§†é¢‘åœ¨åŒæœŸè§†é¢‘ä¸­çš„å·®å¼‚åŒ–ä¼˜åŠ¿ï¼Œå¯èƒ½æ˜¯å‘å¸ƒæ—¶é—´ã€å†…å®¹è§’åº¦æˆ–æ¨å¹¿ç­–ç•¥',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹åŒæœŸå¯¹æ¯”åˆ†ææœºåˆ¶ï¼Œè¯†åˆ«è¡¨ç°çªå‡ºçš„è§†é¢‘ï¼Œæ€»ç»“æˆåŠŸç»éªŒ'
            })
        
        # 4.8 æ ‡é¢˜ä¼˜åŒ–åˆ†æ
        title_length = len(title)
        if 30 <= title_length <= 60:
            reasons.append({
                'factor': 'æ ‡é¢˜ä¼˜åŒ–',
                'score': 85,
                'impact': 'ä¸­',
                'description': f'æ ‡é¢˜é•¿åº¦é€‚ä¸­ï¼ˆ{title_length}å­—ç¬¦ï¼‰ï¼Œç¬¦åˆYouTubeæœ€ä½³å®è·µï¼ŒåŒ…å«å¸å¼•äººçš„å…³é”®è¯',
                'actionable_insight': f'å»ºè®®ï¼šä¿æŒæ ‡é¢˜é•¿åº¦åœ¨30-60å­—ç¬¦ä¹‹é—´ï¼Œç¡®ä¿åœ¨ç§»åŠ¨ç«¯å®Œæ•´æ˜¾ç¤ºï¼Œå¹¶åŒ…å«æ ¸å¿ƒå…³é”®è¯',
                'reusable_strategy': 'ç­–ç•¥ï¼šå»ºç«‹æ ‡é¢˜æ¨¡æ¿åº“ï¼Œæ ¹æ®ä¸åŒå†…å®¹ç±»å‹ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜ç»“æ„'
            })
        
        # ========== 5. AIæ·±åº¦åˆ†æçˆ†ç«åŸå›  ==========
        ai_analysis = self._generate_ai_analysis(
            title,
            content_analysis,
            trending_topics,
            engagement_metrics,
            reasons,
            outlier_ratio
        )
        
        # ========== 6. å¯è½åœ°ã€å¤ç”¨çš„ç†ç”± ==========
        actionable_recommendations = self._generate_actionable_recommendations(
            reasons,
            content_analysis,
            trending_topics,
            engagement_metrics
        )
        
        # è®¡ç®—ç»¼åˆæˆåŠŸå› ç´ 
        success_factors = {
            'viral_potential': float(viral_potential),
            'relevance_score': float(relevance_score),
            'outlier_ratio': float(outlier_ratio),
            'title_optimization': float(85 if 30 <= title_length <= 60 else 50),
            'engagement_rate': float(engagement_metrics.get('engagement_rate', 0)),
            'content_quality_score': float(content_analysis.get('quality_score', 50))
        }
        
        # ç¡®ä¿reasonsä¸­çš„æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
        cleaned_reasons = []
        for reason in reasons:
            cleaned_reasons.append({
                'factor': str(reason.get('factor', '')),
                'score': float(reason.get('score', 0)),
                'impact': str(reason.get('impact', '')),
                'description': str(reason.get('description', '')),
                'actionable_insight': str(reason.get('actionable_insight', '')),
                'reusable_strategy': str(reason.get('reusable_strategy', ''))
            })
        
        return {
            'reasons': cleaned_reasons,
            'success_factors': success_factors,
            'content_analysis': content_analysis,
            'trending_topics': trending_topics,
            'engagement_metrics': engagement_metrics,
            'ai_analysis': ai_analysis,
            'actionable_recommendations': actionable_recommendations,
            'summary': str(self._generate_outlier_summary(outlier, reasons))
        }
    
    def _analyze_video_content(self, title: str, video_data: Dict) -> Dict:
        """
        åˆ†æè§†é¢‘å†…å®¹ï¼šæå–å…³é”®è¯å’Œä¸»è¦å†…å®¹
        """
        description = video_data.get('description', '')
        combined_text = f"{title} {description}".strip()
        
        # ä½¿ç”¨content_analyzeræå–å…³é”®è¯
        if combined_text:
            topics = content_analyzer.extract_topics_from_titles([title])
            if description:
                # å¦‚æœæœ‰æè¿°ï¼Œä¹Ÿåˆ†ææè¿°
                desc_topics = content_analyzer.extract_topics_from_titles([description[:500]])  # é™åˆ¶é•¿åº¦
                topics.extend(desc_topics)
        else:
            topics = []
        
        # æå–ä¸»è¦å…³é”®è¯ï¼ˆTop 10ï¼‰
        top_keywords = [
            {
                'keyword': t['topic'],
                'score': float(t.get('score', 0)),
                'type': t.get('type', 'unknown')
            }
            for t in topics[:10]
        ]
        
        # åˆ†æå†…å®¹ä¸»é¢˜
        content_themes = self._extract_content_themes(title, description)
        
        # è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°
        quality_score = self._calculate_content_quality_score(title, description, topics)
        
        return {
            'top_keywords': top_keywords,
            'content_themes': content_themes,
            'quality_score': quality_score,
            'title_length': len(title),
            'description_length': len(description)
        }
    
    def _extract_content_themes(self, title: str, description: str) -> List[str]:
        """
        æå–å†…å®¹ä¸»é¢˜
        """
        themes = []
        combined = f"{title} {description}".lower()
        
        # ä¸»é¢˜å…³é”®è¯æ˜ å°„
        theme_keywords = {
            'æ•™ç¨‹/æ•™è‚²': ['how to', 'tutorial', 'guide', 'learn', 'teach', 'explain'],
            'è¯„æµ‹/å¯¹æ¯”': ['review', 'vs', 'compare', 'test', 'unboxing'],
            'å¨±ä¹/è¶£å‘³': ['funny', 'prank', 'challenge', 'compilation', 'fails'],
            'æ–°é—»/èµ„è®¯': ['news', 'update', 'breaking', 'latest', 'announcement'],
            'ç§‘æŠ€/äº§å“': ['tech', 'gadget', 'phone', 'laptop', 'device'],
            'ç”Ÿæ´»/æ—¥å¸¸': ['vlog', 'daily', 'life', 'routine', 'day in'],
            'æ¸¸æˆ': ['game', 'gaming', 'gameplay', 'walkthrough', 'playthrough']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(kw in combined for kw in keywords):
                themes.append(theme)
        
        return themes[:3]  # è¿”å›æœ€å¤š3ä¸ªä¸»é¢˜
    
    def _calculate_content_quality_score(self, title: str, description: str, topics: List[Dict]) -> float:
        """
        è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°
        """
        score = 50  # åŸºç¡€åˆ†æ•°
        
        # æ ‡é¢˜é•¿åº¦ä¼˜åŒ–
        if 30 <= len(title) <= 60:
            score += 10
        elif len(title) < 30:
            score += 5
        
        # æè¿°é•¿åº¦ï¼ˆæœ‰æè¿°æ›´å¥½ï¼‰
        if len(description) > 100:
            score += 10
        elif len(description) > 50:
            score += 5
        
        # å…³é”®è¯ä¸°å¯Œåº¦
        if len(topics) >= 5:
            score += 15
        elif len(topics) >= 3:
            score += 10
        
        return min(100, score)
    
    def _extract_trending_topics(self, published_at: str, trend_data: Dict, content_analysis: Dict) -> Dict:
        """
        æå–æ—¶ä¸‹çƒ­ç‚¹
        """
        # ä»è¶‹åŠ¿æ•°æ®ä¸­æå–å…³é”®è¯
        keywords = trend_data.get('keywords', [])
        
        # ä»å†…å®¹åˆ†æä¸­æå–å…³é”®è¯
        content_keywords = [kw['keyword'] for kw in content_analysis.get('top_keywords', [])]
        
        # åˆå¹¶å¹¶å»é‡
        all_keywords = list(set(keywords + content_keywords))
        
        # åŒ¹é…çš„çƒ­ç‚¹è¯é¢˜
        matched_trends = [
            {
                'topic': kw,
                'relevance': 85,
                'source': 'content_analysis' if kw in content_keywords else 'trend_data'
            }
            for kw in all_keywords[:5]
        ]
        
        return {
            'matched_trends': matched_trends,
            'trending_keywords': all_keywords[:10],
            'viral_potential': trend_data.get('viral_potential', 50)
        }
    
    def _analyze_engagement_metrics(self, video_data: Dict, actual_views: int, all_results: List[Dict]) -> Dict:
        """
        åˆ†æäº’åŠ¨ç‡æ•°æ®
        """
        like_count = video_data.get('likeCount', 0)
        comment_count = video_data.get('commentCount', 0)
        # share_count = video_data.get('shareCount', 0)  # å¦‚æœæœ‰åˆ†äº«æ•°æ®
        
        # è®¡ç®—äº’åŠ¨ç‡
        engagement_rate = 0.0
        if actual_views > 0:
            total_engagement = like_count + comment_count * 2  # è¯„è®ºæƒé‡æ›´é«˜
            engagement_rate = total_engagement / actual_views
        
        # è®¡ç®—å¹³å‡äº’åŠ¨ç‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        avg_engagement_rate = 0.0
        if all_results:
            total_avg_engagement = 0
            count = 0
            for result in all_results:
                views = result.get('actual_views', 0)
                if views > 0:
                    # å‡è®¾å…¶ä»–è§†é¢‘çš„äº’åŠ¨æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼°ç®—å€¼
                    # å®é™…åº”ç”¨ä¸­åº”è¯¥ä»video_dataä¸­è·å–
                    count += 1
            if count > 0:
                # ä½¿ç”¨è¡Œä¸šå¹³å‡äº’åŠ¨ç‡ï¼ˆçº¦0.5%ï¼‰
                avg_engagement_rate = 0.005
        
        # è®¡ç®—äº’åŠ¨ç‡å€æ•°
        engagement_multiplier = engagement_rate / avg_engagement_rate if avg_engagement_rate > 0 else 1.0
        
        return {
            'like_count': int(like_count),
            'comment_count': int(comment_count),
            'engagement_rate': float(engagement_rate),
            'avg_engagement_rate': float(avg_engagement_rate),
            'engagement_multiplier': float(engagement_multiplier),
            'engagement_level': (
                'æé«˜' if engagement_rate > 0.02 else
                'é«˜' if engagement_rate > 0.01 else
                'ä¸­ç­‰' if engagement_rate > 0.005 else
                'ä½'
            )
        }
    
    def _generate_ai_analysis(
        self,
        title: str,
        content_analysis: Dict,
        trending_topics: Dict,
        engagement_metrics: Dict,
        reasons: List[Dict],
        outlier_ratio: float
    ) -> Dict:
        """
        AIæ·±åº¦åˆ†æçˆ†ç«åŸå› 
        """
        # ç»¼åˆåˆ†ææ‰€æœ‰å› ç´ 
        primary_factors = []
        secondary_factors = []
        
        for reason in reasons:
            if reason.get('impact') in ['æé«˜', 'é«˜']:
                primary_factors.append(reason.get('factor', ''))
            else:
                secondary_factors.append(reason.get('factor', ''))
        
        # ç”ŸæˆAIåˆ†ææ€»ç»“
        analysis_parts = []
        
        # æ ¸å¿ƒæˆåŠŸå› ç´ 
        if primary_factors:
            analysis_parts.append(f"æ ¸å¿ƒæˆåŠŸå› ç´ ï¼š{', '.join(primary_factors)}")
        
        # å†…å®¹è´¨é‡
        quality_score = content_analysis.get('quality_score', 50)
        if quality_score >= 80:
            analysis_parts.append("å†…å®¹è´¨é‡ä¼˜ç§€ï¼Œå…³é”®è¯ä¸°å¯Œï¼Œä¸»é¢˜æ˜ç¡®")
        
        # çƒ­ç‚¹åŒ¹é…
        if trending_topics.get('matched_trends'):
            analysis_parts.append("å†…å®¹ä¸æ—¶ä¸‹çƒ­ç‚¹é«˜åº¦åŒ¹é…ï¼ŒæŠ“ä½äº†æµé‡çº¢åˆ©")
        
        # äº’åŠ¨è¡¨ç°
        engagement_rate = engagement_metrics.get('engagement_rate', 0)
        if engagement_rate > 0.01:
            analysis_parts.append(f"äº’åŠ¨ç‡è¡¨ç°çªå‡ºï¼ˆ{engagement_rate*100:.2f}%ï¼‰ï¼Œè¯´æ˜å†…å®¹å¼•å‘å¼ºçƒˆå…±é¸£")
        
        # ç»¼åˆè¯„ä¼°
        if outlier_ratio > 2.0:
            analysis_parts.append(f"æ’­æ”¾é‡æ˜¯åŒæœŸå¹³å‡çš„{outlier_ratio:.1f}å€ï¼Œå±äºçˆ†æ¬¾å†…å®¹")
        
        ai_summary = "ï¼›".join(analysis_parts) if analysis_parts else "ç»¼åˆåˆ†ææ˜¾ç¤ºè¯¥è§†é¢‘åœ¨å¤šä¸ªç»´åº¦è¡¨ç°çªå‡º"
        
        return {
            'primary_factors': primary_factors,
            'secondary_factors': secondary_factors,
            'analysis_summary': ai_summary,
            'success_probability': min(100, 50 + (outlier_ratio - 1) * 20)
        }
    
    def _generate_actionable_recommendations(
        self,
        reasons: List[Dict],
        content_analysis: Dict,
        trending_topics: Dict,
        engagement_metrics: Dict
    ) -> Dict:
        """
        ç”Ÿæˆå¯è½åœ°ã€å¤ç”¨çš„ç†ç”±å’Œå»ºè®®
        """
        recommendations = {
            'immediate_actions': [],
            'strategic_actions': [],
            'reusable_templates': []
        }
        
        # ç«‹å³è¡ŒåŠ¨
        for reason in reasons:
            if reason.get('actionable_insight'):
                recommendations['immediate_actions'].append({
                    'action': reason.get('actionable_insight', ''),
                    'priority': 'high' if reason.get('impact') in ['æé«˜', 'é«˜'] else 'medium'
                })
        
        # æˆ˜ç•¥è¡ŒåŠ¨
        for reason in reasons:
            if reason.get('reusable_strategy'):
                recommendations['strategic_actions'].append({
                    'strategy': reason.get('reusable_strategy', ''),
                    'category': reason.get('factor', '')
                })
        
        # å¯å¤ç”¨æ¨¡æ¿
        top_keywords = content_analysis.get('top_keywords', [])
        if top_keywords:
            recommendations['reusable_templates'].append({
                'type': 'å…³é”®è¯æ¨¡æ¿',
                'template': f"åœ¨æ ‡é¢˜å’Œæè¿°ä¸­è‡ªç„¶èå…¥è¿™äº›å…³é”®è¯ï¼š{', '.join([kw['keyword'] for kw in top_keywords[:5]])}",
                'usage': 'é€‚ç”¨äºæ‰€æœ‰å†…å®¹ç±»å‹'
            })
        
        # æ ‡é¢˜æ¨¡æ¿
        title_length = content_analysis.get('title_length', 0)
        if 30 <= title_length <= 60:
            recommendations['reusable_templates'].append({
                'type': 'æ ‡é¢˜é•¿åº¦æ¨¡æ¿',
                'template': f'ä¿æŒæ ‡é¢˜é•¿åº¦åœ¨30-60å­—ç¬¦ä¹‹é—´ï¼ˆå½“å‰{title_length}å­—ç¬¦ï¼‰',
                'usage': 'é€‚ç”¨äºæ‰€æœ‰è§†é¢‘æ ‡é¢˜'
            })
        
        return recommendations
    
    def _generate_outlier_summary(
        self,
        outlier: Dict,
        reasons: List[Dict]
    ) -> str:
        """
        ç”Ÿæˆoutlierè§†é¢‘çš„æ€»ç»“
        """
        title = outlier.get('title', '')
        actual_views = outlier.get('actual_views', 0)
        outlier_ratio = outlier.get('outlier_ratio', 1.0)
        
        summary_parts = [
            f"ã€Š{title}ã€‹è¡¨ç°å¼‚å¸¸çªå‡ºï¼Œæ’­æ”¾é‡è¾¾åˆ°{actual_views:,.0f}æ¬¡ï¼Œ"
            f"æ˜¯åŒæœŸå¹³å‡æ°´å¹³çš„{outlier_ratio:.1f}å€ã€‚"
        ]
        
        if reasons:
            top_reason = reasons[0]
            summary_parts.append(
                f"ä¸»è¦åŸå› ï¼š{top_reason['description']}"
            )
        
        return " ".join(summary_parts)


# å¯¼å‡º
__all__ = ['BacktestAnalyzer']
