"""
Enhanced Social Media Signal Collector with Rate Limiting, Caching, and Deep Analysis
MVP 3.0 - Cross-Platform Signal Enhancement
"""

import asyncio
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from collections import Counter
import re
import json
import hashlib
from functools import wraps
import time
import numpy as np

# Redis for caching (optional but recommended)
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸ Redis not available. Install: pip install redis")

# Twitter API
try:
    import tweepy
    TWITTER_AVAILABLE = True
except ImportError:
    TWITTER_AVAILABLE = False

# Reddit API
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False

# Google Trends
try:
    from pytrends.request import TrendReq
    GOOGLE_TRENDS_AVAILABLE = True
except ImportError:
    GOOGLE_TRENDS_AVAILABLE = False
    print("âš ï¸ Pytrends not available")

# SerpAPI
try:
    from serpapi import GoogleSearch
    SERPAPI_AVAILABLE = True
except ImportError:
    SERPAPI_AVAILABLE = False
    print("âš ï¸ SerpAPI not available. Install: pip install google-search-results")


class RateLimiter:
    """
    æ™ºèƒ½é€Ÿç‡é™åˆ¶å™¨ - è‡ªåŠ¨è°ƒèŠ‚è¯·æ±‚é¢‘ç‡
    """
    def __init__(self, max_calls: int, time_window: int):
        self.max_calls = max_calls
        self.time_window = time_window  # seconds
        self.calls = []
    
    def __call__(self, func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            now = time.time()
            
            # æ¸…ç†è¿‡æœŸçš„è°ƒç”¨è®°å½•
            self.calls = [call_time for call_time in self.calls 
                         if now - call_time < self.time_window]
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.calls) >= self.max_calls:
                wait_time = self.time_window - (now - self.calls[0])
                if wait_time > 0:
                    print(f"â³ Rate limit reached, waiting {wait_time:.1f}s...")
                    await asyncio.sleep(wait_time + 1)
                    self.calls = []
            
            # è®°å½•æœ¬æ¬¡è°ƒç”¨
            self.calls.append(time.time())
            return await func(*args, **kwargs)
        
        return wrapper


class CacheManager:
    """
    ç¼“å­˜ç®¡ç†å™¨ - å‡å°‘APIè°ƒç”¨ï¼Œæå‡å“åº”é€Ÿåº¦
    """
    def __init__(self, redis_url: Optional[str] = None, ttl: int = 3600):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            redis_url: Redis è¿æ¥ URL (ä¾‹å¦‚: redis://localhost:6379)
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 1 å°æ—¶
        """
        self.ttl = ttl  # Time to live in seconds
        self.redis_client = None
        self.local_cache = {}  # Fallback to memory cache
        
        if REDIS_AVAILABLE and redis_url:
            try:
                self.redis_client = redis.from_url(redis_url)
                self.redis_client.ping()
                print("âœ… Redis cache connected")
            except Exception as e:
                print(f"âš ï¸ Redis connection failed: {e}, using local cache")
    
    def _generate_key(self, prefix: str, params: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        params_str = json.dumps(params, sort_keys=True)
        hash_key = hashlib.md5(params_str.encode()).hexdigest()
        return f"{prefix}:{hash_key}"
    
    async def get(self, key: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        if self.redis_client:
            try:
                data = self.redis_client.get(key)
                if data:
                    return json.loads(data)
            except Exception as e:
                print(f"Cache get error: {e}")
        
        # Fallback to local cache
        if key in self.local_cache:
            item = self.local_cache[key]
            if time.time() - item['timestamp'] < self.ttl:
                return item['data']
            else:
                del self.local_cache[key]
        
        return None
    
    async def set(self, key: str, data: dict):
        """è®¾ç½®ç¼“å­˜"""
        if self.redis_client:
            try:
                self.redis_client.setex(
                    key, 
                    self.ttl, 
                    json.dumps(data)
                )
                return
            except Exception as e:
                print(f"Cache set error: {e}")
        
        # Fallback to local cache
        self.local_cache[key] = {
            'data': data,
            'timestamp': time.time()
        }


class EnhancedTwitterCollector:
    """
    å¢å¼ºç‰ˆ Twitter æ”¶é›†å™¨ - æ”¯æŒé€Ÿç‡é™åˆ¶ã€ç¼“å­˜ã€æ·±åº¦åˆ†æ
    """
    def __init__(self, bearer_token: Optional[str] = None, cache_manager: Optional[CacheManager] = None):
        self.bearer_token = bearer_token
        self.client = None
        self.cache = cache_manager or CacheManager()
        
        if TWITTER_AVAILABLE and bearer_token:
            try:
                self.client = tweepy.Client(
                    bearer_token=bearer_token,
                    wait_on_rate_limit=False  # å¿«é€Ÿå¤±è´¥ï¼Œä¸ç­‰å¾…é€Ÿç‡é™åˆ¶
                )
                print("âœ… Twitter API initialized (fast-fail mode)")
            except Exception as e:
                print(f"âš ï¸ Twitter API initialization failed: {e}")
    
    @RateLimiter(max_calls=15, time_window=900)  # 15 calls per 15 minutes
    async def get_trending_topics(self, keywords: List[str], limit: int = 100) -> List[Dict]:
        """
        æ”¶é›† Twitter è¶‹åŠ¿æ•°æ®ï¼ˆå¸¦ç¼“å­˜å’Œé€Ÿç‡é™åˆ¶ï¼‰
        """
        trending_topics = []
        
        for keyword in keywords[:5]:  # Limit keywords to avoid overload
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.cache._generate_key('twitter', {'keyword': keyword, 'limit': limit})
            cached_data = await self.cache.get(cache_key)
            
            if cached_data:
                print(f"ğŸ“¦ Using cached Twitter data for '{keyword}'")
                trending_topics.append(cached_data)
                continue
            
            # å¦‚æœæ²¡æœ‰å®¢æˆ·ç«¯ï¼Œè·³è¿‡ï¼ˆä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
            if not self.client:
                print(f"âš ï¸ Twitter API not available for '{keyword}'")
                continue
            
            try:
                # æœç´¢æ¨æ–‡
                tweets = self.client.search_recent_tweets(
                    query=f"{keyword} -is:retweet -is:reply lang:en",
                    max_results=min(limit, 100),
                    tweet_fields=['public_metrics', 'created_at', 'entities'],
                    expansions=['author_id']
                )
                
                if not tweets.data:
                    print(f"â„¹ï¸ No Twitter data found for '{keyword}'")
                    continue
                
                # æ·±åº¦åˆ†æ
                analysis = self._deep_analyze_tweets(tweets.data, keyword)
                
                trend_data = {
                    'keyword': keyword,
                    'source': 'twitter',
                    'engagement_score': analysis['engagement_score'],
                    'tweet_count': len(tweets.data),
                    'related_hashtags': analysis['top_hashtags'],
                    'sentiment': analysis['sentiment'],
                    'velocity': analysis['velocity'],  # æ–°å¢ï¼šè¶‹åŠ¿é€Ÿåº¦
                    'influencer_ratio': analysis['influencer_ratio'],  # æ–°å¢ï¼šå½±å“åŠ›æ¯”ä¾‹
                    'trend_score': self._calculate_twitter_trend_score(analysis),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                # ç¼“å­˜ç»“æœï¼ˆ1å°æ—¶ï¼‰
                await self.cache.set(cache_key, trend_data)
                trending_topics.append(trend_data)
                
                # é¿å…é€Ÿç‡é™åˆ¶
                await asyncio.sleep(1)
                
            except tweepy.errors.TooManyRequests as e:
                print(f"âš ï¸ Twitter rate limit hit for '{keyword}', skipping (fast-fail)")
                # è¿”å›ç©ºæ•°æ®ï¼Œä¸ç­‰å¾…
                continue
            except tweepy.errors.TweepyException as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶ç›¸å…³çš„é”™è¯¯
                error_str = str(e).lower()
                if "429" in str(e) or "rate limit" in error_str or "too many requests" in error_str:
                    print(f"âš ï¸ Twitter rate limit detected for '{keyword}', skipping (fast-fail)")
                    continue
                print(f"âŒ Twitter error for '{keyword}': {type(e).__name__} - {e}")
                continue
            except Exception as e:
                print(f"âŒ Twitter error for '{keyword}': {type(e).__name__} - {e}")
                continue
        
        return trending_topics
    
    def _deep_analyze_tweets(self, tweets: List, keyword: str) -> Dict:
        """
        æ·±åº¦åˆ†ææ¨æ–‡æ•°æ®
        """
        if not tweets:
            return self._empty_analysis()
        
        # 1. è®¡ç®—å‚ä¸åº¦
        total_engagement = 0
        high_engagement_count = 0
        
        for tweet in tweets:
            metrics = tweet.public_metrics
            engagement = (
                metrics['like_count'] + 
                metrics['retweet_count'] * 2 + 
                metrics['reply_count'] * 1.5
            )
            total_engagement += engagement
            
            # é«˜å‚ä¸åº¦æ¨æ–‡ï¼ˆè¶…è¿‡100äº’åŠ¨ï¼‰
            if engagement > 100:
                high_engagement_count += 1
        
        avg_engagement = total_engagement / len(tweets)
        
        # 2. æå–æ ‡ç­¾
        hashtags = []
        for tweet in tweets:
            if hasattr(tweet, 'entities') and tweet.entities:
                if 'hashtags' in tweet.entities:
                    hashtags.extend([h['tag'].lower() for h in tweet.entities['hashtags']])
        
        top_hashtags = [tag for tag, count in Counter(hashtags).most_common(10)]
        
        # 3. æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        sentiment = self._analyze_sentiment(tweets)
        
        # 4. è¶‹åŠ¿é€Ÿåº¦ï¼ˆæœ€è¿‘æ¨æ–‡çš„æ—¶é—´åˆ†å¸ƒï¼‰
        velocity = self._calculate_velocity(tweets)
        
        # 5. å½±å“åŠ›æ¯”ä¾‹ï¼ˆé«˜å‚ä¸åº¦æ¨æ–‡å æ¯”ï¼‰
        influencer_ratio = (high_engagement_count / len(tweets)) * 100
        
        return {
            'engagement_score': avg_engagement,
            'top_hashtags': top_hashtags,
            'sentiment': sentiment,
            'velocity': velocity,
            'influencer_ratio': influencer_ratio
        }
    
    def _analyze_sentiment(self, tweets: List) -> str:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ"""
        positive_words = {'great', 'amazing', 'awesome', 'excellent', 'best', 'love', 'perfect', 'excited'}
        negative_words = {'bad', 'worst', 'terrible', 'awful', 'hate', 'horrible', 'disappointing', 'issue'}
        
        pos_count = 0
        neg_count = 0
        
        for tweet in tweets:
            text = tweet.text.lower()
            pos_count += sum(1 for word in positive_words if word in text)
            neg_count += sum(1 for word in negative_words if word in text)
        
        if pos_count > neg_count * 1.5:
            return 'positive'
        elif neg_count > pos_count * 1.5:
            return 'negative'
        else:
            return 'neutral'
    
    def _calculate_velocity(self, tweets: List) -> float:
        """
        è®¡ç®—è¶‹åŠ¿é€Ÿåº¦ï¼ˆæ¨æ–‡æ—¶é—´åˆ†å¸ƒï¼‰
        è¿”å›ï¼šæ¯å°æ—¶å¹³å‡æ¨æ–‡æ•°
        """
        if not tweets:
            return 0.0
        
        try:
            timestamps = [tweet.created_at for tweet in tweets if hasattr(tweet, 'created_at')]
            if not timestamps:
                return 0.0
            
            oldest = min(timestamps)
            newest = max(timestamps)
            time_span = (newest - oldest).total_seconds() / 3600  # hours
            
            if time_span < 0.1:  # é¿å…é™¤ä»¥0
                time_span = 0.1
            
            velocity = len(tweets) / time_span
            return round(velocity, 2)
        except:
            return 0.0
    
    def _calculate_twitter_trend_score(self, analysis: Dict) -> float:
        """
        ç»¼åˆè®¡ç®—è¶‹åŠ¿åˆ†æ•°
        """
        # å‚ä¸åº¦åˆ†æ•°ï¼ˆ0-40åˆ†ï¼‰
        engagement_score = min(40, (analysis['engagement_score'] / 200) * 40)
        
        # é€Ÿåº¦åˆ†æ•°ï¼ˆ0-30åˆ†ï¼‰
        velocity_score = min(30, (analysis['velocity'] / 10) * 30)
        
        # å½±å“åŠ›åˆ†æ•°ï¼ˆ0-20åˆ†ï¼‰
        influencer_score = min(20, (analysis['influencer_ratio'] / 20) * 20)
        
        # æƒ…æ„ŸåŠ æˆï¼ˆ0-10åˆ†ï¼‰
        sentiment_bonus = 10 if analysis['sentiment'] == 'positive' else 0
        
        total_score = engagement_score + velocity_score + influencer_score + sentiment_bonus
        return round(total_score, 2)
    
    def _empty_analysis(self) -> Dict:
        return {
            'engagement_score': 0,
            'top_hashtags': [],
            'sentiment': 'neutral',
            'velocity': 0,
            'influencer_ratio': 0
        }


class EnhancedRedditCollector:
    """
    å¢å¼ºç‰ˆ Reddit æ”¶é›†å™¨
    """
    def __init__(self, client_id: Optional[str] = None, 
                 client_secret: Optional[str] = None,
                 user_agent: str = "TrendForge/3.0",
                 cache_manager: Optional[CacheManager] = None):
        self.reddit = None
        self.cache = cache_manager or CacheManager()
        
        if REDDIT_AVAILABLE and client_id and client_secret:
            try:
                self.reddit = praw.Reddit(
                    client_id=client_id,
                    client_secret=client_secret,
                    user_agent=user_agent
                )
                print("âœ… Reddit API initialized")
            except Exception as e:
                print(f"âš ï¸ Reddit API initialization failed: {e}")
    
    @RateLimiter(max_calls=60, time_window=60)  # 60 calls per minute
    async def get_trending_topics(self, keywords: List[str]) -> List[Dict]:
        """
        æ”¶é›† Reddit è¶‹åŠ¿æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰
        """
        trending_topics = []
        
        for keyword in keywords[:5]:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.cache._generate_key('reddit', {'keyword': keyword})
            cached_data = await self.cache.get(cache_key)
            
            if cached_data:
                print(f"ğŸ“¦ Using cached Reddit data for '{keyword}'")
                trending_topics.append(cached_data)
                continue
            
            if not self.reddit:
                print(f"âš ï¸ Reddit API not available for '{keyword}'")
                continue
            
            try:
                # æœç´¢å¸–å­
                search_results = self.reddit.subreddit('all').search(
                    keyword,
                    limit=100,
                    time_filter='week',
                    sort='hot'
                )
                
                posts = list(search_results)
                
                if not posts:
                    print(f"â„¹ï¸ No Reddit data found for '{keyword}'")
                    continue
                
                # æ·±åº¦åˆ†æ
                analysis = self._deep_analyze_posts(posts, keyword)
                
                trend_data = {
                    'keyword': keyword,
                    'source': 'reddit',
                    'upvote_score': analysis['avg_upvotes'],
                    'comment_count': analysis['total_comments'],
                    'post_count': len(posts),
                    'top_subreddits': analysis['top_subreddits'],
                    'discussion_depth': analysis['discussion_depth'],  # æ–°å¢
                    'award_count': analysis['total_awards'],  # æ–°å¢
                    'trend_score': self._calculate_reddit_trend_score(analysis),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                # ç¼“å­˜ç»“æœ
                await self.cache.set(cache_key, trend_data)
                trending_topics.append(trend_data)
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ Reddit error for '{keyword}': {e}")
                continue
        
        return trending_topics
    
    def _deep_analyze_posts(self, posts: List, keyword: str) -> Dict:
        """æ·±åº¦åˆ†æ Reddit å¸–å­"""
        if not posts:
            return self._empty_analysis()
        
        total_upvotes = sum(post.score for post in posts)
        total_comments = sum(post.num_comments for post in posts)
        total_awards = sum(
            post.total_awards_received if hasattr(post, 'total_awards_received') else 0
            for post in posts
        )
        
        # Subreddit åˆ†å¸ƒ
        subreddit_counts = Counter([post.subreddit.display_name for post in posts])
        top_subreddits = [s[0] for s in subreddit_counts.most_common(5)]
        
        # è®¨è®ºæ·±åº¦ï¼ˆå¹³å‡è¯„è®ºæ•°ï¼‰
        discussion_depth = total_comments / len(posts) if posts else 0
        
        return {
            'avg_upvotes': total_upvotes / len(posts),
            'total_comments': total_comments,
            'total_awards': total_awards,
            'top_subreddits': top_subreddits,
            'discussion_depth': discussion_depth
        }
    
    def _calculate_reddit_trend_score(self, analysis: Dict) -> float:
        """è®¡ç®— Reddit è¶‹åŠ¿åˆ†æ•°"""
        upvote_score = min(40, (analysis['avg_upvotes'] / 1000) * 40)
        comment_score = min(30, (analysis['discussion_depth'] / 50) * 30)
        award_score = min(30, (analysis['total_awards'] / 10) * 30)
        
        return round(upvote_score + comment_score + award_score, 2)
    
    def _empty_analysis(self) -> Dict:
        return {
            'avg_upvotes': 0,
            'total_comments': 0,
            'total_awards': 0,
            'top_subreddits': [],
            'discussion_depth': 0
        }


class EnhancedSerpAPICollector:
    """
    SerpAPI æ”¶é›†å™¨ - ä» Googleã€Twitter å’Œ Reddit è·å–è¶‹åŠ¿æ•°æ®
    ä½œä¸ºå…¶ä»– API çš„æ›¿ä»£æ–¹æ¡ˆ
    """
    def __init__(self, api_key: Optional[str] = None, cache_manager: Optional[CacheManager] = None):
        self.api_key = api_key
        self.cache = cache_manager or CacheManager()
        self.available = SERPAPI_AVAILABLE and api_key is not None
        
        if not SERPAPI_AVAILABLE:
            print("âš ï¸ SerpAPI library not installed")
        elif not api_key:
            print("âš ï¸ SerpAPI key not provided")
        else:
            print("âœ… SerpAPI collector initialized")
    
    async def get_trending_topics(self, keywords: List[str], geo: str = 'US') -> List[Dict]:
        """
        ä» SerpAPI è·å–è¶‹åŠ¿æ•°æ®ï¼ˆGoogleã€Twitterã€Redditï¼‰
        """
        if not self.available:
            return []
        
        trending_topics = []
        
        for keyword in keywords[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.cache._generate_key('serpapi', {'keyword': keyword, 'geo': geo})
            cached_data = await self.cache.get(cache_key)
            
            if cached_data:
                print(f"ğŸ“¦ Using cached SerpAPI data for '{keyword}'")
                trending_topics.append(cached_data)
                continue
            
            try:
                # æœç´¢ Googleï¼ˆåŒ…å« Twitter å’Œ Reddit ç»“æœï¼‰
                results = await asyncio.to_thread(
                    self._search_google, keyword, geo
                )
                
                if not results:
                    print(f"â„¹ï¸ No SerpAPI data found for '{keyword}'")
                    continue
                
                # åˆ†æç»“æœ
                trend_data = self._analyze_serpapi_results(results, keyword)
                
                # ç¼“å­˜ç»“æœ
                await self.cache.set(cache_key, trend_data)
                trending_topics.append(trend_data)
                
                await asyncio.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶
                
            except Exception as e:
                print(f"âŒ SerpAPI error for '{keyword}': {e}")
                continue
        
        return trending_topics
    
    def _search_google(self, keyword: str, geo: str = 'US') -> Dict:
        """
        ä½¿ç”¨ SerpAPI æœç´¢ Googleï¼ˆåŒ…å« Twitter å’Œ Reddit ç»“æœï¼‰
        """
        try:
            params = {
                "q": keyword,
                "api_key": self.api_key,
                "engine": "google",
                "location": geo,
                "num": 50,  # è·å–æ›´å¤šç»“æœ
                "tbm": "nws"  # æ–°é—»æœç´¢ï¼ŒåŒ…å«ç¤¾äº¤åª’ä½“ç»“æœ
            }
            
            search = GoogleSearch(params)
            results = search.get_dict()
            
            return results
        except Exception as e:
            print(f"âŒ SerpAPI Google search error: {e}")
            return {}
    
    def _analyze_serpapi_results(self, results: Dict, keyword: str) -> Dict:
        """
        åˆ†æ SerpAPI ç»“æœï¼Œæå– Googleã€Twitter å’Œ Reddit ä¿¡å·
        """
        # æå– Google æœç´¢ç»“æœ
        organic_results = results.get('organic_results', [])
        news_results = results.get('news_results', [])
        
        # æå– Twitter ç»“æœï¼ˆä»æœç´¢ç»“æœä¸­æŸ¥æ‰¾ï¼‰
        twitter_mentions = 0
        twitter_engagement = 0
        twitter_hashtags = []
        
        # æå– Reddit ç»“æœ
        reddit_mentions = 0
        reddit_score = 0
        reddit_subreddits = []
        
        # åˆ†ææ‰€æœ‰ç»“æœ
        all_text = ""
        for result in organic_results + news_results:
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            link = result.get('link', '')
            all_text += f"{title} {snippet} "
            
            # æ£€æµ‹ Twitter é“¾æ¥
            if 'twitter.com' in link or 'x.com' in link:
                twitter_mentions += 1
                # å°è¯•æå–ç‚¹èµæ•°ç­‰ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'position' in result:
                    twitter_engagement += (51 - result.get('position', 50)) * 10
            
            # æ£€æµ‹ Reddit é“¾æ¥
            if 'reddit.com' in link:
                reddit_mentions += 1
                # æå– subreddit
                if '/r/' in link:
                    subreddit = link.split('/r/')[1].split('/')[0]
                    if subreddit not in reddit_subreddits:
                        reddit_subreddits.append(subreddit)
                # ä½ç½®åˆ†æ•°
                if 'position' in result:
                    reddit_score += (51 - result.get('position', 50)) * 10
        
        # æå– hashtags
        hashtags = re.findall(r'#\w+', all_text)
        twitter_hashtags = list(set(hashtags[:10]))
        
        # è®¡ç®—è¶‹åŠ¿åˆ†æ•°
        google_score = min(100, len(organic_results) * 5 + len(news_results) * 3)
        twitter_score = min(100, twitter_mentions * 15 + twitter_engagement * 0.1)
        reddit_score = min(100, reddit_mentions * 20 + reddit_score * 0.1)
        
        # ç»¼åˆåˆ†æ•°ï¼ˆSerpAPI ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œæƒé‡è¾ƒé«˜ï¼‰
        composite_score = (
            google_score * 0.5 +  # Google 50%
            twitter_score * 0.25 +  # Twitter 25%
            reddit_score * 0.25  # Reddit 25%
        )
        
        return {
            'keyword': keyword,
            'source': 'serpapi',
            'google_score': round(google_score, 2),
            'twitter_score': round(twitter_score, 2),
            'reddit_score': round(reddit_score, 2),
            'composite_score': round(composite_score, 2),
            'google_results_count': len(organic_results) + len(news_results),
            'twitter_mentions': twitter_mentions,
            'reddit_mentions': reddit_mentions,
            'twitter_hashtags': twitter_hashtags,
            'reddit_subreddits': reddit_subreddits[:5],
            'trend_score': round(composite_score, 2),
            'timestamp': datetime.utcnow().isoformat()
        }


class EnhancedGoogleTrendsCollector:
    """
    å¢å¼ºç‰ˆ Google Trends æ”¶é›†å™¨ - æ”¯æŒå†å²æ•°æ®
    """
    def __init__(self, cache_manager: Optional[CacheManager] = None):
        self.pytrends = None
        self.cache = cache_manager or CacheManager()
        
        if GOOGLE_TRENDS_AVAILABLE:
            try:
                self.pytrends = TrendReq(hl='en-US', tz=360, timeout=(10, 25), retries=3)
                print("âœ… Google Trends initialized")
            except Exception as e:
                print(f"âš ï¸ Google Trends initialization failed: {e}")
    
    async def get_trending_topics(self, keywords: List[str], geo: str = 'US', 
                                   timeframe: str = 'now 7-d') -> List[Dict]:
        """
        æ”¶é›† Google Trends æ•°æ®ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ—¶é—´èŒƒå›´ï¼‰
        """
        trending_topics = []
        
        if not self.pytrends:
            print("âš ï¸ Google Trends not available")
            return trending_topics
        
        for keyword in keywords[:5]:
            cache_key = self.cache._generate_key('google_trends', {
                'keyword': keyword, 
                'geo': geo, 
                'timeframe': timeframe
            })
            cached_data = await self.cache.get(cache_key)
            
            if cached_data:
                print(f"ğŸ“¦ Using cached Google Trends data for '{keyword}'")
                trending_topics.append(cached_data)
                continue
            
            try:
                # Build payload
                self.pytrends.build_payload([keyword], timeframe=timeframe, geo=geo)
                
                # Interest over time
                interest_df = self.pytrends.interest_over_time()
                
                if interest_df.empty or keyword not in interest_df.columns:
                    continue
                
                values = interest_df[keyword].values
                
                # è®¡ç®—å¢é•¿ç‡
                growth_rate = self._calculate_growth_rate(values)
                
                # è·å–ç›¸å…³æŸ¥è¯¢
                related_queries = self.pytrends.related_queries()
                rising_queries = []
                top_queries = []
                
                if keyword in related_queries:
                    if related_queries[keyword]['rising'] is not None:
                        rising_queries = related_queries[keyword]['rising']['query'].head(10).tolist()
                    if related_queries[keyword]['top'] is not None:
                        top_queries = related_queries[keyword]['top']['query'].head(10).tolist()
                
                # å½“å‰å…´è¶£åº¦
                current_interest = int(values[-1])
                
                # è¶‹åŠ¿æ–¹å‘
                trend_direction = self._determine_trend_direction(values)
                
                trend_data = {
                    'keyword': keyword,
                    'source': 'google_trends',
                    'current_interest': current_interest,
                    'growth_rate': round(growth_rate, 2),
                    'trend_direction': trend_direction,  # æ–°å¢ï¼šä¸Šå‡/ä¸‹é™/ç¨³å®š
                    'volatility': self._calculate_volatility(values),  # æ–°å¢ï¼šæ³¢åŠ¨æ€§
                    'rising_queries': rising_queries,
                    'top_queries': top_queries,
                    'trend_score': self._calculate_trends_score(current_interest, growth_rate, values),
                    'historical_data': values.tolist()[-30:] if len(values) > 30 else values.tolist(),  # ä¿å­˜æœ€è¿‘30å¤©æ•°æ®
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                await self.cache.set(cache_key, trend_data)
                trending_topics.append(trend_data)
                
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"âŒ Google Trends error for '{keyword}': {e}")
                continue
        
        return trending_topics
    
    def _calculate_growth_rate(self, values) -> float:
        """è®¡ç®—å¢é•¿ç‡"""
        if len(values) < 7:
            return 0.0
        
        recent_avg = values[-3:].mean()
        past_avg = values[:3].mean()
        
        if past_avg == 0:
            return 0.0
        
        growth_rate = ((recent_avg - past_avg) / past_avg) * 100
        return growth_rate
    
    def _determine_trend_direction(self, values) -> str:
        """åˆ¤æ–­è¶‹åŠ¿æ–¹å‘"""
        if len(values) < 3:
            return 'stable'
        
        recent_trend = values[-3:].mean()
        past_trend = values[-7:-3].mean() if len(values) >= 7 else values[:-3].mean()
        
        diff_ratio = (recent_trend - past_trend) / max(past_trend, 1)
        
        if diff_ratio > 0.2:
            return 'rising'
        elif diff_ratio < -0.2:
            return 'falling'
        else:
            return 'stable'
    
    def _calculate_volatility(self, values) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®ï¼‰"""
        if len(values) < 2:
            return 0.0
        return round(float(np.std(values)), 2)
    
    def _calculate_trends_score(self, interest: int, growth_rate: float, values) -> float:
        """è®¡ç®—ç»¼åˆè¶‹åŠ¿åˆ†æ•°"""
        # å…´è¶£åº¦åˆ†æ•°ï¼ˆ0-40åˆ†ï¼‰
        interest_score = interest * 0.4
        
        # å¢é•¿ç‡åˆ†æ•°ï¼ˆ0-40åˆ†ï¼‰
        growth_score = min(40, max(0, growth_rate) * 0.4)
        
        # ç¨³å®šæ€§åˆ†æ•°ï¼ˆ0-20åˆ†ï¼‰
        volatility = self._calculate_volatility(values)
        stability_score = max(0, 20 - volatility * 0.5)
        
        return round(interest_score + growth_score + stability_score, 2)


class CrossPlatformSignalAggregator:
    """
    è·¨å¹³å°ä¿¡å·èšåˆå™¨ - æ·±åº¦å…³è”åˆ†æ
    """
    def __init__(self, cache_manager: Optional[CacheManager] = None):
        self.cache = cache_manager or CacheManager()
    
    def aggregate_signals(self, twitter: List[Dict], reddit: List[Dict], 
                         google: List[Dict], serpapi: List[Dict] = None) -> List[Dict]:
        """
        æ·±åº¦èšåˆå’Œå…³è”åˆ†æï¼ˆåŒ…å« SerpAPI ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼‰
        """
        signal_map = {}
        
        # å¤„ç† Twitter ä¿¡å·
        for trend in twitter:
            keyword = trend['keyword']
            signal_map[keyword] = {
                'keyword': keyword,
                'twitter': self._extract_twitter_signals(trend),
                'reddit': {},
                'google': {},
                'serpapi': {},
                'sources': ['twitter']
            }
        
        # å¤„ç† Reddit ä¿¡å·
        for trend in reddit:
            keyword = trend['keyword']
            if keyword in signal_map:
                signal_map[keyword]['reddit'] = self._extract_reddit_signals(trend)
                signal_map[keyword]['sources'].append('reddit')
            else:
                signal_map[keyword] = {
                    'keyword': keyword,
                    'twitter': {},
                    'reddit': self._extract_reddit_signals(trend),
                    'google': {},
                    'serpapi': {},
                    'sources': ['reddit']
                }
        
        # å¤„ç† Google Trends ä¿¡å·
        for trend in google:
            keyword = trend['keyword']
            if keyword in signal_map:
                signal_map[keyword]['google'] = self._extract_google_signals(trend)
                signal_map[keyword]['sources'].append('google_trends')
            else:
                signal_map[keyword] = {
                    'keyword': keyword,
                    'twitter': {},
                    'reddit': {},
                    'google': self._extract_google_signals(trend),
                    'serpapi': {},
                    'sources': ['google_trends']
                }
        
        # å¤„ç† SerpAPI ä¿¡å·ï¼ˆä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼‰
        if serpapi:
            for trend in serpapi:
                keyword = trend['keyword']
                serpapi_signals = {
                    'score': trend.get('composite_score', 0),
                    'google_score': trend.get('google_score', 0),
                    'twitter_score': trend.get('twitter_score', 0),
                    'reddit_score': trend.get('reddit_score', 0),
                    'google_results': trend.get('google_results_count', 0),
                    'twitter_mentions': trend.get('twitter_mentions', 0),
                    'reddit_mentions': trend.get('reddit_mentions', 0),
                    'hashtags': trend.get('twitter_hashtags', []),
                    'subreddits': trend.get('reddit_subreddits', [])
                }
                
                if keyword in signal_map:
                    # å¦‚æœå·²æœ‰ä¿¡å·ï¼Œä½¿ç”¨ SerpAPI è¡¥å……æˆ–æ›¿ä»£
                    signal_map[keyword]['serpapi'] = serpapi_signals
                    # å¦‚æœå…¶ä»–æºå¤±è´¥ï¼Œä½¿ç”¨ SerpAPI æ•°æ®
                    if not signal_map[keyword]['twitter'] and trend.get('twitter_score', 0) > 0:
                        signal_map[keyword]['twitter'] = {
                            'score': trend.get('twitter_score', 0),
                            'hashtags': trend.get('twitter_hashtags', [])
                        }
                    if not signal_map[keyword]['reddit'] and trend.get('reddit_score', 0) > 0:
                        signal_map[keyword]['reddit'] = {
                            'score': trend.get('reddit_score', 0),
                            'subreddits': trend.get('reddit_subreddits', [])
                        }
                    if not signal_map[keyword]['google'] and trend.get('google_score', 0) > 0:
                        signal_map[keyword]['google'] = {
                            'score': trend.get('google_score', 0)
                        }
                    if 'serpapi' not in signal_map[keyword]['sources']:
                        signal_map[keyword]['sources'].append('serpapi')
                else:
                    # å¦‚æœå®Œå…¨æ²¡æœ‰ä¿¡å·ï¼Œä½¿ç”¨ SerpAPI ä½œä¸ºä¸»è¦æº
                    signal_map[keyword] = {
                        'keyword': keyword,
                        'twitter': {
                            'score': trend.get('twitter_score', 0),
                            'hashtags': trend.get('twitter_hashtags', [])
                        } if trend.get('twitter_score', 0) > 0 else {},
                        'reddit': {
                            'score': trend.get('reddit_score', 0),
                            'subreddits': trend.get('reddit_subreddits', [])
                        } if trend.get('reddit_score', 0) > 0 else {},
                        'google': {
                            'score': trend.get('google_score', 0)
                        } if trend.get('google_score', 0) > 0 else {},
                        'serpapi': serpapi_signals,
                        'sources': ['serpapi']
                    }
        
        # è®¡ç®—ç»¼åˆä¿¡å·
        merged_signals = []
        for keyword, signals in signal_map.items():
            composite = self._calculate_composite_signal(signals)
            composite['keyword'] = keyword
            composite['sources'] = signals['sources']
            composite['source_count'] = len(signals['sources'])
            merged_signals.append(composite)
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        merged_signals.sort(key=lambda x: x['composite_score'], reverse=True)
        
        return merged_signals
    
    def _extract_twitter_signals(self, trend: Dict) -> Dict:
        """æå– Twitter å…³é”®ä¿¡å·"""
        return {
            'score': trend.get('trend_score', 0),
            'engagement': trend.get('engagement_score', 0),
            'velocity': trend.get('velocity', 0),
            'sentiment': trend.get('sentiment', 'neutral'),
            'hashtags': trend.get('related_hashtags', [])
        }
    
    def _extract_reddit_signals(self, trend: Dict) -> Dict:
        """æå– Reddit å…³é”®ä¿¡å·"""
        return {
            'score': trend.get('trend_score', 0),
            'discussion_depth': trend.get('discussion_depth', 0),
            'subreddits': trend.get('top_subreddits', [])
        }
    
    def _extract_google_signals(self, trend: Dict) -> Dict:
        """æå– Google Trends å…³é”®ä¿¡å·"""
        return {
            'score': trend.get('trend_score', 0),
            'interest': trend.get('current_interest', 0),
            'growth_rate': trend.get('growth_rate', 0),
            'direction': trend.get('trend_direction', 'stable'),
            'rising_queries': trend.get('rising_queries', [])
        }
    
    def _calculate_composite_signal(self, signals: Dict) -> Dict:
        """
        è®¡ç®—ç»¼åˆä¿¡å·åˆ†æ•°ï¼ˆåŒ…å« SerpAPI ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼‰
        
        æ–°ç®—æ³•ï¼ˆä½¿ç”¨ SerpAPI æ—¶ï¼‰ï¼š
        - Twitter: 25% (ç¤¾äº¤è®¨è®ºçƒ­åº¦)
        - Reddit: 25% (ç¤¾åŒºæ·±åº¦è®¨è®º)
        - Google: 30% (æœç´¢éœ€æ±‚)
        - SerpAPI: 20% (ç»¼åˆæ›¿ä»£æ–¹æ¡ˆï¼Œå½“å…¶ä»–æºå¤±è´¥æ—¶æƒé‡æ›´é«˜)
        - è·¨å¹³å°åŠ æˆ: +15åˆ† (4ä¸ªå¹³å°éƒ½æœ‰) æˆ– +10åˆ† (3ä¸ªå¹³å°)
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ SerpAPI æ•°æ®
        has_serpapi = signals.get('serpapi', {}).get('score', 0) > 0
        use_serpapi_as_primary = (
            has_serpapi and 
            (signals['twitter'].get('score', 0) == 0 or 
             signals['reddit'].get('score', 0) == 0 or 
             signals['google'].get('score', 0) == 0)
        )
        
        if use_serpapi_as_primary:
            # ä½¿ç”¨ SerpAPI ä½œä¸ºä¸»è¦æ•°æ®æºï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰
            serpapi_data = signals['serpapi']
            twitter_score = max(
                signals['twitter'].get('score', 0),
                serpapi_data.get('twitter_score', 0)
            ) * 0.25
            reddit_score = max(
                signals['reddit'].get('score', 0),
                serpapi_data.get('reddit_score', 0)
            ) * 0.25
            google_score = max(
                signals['google'].get('score', 0),
                serpapi_data.get('google_score', 0)
            ) * 0.30
            serpapi_score = serpapi_data.get('score', 0) * 0.20
            
            base_score = twitter_score + reddit_score + google_score + serpapi_score
        else:
            # æ ‡å‡†ç®—æ³•ï¼ˆSerpAPI ä½œä¸ºè¡¥å……ï¼‰
            twitter_score = signals['twitter'].get('score', 0) * 0.25
            reddit_score = signals['reddit'].get('score', 0) * 0.25
            google_score = signals['google'].get('score', 0) * 0.30
            serpapi_score = signals.get('serpapi', {}).get('score', 0) * 0.20
            
            base_score = twitter_score + reddit_score + google_score + serpapi_score
        
        # è·¨å¹³å°åŠ æˆ
        source_bonus = 0
        source_count = len(signals['sources'])
        if source_count >= 4:
            source_bonus = 15  # 4ä¸ªå¹³å°éƒ½æœ‰ï¼Œå¼ºçƒˆä¿¡å·
        elif source_count == 3:
            source_bonus = 10  # 3ä¸ªå¹³å°ï¼Œä¸­ç­‰ä¿¡å·
        elif source_count == 2:
            source_bonus = 5   # 2ä¸ªå¹³å°ï¼ŒåŸºç¡€ä¿¡å·
        
        # è¶‹åŠ¿æ–¹å‘åŠ æˆ
        direction_bonus = 0
        if signals['google'].get('direction') == 'rising':
            direction_bonus = 5
        
        # æƒ…æ„ŸåŠ æˆ
        sentiment_bonus = 0
        if signals['twitter'].get('sentiment') == 'positive':
            sentiment_bonus = 3
        
        # SerpAPI æ›¿ä»£åŠ æˆï¼ˆå½“å…¶ä»–æºå¤±è´¥æ—¶ï¼‰
        serpapi_bonus = 0
        if use_serpapi_as_primary:
            serpapi_bonus = 8  # ä½¿ç”¨ SerpAPI ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆæ—¶çš„é¢å¤–åŠ æˆ
        
        composite_score = min(100, base_score + source_bonus + direction_bonus + sentiment_bonus + serpapi_bonus)
        
        return {
            'composite_score': round(composite_score, 2),
            'twitter_score': round(twitter_score / 0.25, 2) if twitter_score > 0 else 0,
            'reddit_score': round(reddit_score / 0.25, 2) if reddit_score > 0 else 0,
            'google_score': round(google_score / 0.30, 2) if google_score > 0 else 0,
            'serpapi_score': round(serpapi_score / 0.20, 2) if serpapi_score > 0 else 0,
            'growth_rate': signals['google'].get('growth_rate', 0),
            'viral_potential': self._calculate_viral_potential(signals),
            'related_info': {
                'twitter_hashtags': signals['twitter'].get('hashtags', []) or signals.get('serpapi', {}).get('hashtags', []),
                'reddit_subreddits': signals['reddit'].get('subreddits', []) or signals.get('serpapi', {}).get('subreddits', []),
                'rising_queries': signals['google'].get('rising_queries', []),
                'serpapi_used': use_serpapi_as_primary
            }
        }
    
    def _calculate_viral_potential(self, signals: Dict) -> float:
        """
        è®¡ç®—ç—…æ¯’å¼ä¼ æ’­æ½œåŠ›
        åŸºäºè·¨å¹³å°ä¿¡å·å¼ºåº¦
        """
        # Twitter é€Ÿåº¦
        twitter_velocity = signals['twitter'].get('velocity', 0)
        
        # Google å¢é•¿ç‡
        google_growth = signals['google'].get('growth_rate', 0)
        
        # Reddit è®¨è®ºæ·±åº¦
        reddit_depth = signals['reddit'].get('discussion_depth', 0)
        
        # ç»¼åˆè®¡ç®—
        viral_score = (
            min(40, twitter_velocity / 10 * 40) +
            min(40, max(0, google_growth) * 0.4) +
            min(20, reddit_depth / 50 * 20)
        )
        
        return round(min(100, viral_score), 2)


# å…¼å®¹æ€§åŒ…è£…å™¨ - ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§
class EnhancedSocialMediaAggregator:
    """
    å¢å¼ºç‰ˆç¤¾äº¤åª’ä½“èšåˆå™¨ - å…¼å®¹ç°æœ‰æ¥å£
    """
    def __init__(self, twitter_token: Optional[str] = None,
                 reddit_id: Optional[str] = None,
                 reddit_secret: Optional[str] = None,
                 serpapi_key: Optional[str] = None,
                 redis_url: Optional[str] = None):
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = CacheManager(redis_url=redis_url, ttl=3600)
        
        # åˆå§‹åŒ–æ”¶é›†å™¨
        self.twitter = EnhancedTwitterCollector(twitter_token, cache_manager)
        self.reddit = EnhancedRedditCollector(reddit_id, reddit_secret, cache_manager=cache_manager)
        self.google_trends = EnhancedGoogleTrendsCollector(cache_manager)
        self.serpapi = EnhancedSerpAPICollector(serpapi_key, cache_manager)
        
        # åˆå§‹åŒ–èšåˆå™¨
        self.signal_aggregator = CrossPlatformSignalAggregator(cache_manager)
    
    async def collect_all_trends(self, keywords: List[str], geo: str = 'US') -> Dict:
        """
        æ”¶é›†æ‰€æœ‰å¹³å°çš„è¶‹åŠ¿æ•°æ®ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰
        æ¯ä¸ªå¹³å°å•ç‹¬è®¾ç½®è¶…æ—¶ï¼Œé¿å…ä¸€ä¸ªå¹³å°æ…¢å½±å“æ•´ä½“
        """
        # ä¸ºæ¯ä¸ªå¹³å°è®¾ç½®ç‹¬ç«‹è¶…æ—¶ï¼ˆ15ç§’ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
        async def collect_with_timeout(task, platform_name, timeout=15.0):
            try:
                return await asyncio.wait_for(task, timeout=timeout)
            except asyncio.TimeoutError:
                print(f"âš ï¸ {platform_name} collection timeout ({timeout}s), skipping")
                return []
            except Exception as e:
                print(f"âš ï¸ {platform_name} collection failed: {e}")
                return []
        
        # å¹¶è¡Œæ”¶é›†ï¼Œæ¯ä¸ªéƒ½æœ‰ç‹¬ç«‹è¶…æ—¶
        twitter_task = collect_with_timeout(
            self.twitter.get_trending_topics(keywords),
            "Twitter",
            timeout=15.0  # å‡å°‘åˆ°15ç§’
        )
        reddit_task = collect_with_timeout(
            self.reddit.get_trending_topics(keywords),
            "Reddit",
            timeout=15.0  # å‡å°‘åˆ°15ç§’
        )
        google_task = collect_with_timeout(
            self.google_trends.get_trending_topics(keywords, geo),
            "Google Trends",
            timeout=15.0  # å‡å°‘åˆ°15ç§’
        )
        serpapi_task = collect_with_timeout(
            self.serpapi.get_trending_topics(keywords, geo),
            "SerpAPI",
            timeout=15.0  # 15ç§’è¶…æ—¶
        )
        
        twitter_trends, reddit_trends, google_trends, serpapi_trends = await asyncio.gather(
            twitter_task, reddit_task, google_task, serpapi_task,
            return_exceptions=True
        )
        
        # å¤„ç†å¼‚å¸¸
        if isinstance(twitter_trends, Exception):
            print(f"Twitter collection failed: {twitter_trends}")
            twitter_trends = []
        if isinstance(reddit_trends, Exception):
            print(f"Reddit collection failed: {reddit_trends}")
            reddit_trends = []
        if isinstance(google_trends, Exception):
            print(f"Google Trends collection failed: {google_trends}")
            google_trends = []
        if isinstance(serpapi_trends, Exception):
            print(f"SerpAPI collection failed: {serpapi_trends}")
            serpapi_trends = []
        
        # ä½¿ç”¨æ–°çš„èšåˆå™¨ï¼ˆåŒ…å« SerpAPIï¼‰
        merged_trends = self.signal_aggregator.aggregate_signals(
            twitter_trends, reddit_trends, google_trends, serpapi_trends
        )
        
        # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
        compatible_trends = []
        for trend in merged_trends:
            compatible_trend = {
                'keyword': trend['keyword'],
                'composite_score': trend['composite_score'],
                'growth_rate': trend.get('growth_rate', 0),
                'viral_potential': trend.get('viral_potential', 0),
                'sources': trend['sources'],
                'related_info': trend.get('related_info', {}),
                # ä¿æŒå‘åå…¼å®¹
                'twitter_hashtags': trend.get('related_info', {}).get('twitter_hashtags', []),
                'reddit_subreddits': trend.get('related_info', {}).get('reddit_subreddits', []),
                'rising_queries': trend.get('related_info', {}).get('rising_queries', [])
            }
            compatible_trends.append(compatible_trend)
        
        return {
            'merged_trends': compatible_trends,
            'by_source': {
                'twitter': twitter_trends,
                'reddit': reddit_trends,
                'google_trends': google_trends,
                'serpapi': serpapi_trends if not isinstance(serpapi_trends, Exception) else []
            },
            'collected_at': datetime.utcnow().isoformat()
        }


# å…¨å±€å®ä¾‹ï¼ˆå…¼å®¹ç°æœ‰ä»£ç ï¼‰
enhanced_social_aggregator = EnhancedSocialMediaAggregator()
