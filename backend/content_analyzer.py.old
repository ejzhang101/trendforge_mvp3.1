"""
Enhanced YouTube Channel Analyzer with Deep Content Analysis
MVP 2.0 - Intelligent Topic Extraction & Video Content Analysis
"""

import re
from typing import List, Dict, Optional, Tuple
from collections import Counter
import numpy as np

# NLP Libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import spacy

# Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# YouTube Transcript
from youtube_transcript_api import YouTubeTranscriptApi

# Semantic Keywords
try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    KEYBERT_AVAILABLE = False
    print("⚠️  KeyBERT not available. Install: pip install keybert")

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')

# Load spaCy model
try:
    nlp = spacy.load('en_core_web_sm')
except OSError:
    print("⚠️  Downloading spaCy model...")
    import subprocess
    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])
    nlp = spacy.load('en_core_web_sm')


class EnhancedContentAnalyzer:
    """
    Advanced content analyzer with NLP capabilities
    """
    
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # Add custom stop words
        self.stop_words.update([
            'video', 'youtube', 'like', 'subscribe', 'comment', 
            'channel', 'watch', 'episode', 'part', 'new', 'best',
            'top', 'first', 'last', 'full', 'official'
        ])
        
        if KEYBERT_AVAILABLE:
            self.keybert = KeyBERT()
        else:
            self.keybert = None
    
    def extract_topics_from_titles(self, titles: List[str]) -> List[Dict]:
        """
        Extract meaningful topics from video titles using NLP
        
        Returns:
            List of topics with scores and related terms
        """
        # Combine all titles
        combined_text = ' '.join(titles)
        
        # Method 1: TF-IDF + Noun Extraction
        tfidf_topics = self._extract_tfidf_topics(titles)
        
        # Method 2: spaCy Named Entity Recognition
        ner_topics = self._extract_named_entities(combined_text)
        
        # Method 3: KeyBERT semantic keywords (if available)
        if self.keybert:
            semantic_topics = self._extract_semantic_keywords(combined_text)
        else:
            semantic_topics = []
        
        # Merge and rank topics
        all_topics = self._merge_topics(tfidf_topics, ner_topics, semantic_topics)
        
        return all_topics[:15]  # Top 15 topics
    
    def _extract_tfidf_topics(self, titles: List[str]) -> List[Dict]:
        """
        Use TF-IDF to extract important terms, filtered by POS tags
        """
        # Preprocess: extract nouns and proper nouns only
        processed_titles = []
        for title in titles:
            doc = nlp(title.lower())
            # Keep only NOUN, PROPN (proper nouns), and filter stop words
            tokens = [
                token.lemma_ for token in doc 
                if token.pos_ in ['NOUN', 'PROPN'] 
                and token.text not in self.stop_words
                and len(token.text) > 2
                and not token.is_punct
            ]
            processed_titles.append(' '.join(tokens))
        
        if not any(processed_titles):
            return []
        
        # TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=30,
            ngram_range=(1, 3),  # Include bigrams and trigrams
            min_df=1,
            max_df=0.8
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(processed_titles)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calculate average TF-IDF scores
            avg_scores = tfidf_matrix.mean(axis=0).A1
            topics = [
                {
                    'topic': feature_names[i],
                    'score': float(avg_scores[i]),
                    'type': 'tfidf',
                    'frequency': self._count_occurrences(feature_names[i], titles)
                }
                for i in avg_scores.argsort()[-15:][::-1]
            ]
            
            return topics
        except:
            return []
    
    def _extract_named_entities(self, text: str) -> List[Dict]:
        """
        Extract named entities (brands, products, technologies, etc.)
        """
        doc = nlp(text)
        entities = []
        
        entity_counter = Counter()
        for ent in doc.ents:
            if ent.label_ in ['ORG', 'PRODUCT', 'GPE', 'PERSON', 'WORK_OF_ART', 'EVENT']:
                entity_counter[ent.text.lower()] += 1
        
        for entity, count in entity_counter.most_common(10):
            entities.append({
                'topic': entity,
                'score': count / len(doc.ents) if doc.ents else 0,
                'type': 'entity',
                'frequency': count
            })
        
        return entities
    
    def _extract_semantic_keywords(self, text: str) -> List[Dict]:
        """
        Use KeyBERT for semantic keyword extraction
        """
        if not self.keybert or not text.strip():
            return []
        
        try:
            keywords = self.keybert.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 3),
                stop_words='english',
                top_n=10,
                use_maxsum=True,
                diversity=0.7
            )
            
            return [
                {
                    'topic': kw[0],
                    'score': float(kw[1]),
                    'type': 'semantic',
                    'frequency': self._count_occurrences(kw[0], text)
                }
                for kw in keywords
            ]
        except:
            return []
    
    def _merge_topics(self, *topic_lists) -> List[Dict]:
        """
        Merge topics from different methods and rank by composite score
        """
        topic_map = {}
        
        for topics in topic_lists:
            for topic_data in topics:
                topic = topic_data['topic']
                score = topic_data['score']
                
                if topic in topic_map:
                    # Average scores from different methods
                    topic_map[topic]['score'] = (topic_map[topic]['score'] + score) / 2
                    topic_map[topic]['methods'].append(topic_data['type'])
                else:
                    topic_map[topic] = {
                        'topic': topic,
                        'score': score,
                        'frequency': topic_data.get('frequency', 1),
                        'methods': [topic_data['type']]
                    }
        
        # Boost score for topics found by multiple methods
        for topic_data in topic_map.values():
            method_bonus = len(topic_data['methods']) * 0.2
            topic_data['score'] = topic_data['score'] * (1 + method_bonus)
        
        # Sort by composite score
        ranked_topics = sorted(
            topic_map.values(),
            key=lambda x: (x['score'], x['frequency']),
            reverse=True
        )
        
        return ranked_topics
    
    def _count_occurrences(self, term: str, texts) -> int:
        """Count how many times a term appears across texts"""
        if isinstance(texts, str):
            texts = [texts]
        
        count = 0
        term_lower = term.lower()
        for text in texts:
            count += text.lower().count(term_lower)
        return count
    
    def analyze_video_content(self, video_id: str) -> Optional[Dict]:
        """
        Analyze video content from transcript
        
        Returns:
            {
                'main_topics': [...],
                'summary': '...',
                'sentiment': 'positive/neutral/negative',
                'key_points': [...]
            }
        """
        try:
            # Get transcript
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
            transcript_text = ' '.join([entry['text'] for entry in transcript_list])
            
            # Extract topics
            topics = self.extract_topics_from_titles([transcript_text])[:5]
            
            # Generate summary (first 500 chars)
            summary = transcript_text[:500] + '...' if len(transcript_text) > 500 else transcript_text
            
            # Basic sentiment (can be enhanced with proper sentiment analysis)
            sentiment = self._analyze_sentiment(transcript_text)
            
            return {
                'main_topics': [t['topic'] for t in topics],
                'summary': summary,
                'sentiment': sentiment,
                'transcript_length': len(transcript_text),
                'has_transcript': True
            }
        
        except Exception as e:
            print(f"Could not get transcript for {video_id}: {e}")
            return {
                'has_transcript': False,
                'error': str(e)
            }
    
    def _analyze_sentiment(self, text: str) -> str:
        """
        Basic sentiment analysis
        (Can be enhanced with proper sentiment models)
        """
        positive_words = {'great', 'amazing', 'awesome', 'excellent', 'best', 'love', 'perfect'}
        negative_words = {'bad', 'worst', 'terrible', 'awful', 'hate', 'horrible', 'disappointing'}
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count * 1.5:
            return 'positive'
        elif neg_count > pos_count * 1.5:
            return 'negative'
        else:
            return 'neutral'
    
    def analyze_high_performing_videos(self, videos: List[Dict]) -> Dict:
        """
        Analyze what makes high-performing videos successful
        
        Args:
            videos: List of video dicts with viewCount, title, etc.
        
        Returns:
            {
                'common_topics': [...],
                'avg_title_length': int,
                'best_posting_time': str,
                'engagement_patterns': {...}
            }
        """
        if not videos:
            return {}
        
        # Sort by view count
        sorted_videos = sorted(videos, key=lambda x: x.get('viewCount', 0), reverse=True)
        top_20_percent = sorted_videos[:max(1, len(sorted_videos) // 5)]
        
        # Extract topics from top performers
        top_titles = [v['title'] for v in top_20_percent]
        common_topics = self.extract_topics_from_titles(top_titles)
        
        # Analyze patterns
        avg_title_length = np.mean([len(v['title']) for v in top_20_percent])
        
        # Posting time analysis (hour of day)
        posting_hours = [v.get('publishedAt', '').split('T')[1].split(':')[0] 
                        for v in top_20_percent if 'publishedAt' in v]
        best_hour = Counter(posting_hours).most_common(1)[0][0] if posting_hours else 'N/A'
        
        # Engagement rate
        avg_engagement = np.mean([
            (v.get('likeCount', 0) + v.get('commentCount', 0)) / max(v.get('viewCount', 1), 1)
            for v in top_20_percent
        ])
        
        return {
            'common_topics': common_topics[:5],
            'avg_title_length': int(avg_title_length),
            'best_posting_hour': best_hour,
            'avg_engagement_rate': float(avg_engagement),
            'top_performer_count': len(top_20_percent)
        }
    
    def identify_content_style(self, videos: List[Dict]) -> Dict:
        """
        Identify the channel's content style and format
        """
        titles = [v['title'] for v in videos]
        combined = ' '.join(titles).lower()
        
        # Identify style indicators
        styles = {
            'tutorial': ['how to', 'tutorial', 'guide', 'tips', 'learn'],
            'review': ['review', 'unboxing', 'first look', 'hands on', 'vs'],
            'entertainment': ['funny', 'prank', 'challenge', 'compilation', 'fails'],
            'news': ['news', 'update', 'breaking', 'latest', 'today'],
            'vlog': ['vlog', 'daily', 'day in', 'life', 'routine'],
            'educational': ['explained', 'science', 'history', 'documentary', 'facts'],
            'gaming': ['gameplay', 'walkthrough', 'let\'s play', 'speedrun'],
            'tech': ['tech', 'gadget', 'phone', 'laptop', 'specs']
        }
        
        style_scores = {}
        for style, keywords in styles.items():
            score = sum(combined.count(kw) for kw in keywords)
            if score > 0:
                style_scores[style] = score
        
        # Get dominant styles
        sorted_styles = sorted(style_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'primary_style': sorted_styles[0][0] if sorted_styles else 'general',
            'style_distribution': dict(sorted_styles[:3]),
            'is_multi_format': len(sorted_styles) > 3
        }


class ChannelAudienceAnalyzer:
    """
    Analyze target audience based on content and engagement
    """
    
    def analyze_target_audience(self, videos: List[Dict], channel_data: Dict) -> Dict:
        """
        Infer target audience demographics and interests
        """
        # Analyze content complexity
        avg_description_length = np.mean([
            len(v.get('description', '')) for v in videos
        ])
        
        # Engagement patterns
        avg_comment_rate = np.mean([
            v.get('commentCount', 0) / max(v.get('viewCount', 1), 1) * 100
            for v in videos
        ])
        
        # Infer age group based on content style and language
        titles_text = ' '.join([v['title'] for v in videos]).lower()
        
        age_indicators = {
            'kids': ['kids', 'children', 'fun', 'cartoon', 'toy'],
            'teens': ['teen', 'tiktok', 'trending', 'viral', 'meme'],
            'young_adults': ['college', 'university', 'career', 'startup', 'lifestyle'],
            'adults': ['professional', 'business', 'finance', 'investment', 'advanced'],
            'all_ages': ['family', 'everyone', 'beginner', 'basic']
        }
        
        age_scores = {}
        for age_group, keywords in age_indicators.items():
            score = sum(titles_text.count(kw) for kw in keywords)
            age_scores[age_group] = score
        
        primary_age_group = max(age_scores.items(), key=lambda x: x[1])[0] if any(age_scores.values()) else 'general'
        
        return {
            'primary_age_group': primary_age_group,
            'engagement_level': 'high' if avg_comment_rate > 0.5 else 'medium' if avg_comment_rate > 0.2 else 'low',
            'content_depth': 'in-depth' if avg_description_length > 500 else 'moderate' if avg_description_length > 200 else 'brief',
            'subscriber_count': channel_data.get('subscriberCount', 0),
            'audience_size': 'large' if channel_data.get('subscriberCount', 0) > 100000 else 'medium' if channel_data.get('subscriberCount', 0) > 10000 else 'small'
        }


# Initialize analyzers
content_analyzer = EnhancedContentAnalyzer()
audience_analyzer = ChannelAudienceAnalyzer()


def analyze_channel_deeply(videos: List[Dict], channel_data: Dict) -> Dict:
    """
    Comprehensive channel analysis with all new features
    
    Returns:
        {
            'topics': [...],           # Extracted topics
            'high_performers': {...},  # What works well
            'content_style': {...},    # Channel style
            'target_audience': {...},  # Audience insights
            'video_analyses': [...]    # Individual video insights
        }
    """
    # Extract topics
    titles = [v['title'] for v in videos]
    topics = content_analyzer.extract_topics_from_titles(titles)
    
    # Analyze high performers
    high_performers = content_analyzer.analyze_high_performing_videos(videos)
    
    # Identify content style
    content_style = content_analyzer.identify_content_style(videos)
    
    # Analyze target audience
    target_audience = audience_analyzer.analyze_target_audience(videos, channel_data)
    
    # Analyze individual videos (limit to top 5)
    video_analyses = []
    top_videos = sorted(videos, key=lambda x: x.get('viewCount', 0), reverse=True)[:5]
    for video in top_videos:
        analysis = content_analyzer.analyze_video_content(video['videoId'])
        if analysis and analysis.get('has_transcript'):
            video_analyses.append({
                'video_id': video['videoId'],
                'title': video['title'],
                'views': video.get('viewCount', 0),
                'analysis': analysis
            })
    
    return {
        'topics': topics,
        'high_performers': high_performers,
        'content_style': content_style,
        'target_audience': target_audience,
        'video_analyses': video_analyses,
        'total_videos_analyzed': len(videos)
    }
